---
layout: page
title: Глава 30. Надёжность и журнал предзаписи
description: ""
tags: [PostgreSQL]
image:
  feature: abstract-11.jpg
  #credit: dargadgetz
  #creditlink: http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/
share: true
modified: 2018-12-03 T15:14:43-04:00
---

Глава 30. Надёжность и журнал предзаписи


В данной главе рассказывается, как для обеспечения эффективной и надёжной работы использу-
ется журнал предзаписи.
30.1. Надёжность
Надёжность — это важное свойство любой серьёзной СУБД и PostgreSQL делает всё возможное,
чтобы гарантировать надёжность своего функционирования. Один из аспектов надёжности состо-
ит в том, что все данные записываются с помощью подтверждённых транзакций, которые сохра-
няются в энергонезависимой области, которая защищена от потери питания, сбоев операционной
системы и аппаратных отказов (разумеется, за исключением отказа самой энергонезависимой об-
ласти). Успешная запись данных в постоянное место хранения (диск или эквивалентный носитель)
обычно всё, что требуется. Фактически, даже если компьютер полностью вышел из строя, если
диски выжили, то они могут быть переставлены в другой похожий компьютер и все подтверждён-
ные транзакции останутся неповреждёнными.
Хотя периодическая запись данных на пластины диска может выглядеть как простая операция, это
не так, потому что диски значительно медленнее, чем оперативная память и процессор, а также
потому что между оперативной памятью и пластинами диска есть некоторые механизмы кеширо-
вания. Во-первых, есть буферный кеш операционной системы, который кеширует частые запросы к
блокам диска и комбинирует запись на диск. К счастью, все операционные системы предоставляют
приложениям способ принудительной записи из буферного кеша на диск и PostgreSQL использует
эту возможность. (Смотрите параметр wal_sync_method который отвечает за то как это делается.)
Далее, кеширование также может осуществляться контроллером диска; в особенности это касает-
ся RAID-контроллеров. В некоторых случаях это кеширование работает в режиме сквозной записи,
что означает, что запись осуществляется на диск как только приходят данные. В других случаях,
возможна работа в режиме отложенной записи, что означает, что запись осуществляется неко-
торое время спустя. Такой режим кеширования может создавать риск для надёжности, потому что
память контроллера диска непостоянна и будет потеряна в случае потери питания. Лучшие кон-
троллеры имеют так называемую батарею резервного питания (Battery-Backup Unit, BBU), кото-
рая сохраняет кеш контроллера на батарее, если пропадёт системное питание. После возобновле-
ния питания, данные, оставшиеся в кеше контроллера, будут записаны на диски.
И наконец, многие диски имеют кеширование. На каких-то дисках оно работает в режиме сквозной
записи, на других в режиме отложенной записи, что приводит к тем же проблемам потери данных
для кеша отложенной записи, что и с кешем в контроллерах дисков. Диски IDE и SATA, потреби-
тельского класса особенно, часто имеют кеш отложенной записи, который сбрасывается при поте-
ре питания. Многие SSD-накопители также имеют зависимый от питания кеш отложенной записи.
Обычно, такое кеширование можно выключить; однако, то, как это делается, различается для опе-
рационной системы и для типа диска:
• В Linux параметры дисков IDE и SATA могут быть получены с помощью команды hdparm -I;
кеширование записи включено, если за строкой Write cache следует *. Для выключения ке-
ширования записи может быть использована команда hdparm -W 0. Параметры SCSI-дисков
могут быть получены с помощью утилиты sdparm. Используйте sdparm --get=WCE, чтобы про-
верить, включено ли кеширование записи, и sdparm --clear=WCE, чтобы выключить его.
• Во FreeBSD параметры IDE-дисков могут быть получены с помощью команды atacontrol, а
кеширование записи выключается при помощи установки параметра hw.ata.wc=0 в файле
/boot/loader.conf; Для SCSI-дисков параметры могут быть получены, используя команду
camcontrol identify, а кеширование записи изменяется при помощи утилиты sdparm.
• В Solaris кешированием записи на диск управляет команда format -e. (Использование файло-
вой системы Solaris ZFS, при включённом кешировании записи на диск, является безопасным,
потому что она использует собственные команды сброса кеша на диск.)
727Надёжность и журнал предзаписи
• В Windows, если параметр wal_sync_method установлен в open_datasync (по умолчанию), ке-
ширование записи на диск может быть выключено снятием галочки My Computer\Open\disk
drive\Properties\Hardware\Properties\Policies\Enable write caching on the disk. В
качестве альтернативы, можно установить параметр wal_sync_method в значение fsync или
fsync_writethrough, что предотвращает кеширование записи.
• В macOS кеширование записи можно отключить, установив для параметра wal_sync_method
значение fsync_writethrough.
Новые модели SATA-дисков (которые соответствуют стандарту ATAPI-6 или более позднему) пред-
лагают команду сброса кеша на диск (FLUSH CACHE EXT), а SCSI-диски уже давно поддерживают
похожую команду SYNCHRONIZE CACHE. Эти команды недоступны из PostgreSQL напрямую, но неко-
торые файловые системы (например, ZFS, ext4), могут использовать их для сброса данных из кеша
на пластины диска при включённом режиме кеша сквозной записи. К сожалению, такие файло-
вые системы ведут себя неоптимально при комбинировании с батареей резервного питания (BBU)
дискового контроллера. В таких случаях, команда синхронизации принуждает сохранять все дан-
ные на диск из кеша контроллера, сводя преимущество BBU к нулю. Вы можете запустить модуль
pg_test_fsync, чтобы увидеть, что вы попали в эту ситуацию. Если это так, преимущества произ-
водительности BBU могут быть восстановлены с помощью выключения барьеров записи для фай-
ловой системы или переконфигурирования контроллера диска, если это возможно. Если барьеры
записи выключены, убедитесь, что батарея годная; при отказе батареи может произойти потеря
данных. Есть надежда, что разработчики файловых систем и контроллеров дисков, в конце концов,
устранят это неоптимальное поведение.
Когда операционная система отправляет запрос на запись к аппаратному обеспечению для хра-
нения данных, она мало что может сделать, чтобы убедиться, что данные действительно сохране-
ны в какой-либо энергонезависимой области. Скорее, это является зоной ответственности адми-
нистратора, убедиться в целостности данных на всех компонентах хранения. Избегайте дисковых
контроллеров, которые не имеют батарей резервного питания для кеширования записи. На уровне
диска, запретите режим отложенной записи, если диск не может гарантировать, что данные будут
записаны перед выключением. Если вы используете SSD, знайте, что многие их них по умолчанию
не выполняют команды сброса кеша на диск. Вы можете протестировать надёжность поведения
подсистемы ввода/вывода, используя diskchecker.pl.
Другой риск потери данных состоит в самой записи на пластины диска. Пластины диска разделя-
ются на секторы, обычно по 512 байт каждый. Каждая операция физического чтения или записи
обрабатывает целый сектор. Когда дисковый накопитель получает запрос на запись, он может со-
ответствовать нескольким секторам по 512 байт (PostgreSQL обычно за один раз записывает 8192
байта или 16 секторов) и из-за отказа питания процесс записи может закончится неудачей в любое
время, что означает, что некоторые из 512-байтовых секторов будут записаны, а некоторые нет.
Чтобы защититься от таких сбоев, перед изменением фактической страницы на диске, PostgreSQL
периодически записывает полные образы страниц на постоянное устройство хранения WAL. С по-
мощью этого, во время восстановления после краха, PostgreSQL может восстановить из WAL стра-
ницы, которые записаны частично. Если у вас файловая система, которая защищена от частич-
ной записи страниц (например, ZFS), вы можете выключить работу с образами страниц, выключив
параметр full_page_writes. Батарея резервного питания (BBU) контроллера диска не защищает от
частичной записи страниц, если не гарантируется, что данные записаны в BBU как полные (8kB)
страницы.
PostgreSQL также защищает от некоторых видов повреждения данных на устройствах хранения,
которые могут произойти из-за аппаратных ошибок или из-за дефектов поверхности с течением
времени, например, при операциях чтения/записи во время сборки мусора.
• Каждая индивидуальная запись в WAL защищена с помощью контрольной суммы по алгорит-
му CRC-32 (32-bit), что позволяет судить о корректности данных в записи. Значение CRC уста-
навливается, когда мы пишем каждую запись WAL и проверяется в ходе восстановления после
сбоя, восстановления из архива, и при репликации.
• Страницы данных в настоящее время не защищаются контрольными суммами по умолчанию,
хотя полные образы страниц, записанные в WAL будут защищены; смотрите initdb для деталей
о включении в страницы данных информации о контрольных суммах.
728Надёжность и журнал предзаписи
• Для внутренних структур данных, таких как pg_xact, pg_subtrans, pg_multixact, pg_serial,
pg_notify, pg_stat, pg_snapshots не ведётся расчёт контрольной суммы, равно как и для
страниц, защищённых посредством полностраничной записи. Однако там, где такие структу-
ры данных являются постоянными, записи WAL пишутся таким образом, чтобы после сбоя бы-
ло возможно аккуратно повторить последние изменения, а эти записи WAL защищаются так
же, как описано выше.
• Файлы каталога pg_twophase защищены с помощью контрольной суммы CRC-32.
• Временные файлы данных, используемые в больших SQL-запросах для сортировки, материа-
лизации и промежуточных результатов, в настоящее время не защищаются контрольной сум-
мой, а изменения в этих файлах не отражаются в WAL.
PostgreSQL не защищает от корректируемых ошибок памяти; предполагается, что вы будете ра-
ботать с памятью, которая использует промышленный стандарт коррекции ошибок (ECC, Error
Correcting Codes) или лучшую защиту.
30.2. Журнал предзаписи (WAL)
Журнал предзаписи (WAL) — это стандартный метод обеспечения целостности данных. Детальное
описание можно найти в большинстве книг (если не во всех) по обработке транзакций. Вкратце,
основная идея WAL состоит в том, что изменения в файлах с данными (где находятся таблицы и
индексы) должны записываться только после того, как эти изменения были занесены в журнал,
т. е. после того как записи журнала, описывающие данные изменения, будут сохранены на посто-
янное устройство хранения. Если следовать этой процедуре, то записывать страницы данных на
диск после подтверждения каждой транзакции нет необходимости, потому что мы знаем, что если
случится сбой, то у нас будет возможность восстановить базу данных с помощью журнала: любые
изменения, которые не были применены к страницам с данными, могут быть воссозданы из запи-
сей журнала. (Это называется восстановлением с воспроизведением, или REDO.)
Подсказка
Поскольку WAL восстанавливает содержимое файлов базы данных, журналируемая
файловая система не является необходимой для надёжного хранения файлов с дан-
ными или файлов WAL. Фактически, журналирование может снизить производитель-
ность, особенно если журналирование заставляет сохранять данные файловой систе-
мы на диск. К счастью, такое сохранение при журналировании часто можно отключить
с помощью параметров монтирования файловой системы, например, data=writeback
для файловой системы ext3 в Linux. С другой стороны, с журналируемыми файловыми
системами увеличивается скорость загрузки после сбоя.
Результатом использования WAL является значительное уменьшение количества запросов запи-
си на диск, потому что для гарантии, что транзакция подтверждена, в записи на диск нуждается
только файл журнала, а не каждый файл данных изменённый в результате транзакции. Файл жур-
нала записывается последовательно и таким образом, затраты на синхронизацию журнала намно-
го меньше, чем затраты на запись страниц с данными. Это особенно справедливо для серверов,
которые обрабатывают много маленьких транзакций, изменяющих разные части хранилища дан-
ных. Таким образом, когда сервер обрабатывает множество мелких конкурентных транзакций, для
подтверждения многих транзакций достаточно одного вызова fsync на файл журнала.
WAL также делает возможным поддержку онлайнового резервного копирования и восстановления
на определённый момент времени, как описывается в Разделе  25.3. С помощью архивирования
данных WAL поддерживается возврат к любому моменту времени, который доступен в данных WAL:
мы просто устанавливаем предыдущую физическую резервную копию базы данных и воспроизво-
дим журнал WAL до нужного момента времени. Более того, физическая резервная копия не долж-
на быть мгновенным снимком состояния баз данных — если она была сделана некоторое время
назад, воспроизведение журнала WAL за этот период исправит все внутренние несоответствия.
729Надёжность и журнал предзаписи
30.3. Асинхронное подтверждение транзакций
Асинхронная фиксация — это возможность завершать транзакции быстрее, ценой того, что в слу-
чае краха СУБД последние транзакции могут быть потеряны. Для многих приложений такой ком-
промисс приемлем.
Как описано в предыдущей части, подтверждение транзакции обычно синхронное: сервер ждёт
сохранения записей WAL транзакции в постоянном хранилище, прежде чем сообщить клиенту об
успешном завершении. Таким образом, клиенту гарантируется, что транзакция, которую подтвер-
дил сервер, будет защищена, даже если сразу после этого произойдёт крах сервера. Однако, для
коротких транзакций данная задержка будет основной составляющей общего времени транзак-
ции. В режиме асинхронного подтверждения сервер сообщает об успешном завершении сразу, как
только транзакция будет завершена логически, прежде чем сгенерированные записи WAL факти-
чески будут записаны на диск. Это может значительно увеличить производительность при выпол-
нении небольших транзакций.
Асинхронное подтверждение транзакций приводит к риску потери данных. Существует короткое
окно между отчётом о завершении транзакции для клиента и временем, когда транзакция реаль-
но подтверждена (т. е. гарантируется, что она не будет потеряна в случае краха сервера). Таким
образом, асинхронное подтверждение транзакций не должно использоваться, если клиент будет
выполнять внешние действия, опираясь на предположение, что транзакция будет сохранена. На-
пример, банк конечно не должен использовать асинхронное подтверждение для транзакций в бан-
коматах, выдающих наличные. Но во многих случаях, таких как журналирование событий, столь
серьёзная гарантия сохранности данных не нужна.
Риск потери данных при использовании асинхронного подтверждения транзакций — это не риск
повреждения данных. Если случился крах СУБД, она будет восстановлена путём воспроизведения
WAL до последней записи, которая была записана на диск. Таким образом, будет восстановлено
целостное состояние СУБД, но любые транзакции, которые ещё не были сохранены на диск, в этом
состоянии не будут отражены. Чистый эффект будет заключаться в потере нескольких последних
транзакций. Поскольку транзакции воспроизводятся в том же порядке, в котором подтверждались,
воспроизведение не нарушает целостность — например, если транзакция "B" выполнила измене-
ния, которые влияют на предыдущую транзакцию "A", то не может быть такого, что изменения,
выполненные "A" были потеряны, а изменения, внесённые "B" сохранены.
Пользователь может выбрать режим подтверждения для каждой транзакции, так что возможен
конкурентный запуск транзакций в синхронном и асинхронном режиме. Это позволяет достичь
гибкого компромисса между производительностью и конечно надёжностью транзакций. Режим
подтверждения транзакций управляется параметром synchronous_commit, который может быть из-
менён любым из способов, пригодным для установки параметров конфигурации. Режим, исполь-
зуемый для какой-либо конкретной транзакции, зависит от значения synchronous_commit, которое
действует на момент начала этой транзакции.
Некоторые команды, например DROP TABLE, принудительно запускают синхронное подтверждение
транзакции, независимо от значения synchronous_commit. Это сделано для того, чтобы иметь уве-
ренность в целостности данных между файловой системой сервера и логическим состоянием ба-
зы данных. Команды, которые поддерживают двухфазное подтверждение транзакций, такие как
PREPARE TRANSACTION, также всегда синхронные.
Если во время окна риска между асинхронным подтверждением транзакции и сохранением на
диск записей WAL, происходит крах СУБД, то изменения, сделанные во время этой транзак-
ции будут потеряны. Продолжительность окна риска ограничена, потому что фоновый процесс
(«WAL writer»), сохраняет не записанные записи WAL на диск каждые wal_writer_delay миллисе-
кунд. Фактически, максимальная продолжительность окна риска составляет трёхкратное значе-
ние wal_writer_delay, потому что WAL writer разработан так, чтобы сразу сохранять целые стра-
ницы во время периодов занятости.
730Надёжность и журнал предзаписи
Внимание
Режим немедленного завершения работы (immediate) эквивалентен краху сервера и
приведёт, таким образом, к потере всех не сохранённых асинхронных транзакций.
Асинхронное подтверждение транзакций предоставляет поведение, которое отличается от того,
что соответствует установке параметра fsync = off. Настройка fsync касается всего сервера и мо-
жет изменить поведение всех транзакций. Она выключает всю логику внутри PostgreSQL, которая
пытается синхронизировать запись отдельных порций в базу данных и, таким образом, крах си-
стемы (обусловленный отказом аппаратного обеспечения или операционной системы, который не
является сбоем самой СУБД PostgreSQL ) может в результате привести к повреждению состояния
базы данных. Во многих случаях, асинхронное подтверждение транзакций предоставляет лучшую
производительность, чем то, что можно получить выключением fsync, но без риска повреждения
данных.
commit_delay также выглядит очень похоже на асинхронное подтверждение транзакций, но по су-
ти это является методом асинхронного подтверждения транзакций (фактически, во время асин-
хронных транзакций commit_delay игнорируется). commit_delay приводит к задержке только пе-
ред тем, как синхронная транзакция пытается записать данные WAL на диск, в надежде, что оди-
ночная запись, выполняемая на одну такую транзакцию, сможет также обслужить другие транзак-
ции, которые подтверждаются приблизительно в это же время. Установку этого параметра мож-
но рассматривать как способ увеличения промежутка времени, в течение которого транзакции
группируются для единовременной записи на диск. Это распределяет стоимость записи между
несколькими транзакциями.
30.4. Настройка WAL
Существует несколько конфигурационных параметров относящихся к WAL, которые влияют на
производительность СУБД. Далее рассказывается об их использовании. Общую информацию об
установке параметров конфигурации сервера смотрите в Главе 19.
Контрольные точки— это точки в последовательности транзакций, в которых гарантируется, что
файлы с данными и индексами были обновлены всей информацией записанной перед контрольной
точкой. Во время контрольной точки, все страницы данных, находящиеся в памяти, сохраняются
на диск, а в файл журнала записывается специальная запись контрольной точки. (Сделанные из-
менения были перед этим записаны в файлы WAL.) В случае краха процедура восстановления ищет
последнюю запись контрольной точки, чтобы определить эту точку в журнале (называемую запи-
сью REDO), от которой процедура должна начать операцию воспроизведения изменений. Любые
изменения файлов данных перед этой точкой гарантированно находятся уже на диске. Таким об-
разом, после контрольной точки, сегменты журнала, которые предшествуют записи воспроизве-
дения, больше не нужны и могут быть удалены или пущены в циклическую перезапись. (Когда ар-
хивирование WAL будет завершено, сегменты журнала должны быть архивированы перед их уда-
лением или циклической перезаписи.)
Запись всех страниц данных из памяти на диск, которая требуется для контрольной точки, может
вызвать значительную нагрузку на дисковый ввод/вывод. По этой причине, активность записи по
контрольной точке регулируется так, что ввод/вывод начинается при старте контрольной точки и
завершается перед стартом следующей контрольной точки; это минимизирует потерю производи-
тельности во время прохождения контрольных точек.
Отдельный серверный процесс контрольных точек автоматически выполняет контрольные точки
с заданной частотой. Контрольные точки производятся каждые checkpoint_timeout секунд либо
при приближении к пределу max_wal_size, если это имеет место раньше. Значения по умолча-
нию: 5 минут и 1 Гбайт, соответственно. Если после предыдущей контрольной точки новые записи
WAL не добавились, следующие контрольные точки будут пропущены, даже если проходит время
checkpoint_timeout. (Если вы применяете архивацию WAL и хотите установить нижний предел
для частоты архивации, чтобы ограничить потенциальную потерю данных, вам следует настраи-
731Надёжность и журнал предзаписи
вать параметр archive_timeout, а не параметры контрольных точек.) Также можно выполнить кон-
трольную точку принудительно, воспользовавшись SQL-командой CHECKPOINT.
Уменьшение значений checkpoint_timeout и/или max_wal_size приводит к учащению контроль-
ных точек. Это позволяет ускорить восстановление после краха (поскольку для воспроизведения
нужно меньше данных), но с другой стороны нужно учитывать дополнительную нагрузку, возни-
кающую вследствие более частого сброса изменённых страниц данных на диск. Если включён ре-
жим full_page_writes (по умолчанию это так), нужно учесть и ещё один фактор. Для обеспечения
целостности страницы данных, при первом изменении страницы данных после контрольной точки
эта страница записывается в журнал целиком. В данном случае, чем меньше интервал между кон-
трольными точками, тем больше объём записи в журнал WAL, так что это частично дискредитиру-
ет идею уменьшения интервала записи, и в любом случае приводит к увеличению объёма обмена
с диском
Контрольные точки довольно дороги с точки зрения ресурсов, во-первых, потому что они требу-
ют записи всех буферов из памяти на диск, и во-вторых потому что они создают дополнительный
трафик WAL, о чём говорилось выше. Таким образом, будет благоразумным установить параметры
контрольных точек так, чтобы контрольные точки не выполнялись слишком часто. Для простой
проверки параметров контрольной точки можно установить параметр checkpoint_warning. Если
промежуток времени между контрольными точками будет меньше чем количество секунд, задан-
ное параметром checkpoint_warning, то в журнал сервера будет выдано сообщение с рекомен-
дацией увеличить max_wal_size. Эпизодическое появление такого сообщения не является пово-
дом для беспокойства. Но если оно появляется часто, необходимо увеличить значения парамет-
ров управления контрольными точками. Массовые операции, такие как COPY с большим объёмом
данных, могут привести к появлению нескольких таких предупреждений, если вы не установили
max_wal_size достаточно большим.
Чтобы избежать «заваливания» системы ввода/вывода при резкой интенсивной записи страниц, за-
пись «грязных» буферов во время контрольной точки растягивается на определённый период вре-
мени. Этот период управляется параметром checkpoint_completion_target, который задаётся как
часть интервала контрольной точки. Скорость ввода/вывода подстраивается так, чтобы контроль-
ная точка завершилась к моменту истечения заданной части от checkpoint_timeout секунд или до
превышения max_wal_size, если оно имеет место раньше. Со значением 0.5, заданным по умолча-
нию, можно ожидать, что PostgreSQL завершит процедуру контрольной точки примерно за поло-
вину времени до начала следующей. В системе, которая работает практически на пределе мощно-
сти ввода/вывода в обычном режиме, есть смысл увеличить checkpoint_completion_target, чтобы
уменьшить нагрузку ввода/вывода, связанную с контрольными точками. Но с другой стороны, рас-
тягивание контрольных точек влияет на время восстановления, так как для восстановления нужно
будет задействовать большее количество сегментов WAL. Хотя в checkpoint_completion_target
можно задать значение вплоть до 1.0, лучше выбрать значение меньше (по крайней мере, не боль-
ше 0.9), так как при контрольных точках выполняются и некоторые другие операции, помимо за-
писи «грязных» буферов. Со значением 1.0 контрольные точки, скорее всего, не будут завершать-
ся вовремя, что приведёт к потере производительности из-за неожиданных колебаний требуемого
количества сегментов WAL.
На платформах Linux и POSIX параметр checkpoint_flush_after позволяет принудить ОС к сбросу
страниц, записываемых во время контрольной точки, при накоплении заданного количества байт.
Если его не настроить, эти страницы могут оставаться в кеше страниц ОС, что повлечёт заторма-
живание при выполнении fsync в конце контрольной точки. Этот параметр часто помогает умень-
шить задержки транзакций, но может оказать и негативное влияние на производительность; осо-
бенно, когда объём нагрузки больше shared_buffers, но меньше кеша страниц в ОС.
Число файлов сегментов WAL в каталоге pg_wal зависит от min_wal_size, max_wal_size и объёма
WAL, сгенерированного в предыдущих циклах контрольных точек. Когда старые файлы сегментов
оказываются не нужны, они удаляются или перерабатываются (то есть переименовываются, чтобы
стать будущими сегментами в нумерованной последовательности). Если вследствие кратковремен-
ного скачка интенсивности записи в журнал, предел max_wal_size превышается, ненужные файлы
сегментов будут удаляться, пока система не опустится ниже этого предела. Оставаясь ниже этого
предела, система перерабатывает столько файлов WAL, сколько необходимо для покрытия ожида-
732Надёжность и журнал предзаписи
емой потребности до следующей контрольной точки, и удаляет остальные. Эта оценка базируется
на скользящем среднем числа файлов WAL, задействованных в предыдущих циклах контрольных
точек. Скользящее среднее увеличивается немедленно, если фактическое использование превы-
шает оценку, так что в нём в некоторой степени накапливается пиковое использование, а не сред-
нее. Значение min_wal_size ограничивает снизу число файлов WAL, которые будут переработаны
для будущего использования; такой объём WAL всегда будет перерабатываться, даже если система
простаивает и оценка использования говорит, что нужен совсем небольшой WAL.
Вне зависимости от max_wal_size, в количестве wal_keep_segments + 1 самые последние файлы
WAL сохраняются в любом случае. Так же, если применяется архивация WAL, старые сегменты не
могут быть удалены или переработаны, пока они не будут заархивированы. Если WAL архивирует-
ся медленнее, чем генерируется, либо если команда archive_command постоянно даёт сбои, старые
файлы WAL будут накапливаться в pg_wal, пока ситуация не будет разрешена. Медленно работа-
ющий или отказавший ведомый сервер, использующий слот репликации, даст тот же эффект (см.
Подраздел 26.2.6).
В режиме восстановления архива или горячего резерва сервер периодически выполняет точки
перезапуска, которые похожи на контрольные точки в обычном режиме работы: сервер принуди-
тельно сбрасывает своё состояние на диск, обновляет файл pg_control, чтобы показать, что уже
обработанные данные WAL не нужно сканировать снова, и затем перерабатывает все старые файлы
сегментов журнала в каталоге pg_wal. Точки перезапуска не могут выполняться чаще, чем кон-
трольные точки на главном сервере, так как они могут происходить только в записях контроль-
ных точек. Точка перезапуска производится, когда достигается запись контрольной точки и по-
сле предыдущей точки перезапуска прошло не меньше checkpoint_timeout секунд или размер
WAL может превысить max_wal_size. Однако из-за того, что на время выполнения точек переза-
пуска накладываются ограничения, max_wal_size часто превышается при восстановлении, вплоть
до объёма WAL, записываемого в цикле между контрольными точками. (Значение max_wal_size
никогда и не было жёстким пределом, так что всегда следует оставлять приличный запас сверху,
чтобы не остаться без свободного места на диске.)
Наиболее часто используются две связанные с WAL внутренние функции: XLogInsertRecord и
XLogFlush. XLogInsertRecord применяется для добавления записи в буферы WAL в разделяе-
мой памяти. Если места для новой записи недостаточно, XLogInsertRecord придётся записать
(переместить в кеш ядра) несколько заполненных буферов WAL. Это нежелательно, так как
XLogInsertRecord используется при каждом изменении в базе данных на низком уровне (напри-
мер, при добавлении строки) в момент, когда установлена исключительная блокировка задейство-
ванных страниц данных, поэтому данная операция должна быть максимально быстрой. Что ещё
хуже, запись буферов WAL может также повлечь создание нового сегмента журнала, что займёт
ещё больше времени. Обычно буферы WAL должны записываться и сохраняться на диске в функ-
ции XLogFlush, которая вызывается, по большей части, при фиксировании транзакции, чтобы ре-
зультаты транзакции сохранились в надёжном хранилище. В системах с интенсивной записью в
журнал вызовы XLogFlush могут иметь место не так часто, чтобы XLogInsertRecord не приходилось
производить запись. В таких системах следует увеличить число буферов WAL, изменив параметр
wal_buffers. Когда включён режим full_page_writes и система очень сильно загружена, увеличение
wal_buffers поможет сгладить скачки во времени ответа в период сразу после каждой контроль-
ной точки.
Параметр commit_delay определяет, на сколько микросекунд будет засыпать ведущий процесс
группы, записывающий в журнал, после получения блокировки в XLogFlush, пока подчинённые
формируют очередь на запись. Во время этой задержки другие серверные процессы смогут добав-
лять записи в WAL буферы журнала, чтобы все эти записи сохранились на диск в результате одной
операции синхронизации, которую выполнит ведущий. Ведущий процесс не засыпает, если отклю-
чён режим fsync, либо число сеансов с активными транзакциями меньше commit_siblings, так как
маловероятно, что какой-либо другой сеанс зафиксирует транзакцию в ближайшее время. Заметь-
те, что на некоторых платформах, разрешение этого таймера сна составляет 10 миллисекунд, так
что любое значение параметра commit_delay от 1 до 10000 микросекунд будет действовать одина-
ково. Кроме того, в некоторых системах состояние сна может продлиться несколько дольше, чем
требует параметр.
733Надёжность и журнал предзаписи
Так как цель commit_delay состоит в том, чтобы позволить стоимости каждой операции син-
хронизации амортизироваться через параллельную фиксацию транзакций (потенциально за счёт
задержки транзакции), необходимо определить количество той стоимости, прежде чем урегу-
лирование сможет быть выбрано разумно. Чем выше стоимость, тем более эффективный будет
commit_delay в увеличении пропускной способности транзакций в какой-то степени. Программа
pg_test_fsync может использоваться, чтобы измерить среднее время в микросекундах, которое за-
нимает одиночная работа сброса WAL на диск. Значение половины среднего времени сообщаемого
программой рекомендуется в качестве отправной точки для использования значения в параметре
commit_delay при оптимизации для конкретного объёма работы, и говорит о том, сколько нужно
времени для синхронизации сброса единственной операции записи 8 КБ. Настройка параметра
commit_delay особенно полезна в случае хранения WAL в хранилище с высокоскоростными диска-
ми, такими как твердотельные накопители (SSD) или RAID-массивы с кешем записи и аварийным
питанием на батарее; но это определённо должно тестироваться на репрезентативной рабочей
нагрузке. Более высокие значения commit_siblings должны использоваться в таких случаях, то-
гда как меньшие значения commit_siblings часто полезны на носителях с большими задержками.
Обратите внимание на то, что увеличение значения параметра commit_delay может увеличить за-
держку транзакции настолько, что пострадает общая производительность транзакций.
Даже если commit_delay равен нулю (значение по умолчанию), групповая фиксация все равно мо-
жет произойти, но группа будет состоять только из тех сеансов, которым понадобилось сбросить
записи о фиксации на диск за то время, пока происходил предыдущий сброс. Чем больше сеансов,
тем чаще это происходит даже при нулевом commit_delay, поэтому увеличение этого параметра
может и не оказать заметного действия. Установка commit_delay имеет смысл в двух случаях: (1)
когда несколько транзакций одновременно фиксируют изменения, (2) либо когда частота фикса-
ций ограничена пропускной способностью дисковой подсистемы. Однако при задержке из-за низ-
кой скорости вращения диска, эта настройка может оказаться полезной даже всего при двух се-
ансах.
Параметр wal_sync_method определяет, как PostgreSQL будет обращаться к ядру, чтобы принуди-
тельно сохранить WAL на диск. Все методы должны быть одинаковыми в плане надёжности, за
исключением fsync_writethrough, который может иногда принудительно сбрасывать кеш диска,
даже если другие методы не делают этого. Однако, какой из них самый быстрый, во многом опре-
деляется платформой; вы можете протестировать скорость, используя модуль pg_test_fsync. Обра-
тите внимание, что данный параметр не имеет значения, если fsync выключен.
Включение параметра конфигурации wal_debug (предоставляется, если PostgreSQL был скомпи-
лирован с его поддержкой) будет приводить к тому, что все вызовы связанных с WAL функций
XLogInsertRecord и XLogFlush будут протоколироваться в журнале сервера. В будущем данный
параметр может быть заменён более общим механизмом.
30.5. Внутреннее устройство WAL
WAL включается автоматически; от администратора не требуется никаких действий за исключе-
нием того, чтобы убедиться, что выполнены требования WAL к месту на диске, и что выполнены
все необходимые действия по тонкой настройке (см. Раздел 30.4).
Записи WAL добавляются в журналы WAL по мере поступления. Позицию добавления в журнал
определяет значение LSN (Log Sequence Number, Последовательный номер в журнале), представ-
ляющее собой смещение в байтах внутри журнала, монотонно увеличивающееся с каждой новой
записью. Значения LSN возвращаются с типом данных pg_lsn. Сравнивая эти значения, можно
вычислить объём данных WAL между ними, так что они могут быть полезны для вычисления про-
гресса при репликации и восстановлении.
Журналы WAL хранятся в виде набора файлов сегментов в каталоге pg_wal, который находится
в каталоге данных. Эти файлы обычно имеют размер 16 Мбайт каждый (его можно изменить, пе-
редав initdb другое значение в --wal-segsize). Каждый файл сегмента разделяется на страницы,
обычно по 8 Кбайт каждая (данный размер может быть изменён указанием configure --with-wal-
blocksize). Заголовки записей журнала описываются в access/xlogrecord.h; содержимое самой
734Надёжность и журнал предзаписи
записи зависит от типа события, которое сохраняется в журнале. Файлы сегментов имеют име-
на-номера, которые начинаются с 000000010000000000000000 и увеличиваются автоматически. За-
цикливание этих номеров не предусмотрено, но для использования всех доступных номеров потре-
буется очень, очень много времени.
Выгодно размещать журналы WAL на другом диске, отличном от того, где находятся основные фай-
лы базы данных. Для этого можно переместить каталог pg_wal в другое место (разумеется, когда
сервер остановлен) и создать символьную ссылку из исходного места на перемещённый каталог.
Для WAL важно, чтобы запись в журнал выполнялась до изменений данных в базе. Но этот порядок
могут нарушить дисковые устройства, которые ложно сообщают ядру об успешном завершении
записи, хотя фактически они только выполнили кеширование данных и пока не сохранили их на
диск. Сбой питания в такой ситуации может привести к неисправимому повреждению данных.
Администраторы должны убедиться, что диски, где хранятся файлы журналов WAL PostgreSQL, не
выдают таких ложных сообщений ядру. (См. Раздел 30.1.)
После выполнения контрольной точки и сброса журнала позиция контрольной точки сохраня-
ется в файл pg_control. Таким образом, при старте восстановления сервер сперва читает файл
pg_control и затем запись контрольной точки; затем он выполняет операцию REDO, сканируя впе-
рёд от позиции в журнале, обозначенной в записи контрольной точки. Поскольку полное содержи-
мое страниц данных сохраняется в журнале в первой странице после контрольной точки (предпо-
лагается, что включён режим full_page_writes), все страницы, изменённые с момента контрольной
точки, будут восстановлены в целостном состоянии.
В случае, если файл pg_control повреждён, мы должны поддерживать возможность сканирования
существующих сегментов журнала в обратном порядке — от новых к старым — чтобы найти по-
следнюю контрольную точку. Это пока не реализовано. pg_control является достаточно малень-
ким файлом (меньше, чем одна дисковая страница), который не должен попадать под проблему
частичной записи и на момент написания данной документации, не было ни одного сообщения
о сбоях СУБД исключительно из-за невозможности чтения самого файла pg_control. Таким обра-
зом, хотя теоретически это является слабым местом, на практике проблем с pg_control не обна-
ружено.
735
