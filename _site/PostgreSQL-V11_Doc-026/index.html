<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Глава 26. Отказоустойчивость,балансировка нагрузки и репликация &#8211; Sirius Blog</title>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130427752-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130427752-1');
</script>

</head>
<meta name="description" content="">
<meta name="keywords" content="PostgreSQL, PostgreSQL_Book_11">

<!-- Twitter Cards -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/abstract-11.jpg">

<meta name="twitter:title" content="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация">
<meta name="twitter:description" content="">
<meta name="twitter:creator" content="@2hotab2">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация">
<meta property="og:description" content="">
<meta property="og:url" content="http://localhost:4000/PostgreSQL-V11_Doc-026/">
<meta property="og:site_name" content="Sirius Blog">





<link rel="canonical" href="http://localhost:4000/PostgreSQL-V11_Doc-026/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Sirius Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.jpg">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.jpg">
<!-- 114x72 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x72" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.jpg">
<!-- 144x72 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x72" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.jpg">



</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://localhost:4000/images/avatar.jpg" alt="Sergey Khatsiola photo" class="author-photo">
					<h4>Sergey Khatsiola</h4>
					<p>Кратко обо мне ...</p>
				</li>
				<li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:2hotab2@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				<li>
					<a href="https://twitter.com/2hotab2"><i class="fa fa-fw fa-twitter"></i> Twitter</a>
				</li>
				<li>
					<a href="https://facebook.com/sergej.ha1"><i class="fa fa-fw fa-facebook"></i> Facebook</a>
				</li>
				
				
				<li>
					<a href="https://github.com/Sergey-sirius"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    
	    <li><a href="http://localhost:4000/handbook/" >HandBook</a></li>
	  
	    
	    <li><a href="https://github.com/Sergey-sirius" target="_blank">Main Link</a></li>
	  
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  
  <div class="entry-image">
    <img src="http://localhost:4000/images/abstract-11.jpg" alt="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://localhost:4000/PostgreSQL-V11_Doc-026/" rel="bookmark" title="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация">Глава 26. Отказоустойчивость,балансировка нагрузки и репликация</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2018-12-01T00:00:00+02:00">December 01, 2018</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
Reading time ~57 minutes
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>Глава 26. Отказоустойчивость,балансировка нагрузки и репликация</p>

<p>Серверы базы данных могут работать совместно для обеспечения возможности быстрого переклю-
чения на другой сервер в случае отказа первого (отказоустойчивость) или для обеспечения воз-
можности нескольким серверам БД обрабатывать один набор данных (балансировка нагрузки). В
идеале, серверы БД могут работать вместе прозрачно для клиента. Веб-серверы, обрабатывающие
статические страницы, можно совместить достаточно легко посредством простого распределения
запросов на несколько машин. Фактически серверы баз данных только для чтения тоже могут быть
совмещены достаточно легко. К сожалению, большинство серверов баз данных получают смешан-
ные запросы на чтение/запись, а серверы с доступом на чтение/запись совместить гораздо слож-
нее. Это объясняется тем, что данные только для чтения достаточно единожды разместить на каж-
дом сервере, а запись на любой из серверов должна распространиться на все остальные серверы,
чтобы будущие запросы на чтение возвращали согласованные результаты.
Проблема синхронизации является главным препятствием для совместной работы серверов. Так
как единственного решения, устраняющего проблему синхронизации во всех случаях, не суще-
ствует, предлагается несколько решений. Разные решения подходят к проблеме по-разному и ми-
нимизируют её влияние в разных рабочих условиях.
Некоторые решения применяют синхронизацию, позволяя только одному серверу изменять дан-
ные. Сервер, который может изменять данные, называется сервером чтения/записи, ведущим или
главным сервером. Сервер, который отслеживает изменения на ведущем, называется ведомым
или резервным сервером. Резервный сервер, к которому нельзя подключаться до тех пор, пока
он не будет повышен до главного, называется сервером тёплого резерва, а тот, который может
принимать соединения и обрабатывать запросы только на чтение, называется сервером горячего
резерва.
Некоторые решения являются синхронными, при которых транзакция, модифицирующая данные,
не считается подтверждённой, пока все серверы не подтвердят транзакцию. Это гарантирует, что
при отработке отказа не произойдёт потеря данных и что все балансирующие серверы возвращают
целостные данные вне зависимости от того, к какому серверу был запрос. Асинхронное решение,
напротив, допускает некоторую задержку между временем подтверждения транзакции и её пере-
дачей на другие серверы, допуская возможность, что некоторые транзакции могут быть потеряны
в момент переключения на резервный сервер и что балансирующие серверы могут вернуть слегка
устаревшие данные. Асинхронная передача используется, когда синхронная будет слишком мед-
ленной.
Решения могут так же разделяться по степени детализации. Некоторые решения работают только
на уровне всего сервера БД целиком, в то время как другие позволяют работать на уровне таблиц
или уровне БД.
В любом случае необходимо принимать во внимание быстродействие. Обычно выбирается компро-
мисс между функциональностью и производительностью. Например, полностью синхронное реше-
ние в медленной сети может снизить производительность больше чем наполовину, в то время как
асинхронное решение будет оказывать минимальное воздействие.
В продолжении этого раздела рассматриваются различные решения по организации отказоустой-
чивости, репликации и балансировки нагрузки.
26.1. Сравнение различных решений
Отказоустойчивость на разделяемых дисках
Отказоустойчивость на разделяемых дисках позволяет избежать избыточности синхронизации
путём задействования только одной копии базы данных. Она использует единственный диско-
вый массив, который разделяется между несколькими серверами. Если основной сервер БД
откажет, резервный сервер может подключиться и запустить базу данных, что позволит вос-
становить БД после аварии. Это обеспечивает быстрое переключение без потери данных.
646Отказоустойчивость, баланси-
ровка нагрузки и репликация
Функциональность разделяемого оборудования обычно реализована в сетевых устройствах хра-
нения. Так же возможно применение сетевой файловой системы; особое внимание следует уде-
лить тому, чтобы поведение системы полностью соответствовало POSIX (см. Подраздел 18.2.2).
Существенное ограничение этого метода состоит в том, что в случае отказа или порчи разде-
ляемого дискового массива оба сервера: главный и резервный — станут нерабочими. Другая
особенность — резервный сервер никогда не получает доступ к разделяемым дискам во время
работы главного.
Репликация на уровне файловой системы (блочного устройства)
Видоизменённая версия функциональности разделяемого оборудования представлена в виде
репликации на уровне файловой системы, когда все изменения в файловой системе отражают-
ся в файловой системе другого компьютера. Единственное ограничение: синхронизация долж-
на выполняться методом, гарантирующим целостность копии файловой системы на резервном
сервере — в частности, запись на резервном сервере должна происходить в том же порядке,
что и на главном. DRBD является популярным решением на основе репликации файловой си-
стемы для Linux.
Трансляция журнала предзаписи
Серверы тёплого и горячего резерва могут так же поддерживаться актуальными путём чтения
потока записей из журнала изменений (WAL). Если основной сервер отказывает, резервный
содержит почти все данные с него и может быть быстро преобразован в новый главный сервер
БД. Это можно сделать синхронно или асинхронно, но может быть выполнено только на уровне
сервера БД целиком.
Резервный сервер может быть реализован с применением трансляции файлов журналов (см.
Раздел  26.2), или потоковой репликации (см. Подраздел  26.2.5), или их комбинацией. За ин-
формацией о горячем резерве обратитесь к Разделу 26.5.
Логическая репликация
В схеме с логической репликацией сервер баз данных может передавать поток изменений дан-
ных на другой сервер. Механизм логической репликации в PostgreSQL формирует поток логи-
ческих изменений данных, обрабатывая WAL. Логическая репликация позволяет переносить
изменения, происходящие только в отдельных таблицах. Для логической репликации не требу-
ется, чтобы за определённым сервером закреплялась роль главного или реплицирующего; на-
против, данные могут передаваться в разных направлениях. За дополнительными сведениями
о логической репликации обратитесь к Главе 31. Используя интерфейс логического декодиро-
вания (Глава 49), подобную функциональность могут предоставлять и сторонние расширения.
Репликация главный-резервный на основе триггеров
При репликации главный-резервный все запросы, изменяющие данные, пересылаются главно-
му серверу. Главный сервер, в свою очередь, асинхронно пересылает изменённые данные ре-
зервному. Резервный сервер может обрабатывать запросы только на чтение при работающем
главном. Такой резервный сервер идеален для обработки запросов к хранилищам данных.
Slony-I является примером подобного типа репликации, действующей на уровне таблиц, и под-
держивает множество резервных серверов. Так как обновления на резервных серверах проис-
ходят асинхронно (в пакетах), возможна потеря данных во время отказа.
Репликация запросов в среднем слое
В схеме с репликацией запросов в среднем слое, средний слой перехватывает каждый SQL-за-
прос и пересылает его на один или все серверы. Каждый сервер работает независимо. Модифи-
цирующие запросы должны быть направлены всем серверам, чтобы каждый из них получал все
изменения. Но читающие запросы могут быть посланы только на один сервер, что позволяет
перераспределить читающую нагрузку между всеми серверами.
Если запросы просто перенаправлять без изменений, функции подобные random(),
CURRENT_TIMESTAMP и последовательности могут получить различные значения на разных сер-
верах. Это происходит потому что каждый сервер работает независимо, а эти запросы неиз-
бирательные (и действительно не изменяют строки). Если такая ситуация недопустима, или
647Отказоустойчивость, баланси-
ровка нагрузки и репликация
средний слой, или приложение должно запросить подобные значения с одного сервера, затем
использовать его в других пишущих запросах. Другим способом является применения этого
вида репликации совместно с другим традиционным набором репликации главный-резервный,
то есть изменяющие данные запросы посылаются только на главный сервер, а затем применя-
ются на резервном в процессе этой репликации, но не с помощью реплицирующего среднего
слоя. Следует иметь в виду, что все транзакции фиксируются или прерываются на всех серве-
рах, возможно с применением двухфазной фиксации (см. PREPARE TRANSACTION и COMMIT
PREPARED). Репликацию такого типа реализуют, например Pgpool-II и Continuent Tungsten.
Асинхронная репликация с несколькими главными серверами
Если серверы не находятся постоянно в единой сети, как например, ноутбуки или удалённые
серверы, обеспечение согласованности данных между ними представляет проблему. Когда ис-
пользуется асинхронная репликация с несколькими главными серверами, каждый из них ра-
ботает независимо и периодически связывается с другими серверами для определения кон-
фликтующих транзакций. Конфликты могут урегулироваться пользователем или по правилам
их разрешения. Примером такого типа репликации является Bucardo.
Синхронная репликация с несколькими главными серверами
При синхронной репликации с несколькими главными серверами каждый сервер может прини-
мать запросы на запись, а изменённые данные передаются с начального сервера всем осталь-
ным, прежде чем транзакция будет подтверждена. Если запись производится интенсивно, это
может провоцировать избыточные блокировки, что приводит к снижению производительности.
На самом деле производительность при записи часто бывает хуже, чем с одним сервером. За-
просы на чтение также могут быть обработаны любым сервером. В некоторых конфигурациях
для более эффективного взаимодействия серверов применяются разделяемые диски. Синхрон-
ная репликация с несколькими главными серверами лучше всего работает, когда преобладают
операции чтения, хотя её большой плюс в том, что любой сервер может принимать запросы на
запись — нет необходимости искусственно разделять нагрузку между главным и резервными
серверами, а так как изменения передаются от одного сервера другим, не возникает проблем
с недетерминированными функциями вроде random().
PostgreSQL не предоставляет данный тип репликации, но так как PostgreSQL поддерживает
двухфазное подтверждение транзакции (PREPARE TRANSACTION и COMMIT PREPARED) такое
поведение может быть реализовано в коде приложения или среднего слоя.
Коммерческие решения
Так как PostgreSQL обладает открытым кодом и легко расширяется, некоторые компании взяли
за основу PostgreSQL и создали коммерческие решения с закрытым кодом со своими реализа-
циями свойств отказоустойчивости, репликации и балансировки нагрузки.
Таблица 26.1 итоговая таблица возможностей различных решений приведена ниже.
Таблица 26.1. Таблица свойств отказоустойчивости, балансировки нагрузки и репликации
Тип
Наиболее
типичные
реализа-
ции
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы
NAS DRBD
Трансля-
ция жур-
нала
предза-
писи
Логиче-
ская ре-
плика-
ция
Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров
встроен- встроен- Londiste,
ная пото- ная логи-
Slony
ковая ре-
ческая
пликация репли-
кация,
pglogical
648
Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
pgpool-II Bucardo  Отказоустойчивость, баланси-
ровка нагрузки и репликация
Тип
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы Трансля-
ция жур-
нала
предза-
писи Логиче-
ская ре-
плика-
ция Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
Метод
взаимо-
действия разде-
ляемые
диски дисковые
блоки WAL логиче-
ское де-
кодиро-
вание Строки
таблицы SQL Строки
таблицы Строки
табли-
цы и бло-
киров-
ки строк
Не требу-
ется спе-
циально-
го обору-
дования   • • • • • • •
Допуска-
ется
несколь-
ко глав-
ных сер-
веров       •   • • •
Нет избы-
точности
главного
сервера •   • •   •  <br />
Нет
за-
держки
при
несколь-
ких серве-
рах •   без
синхр. без
синхр. •   •<br />
Отказ
главного
сервера
не
мо-
жет при-
вести
к
потере
данных • • с синхр. с синхр.   •   •
Сервер
реплики
принима-
ет чита-
ющие за-
просы     с горя-
чим ре-
зервом • • • • •
Реплика-
ция
на
уровне
таблиц       • •   • •
649Отказоустойчивость, баланси-
ровка нагрузки и репликация
Тип
Не требу-
ется раз-
решение
конфлик-
тов
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы Трансля-
ция жур-
нала
предза-
писи Логиче-
ская ре-
плика-
ция Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
• • •   •     •
Несколько решений, которые не подпадают под указанные выше категории:
Секционирование данных
При секционировании таблицы расщепляются на наборы данных. Каждый из наборов может
быть изменён только на одном сервере. Например, данные могут быть секционированы по офи-
сам, например, Лондон и Париж, с сервером в каждом офисе. В случае необходимости обра-
щения одновременно к данным Лондона и Парижа, приложение может запросить оба сервера,
или может быть применена репликация главный-резервный для предоставления копии только
для чтения в другом офисе для каждого из серверов.
Выполнение параллельных запросов на нескольких серверах
Многие из указанных выше решений позволяют обрабатывать несколько запросов на несколь-
ких серверах, но ни один из них не может обрабатывать один запрос с применением нескольких
серверов для уменьшения общего времени выполнения. Подобное решение позволяет несколь-
ким серверам обрабатывать один запрос одновременно. Такое обычно достигается путём раз-
деления данных между серверами, обработкой на сервере своей части запроса с возвратом ре-
зультата на центральный сервер. Там данные проходят окончательную обработку и возвраща-
ются пользователю. Pgpool-II предоставляет такую возможность. Так же это может быть реа-
лизовано с применением набора средств PL/Proxy.
26.2. Трансляция журналов на резервные серверы
Постоянная архивация может использоваться для создания кластерной конфигурации высокой
степени доступности (HA) с одним или несколькими резервными серверами, способными заме-
нить ведущий сервер в случае выхода его из строя. Такую реализацию отказоустойчивости часто
называют тёплый резерв или трансляция журналов.
Ведущий и резервный серверы работают совместно для обеспечения этой возможности, при этом
они связаны опосредованно. Ведущий сервер работает в режиме постоянной архивации измене-
ний, в то время как каждый резервный сервер работает в режиме постоянного приема архивных
изменений, зачитывая WAL-файлы с ведущего. Для обеспечения этой возможности не требуется
вносить изменения в таблицы БД, что требует существенно меньших административных издержек
в сравнении с некоторыми другими решениями репликации. Так же такая конфигурация относи-
тельно слабо влияет на производительность ведущего сервера.
Непосредственную передачу записей WAL с одного сервера БД на другой обычно называют транс-
ляцией журналов (или доставкой журналов). PostgreSQL реализует трансляцию журналов на уров-
не файлов, передавая записи WAL по одному файлу (сегменту WAL) единовременно. Файлы WAL
(размером 16 МБ) можно легко и эффективно передать на любое расстояние, будь то соседний
сервер, другая система в местной сети или сервер на другом краю света. Требуемая пропускная
способность при таком подходе определяется скоростью записи транзакций на ведущем сервере.
Трансляция журналов на уровне записей более фрагментарная операция, при которой изменения
WAL передаются последовательно через сетевое соединение (см. Подраздел 26.2.5).
650Отказоустойчивость, баланси-
ровка нагрузки и репликация
Следует отметить, что трансляция журналов асинхронна, то есть записи WAL доставляются после
завершения транзакции. В результате образуется окно, когда возможна потеря данных при отказе
сервера: будут утеряны ещё не переданные транзакции. Размер этого окна при трансляции файлов
журналов может быть ограничен параметром archive_timeout, который может принимать значе-
ние меньше нескольких секунд. Тем не менее подобные заниженные значения могут потребовать
существенного увеличения пропускной способности, необходимой для трансляции файлов. При
потоковой репликации (см. Подраздел 26.2.5) окно возможности потери данных гораздо меньше.
Скорость восстановления достаточно высока, обычно резервный сервер становится полностью до-
ступным через мгновение после активации. В результате такое решение называется тёплым ре-
зервом, что обеспечивает отличную отказоустойчивость. Восстановление сервера из архивной ко-
пии базы и применение изменений обычно происходит существенно дольше. Поэтому такие дей-
ствия обычно требуются при восстановлении после аварии, не для отказоустойчивости. Так же
резервный сервер может обрабатывать читающие запросы. В этом случае он называется сервером
горячего резерва. См. Раздел 26.5 для подробной информации.
26.2.1. Планирование
Обычно разумно подбирать ведущий и резервный серверы так, чтобы они были максимально по-
хожи, как минимум с точки зрения базы данных. Тогда в частности, пути, связанные с табличны-
ми пространствами, могут передаваться без изменений. Таким образом, как на ведущем, так и на
резервных серверах должны быть одинаковые пути монтирования для табличных пространств при
использовании этой возможности БД. Учитывайте, что если CREATE TABLESPACE выполнена на
ведущем сервере, новая точка монтирования для этой команды уже должна существовать на ре-
зервных серверах до её выполнения. Аппаратная часть не должна быть в точности одинаковой, но
опыт показывает, что сопровождать идентичные системы легче, чем две различные на протяже-
нии жизненного цикла приложения и системы. В любом случае архитектура оборудования должна
быть одинаковой — например, трансляция журналов с 32-битной на 64-битную систему не будет
работать.
В общем случае трансляция журналов между серверами с различными основными версиями
PostgreSQL невозможна. Политика главной группы разработки PostgreSQL состоит в том, чтобы
не вносить изменения в дисковые форматы при обновлениях корректирующей версии, таким об-
разом, ведущий и резервный серверы, имеющие разные корректирующие версии, могут работать
успешно. Тем не менее, формально такая возможность не поддерживается и рекомендуется под-
держивать одинаковую версию ведущего и резервных серверов, насколько это возможно. При об-
новлении корректирующей версии безопаснее будет в первую очередь обновить резервные серве-
ры — новая корректирующая версия с большей вероятностью прочитает файл WAL предыдущей
корректирующей версии, чем наоборот.
26.2.2. Работа резервного сервера
Сервер, работающий в режиме резервного, последовательно применяет файлы WAL, полученные
от главного. Резервный сервер может читать файлы WAL из архива WAL (см. restore_command)
или напрямую с главного сервера по соединению TCP (потоковая репликация). Резервный сервер
также будет пытаться восстановить любой файл WAL, найденный в кластере резервного в каталоге
pg_wal. Это обычно происходит после перезапуска сервера, когда он применяет заново файлы WAL,
полученные от главного сервера перед перезапуском. Но можно и вручную скопировать файлы в
каталог pg_wal, чтобы применить их в любой момент времени.
В момент запуска резервный сервер начинает восстанавливать все доступные файлы WAL, раз-
мещённые в архивном каталоге, указанном в команде restore_command. По достижении конца до-
ступных файлов WAL или при сбое команды restore_command сервер пытается восстановить все
файлы WAL, доступные в каталоге pg_wal. Если это не удаётся и потоковая репликация настрое-
на, резервный сервер пытается присоединиться к ведущему и начать закачивать поток WAL с по-
следней подтверждённой записи, найденной в архиве или pg_wal. Если это действие закончилось
неудачей, или потоковая репликация не настроена, или соединение позднее разорвалось, резерв-
ный сервер возвращается к шагу 1 и пытается восстановить файлы из архива вновь. Цикл обраще-
651Отказоустойчивость, баланси-
ровка нагрузки и репликация
ния за файлами WAL к архиву, pg_wal, и через потоковую репликацию продолжается до остановки
сервера или переключения его роли, вызванного файлом-триггером.
Режим резерва завершается и сервер переключается в обычный рабочий режим при получении
команды pg_ctl promote или при обнаружении файла-триггера (trigger_file). Перед переклю-
чением сервер восстановит все файлы WAL, непосредственно доступные из архива или pg_wal, но
пытаться подключиться к главному серверу он больше не будет.
26.2.3. Подготовка главного сервера для работы с резервными
Настройка постоянного архивирования на ведущем сервере в архивный каталог, доступный с ре-
зервного, описана в Разделе 25.3. Расположение архива должно быть доступно с резервного сер-
вера даже при отключении главного, то есть его следует разместить на резервном или другом до-
веренном, но не на главном сервере.
При использовании потоковой репликации следует настроить режим аутентификации на ведущем
сервере, чтобы разрешить соединения с резервных. Для этого создать роль и обеспечить подходя-
щую запись в файле pg_hba.conf в разделе доступа к БД replication. Так же следует убедиться,
что для параметра max_wal_senders задаётся достаточно большое значение в конфигурационном
файле ведущего сервера. При использовании слотов для репликации также достаточно большое
значение нужно задать для max_replication_slots.
Создание базовой резервной копии, необходимой для запуска резервного сервера, описано в Под-
разделе 25.3.2.
26.2.4. Настройка резервного сервера
Для запуска резервного сервера нужно восстановить резервную копию, снятую с ведущего (см.
Подраздел 25.3.4). Затем нужно создать файл команд восстановления recovery.conf в каталоге
данных кластера резервного сервера и включить режим standby_mode. Задайте в restore_command
обычную команду копирования файлов из архива WAL. Если планируется несколько резервных
серверов в целях отказоустойчивости, установите для recovery_target_timeline значение latest,
чтобы резервный сервер переходил на новую линию времени, образуемую при отработке отказа
и переключении на другой сервер.
Примечание
Не используйте pg_standby или подобные средства со встроенным режимом резервно-
го сервера, описанным здесь. restore_command должна немедленно прекратиться при
отсутствии файла; сервер повторит команду вновь при необходимости. Использование
средств, подобных pg_standby, описано в Разделе 26.4.
При необходимости потоковой репликации заполните primary_conninfo параметрами строки со-
единения для libpq, включая имя (или IP-адрес) сервера и все остальные необходимые данные для
соединения с ведущим сервером. Если ведущий требует пароль для аутентификации, пароль мо-
жет быть так же передан в primary_conninfo.
Если резервный сервер настраивается в целях отказоустойчивости, на нём следует настроить ар-
хивацию WAL, соединения и аутентификацию, как на ведущем сервере, потому что резервный сер-
вер станет ведущим после отработки отказа.
При использовании архива WAL его размер может быть уменьшен с помощью команды в па-
раметре archive_cleanup_command, которая удаляет файлы уже не нужные для дальнейшей ра-
боты резервного сервера. Утилита pg_archivecleanup разработана специально для использова-
ния в archive_cleanup_command при типичной конфигурации с одним резервным сервером (см.
pg_archivecleanup). Следует отметить, что если архив используется в целях резервирования, сле-
дует сохранять все файлы необходимые для восстановления как минимум с последней базовой ре-
зервной копии, даже если они не нужны для резервного сервера.
652Отказоустойчивость, баланси-
ровка нагрузки и репликация
Простой пример recovery.conf:
standby_mode = ‘on’
primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
restore_command = ‘cp /path/to/archive/%f %p’
archive_cleanup_command = ‘pg_archivecleanup /path/to/archive %r’
Можно поддерживать любое количество резервных серверов, но при применении потоковой ре-
пликации необходимо убедиться, что значение max_wal_senders на ведущем достаточно большое,
чтобы все они могли подключиться одновременно.
26.2.5. Потоковая репликация
При потоковой репликации резервный сервер может работать с меньшей задержкой, чем при
трансляции файлов. Резервный сервер подключается к ведущему, который передаёт поток запи-
сей WAL резервному в момент их добавления, не дожидаясь окончания заполнения файла WAL.
Потоковая репликация асинхронна по умолчанию (см. Подраздел 26.2.8), то есть имеется неболь-
шая задержка между подтверждением транзакции на ведущем сервере и появлением этих изме-
нений на резервном. Тем не менее, эта задержка гораздо меньше, чем при трансляции файлов
журналов, обычно в пределах одной секунды, если резервный сервер достаточно мощный и справ-
ляется с нагрузкой. При потоковой репликации настраивать archive_timeout для уменьшения ок-
на потенциальной потери данных не требуется.
При потоковой репликации без постоянной архивации на уровне файлов, сервер может избавить-
ся от старых сегментов WAL до того, как резервный получит их. В этом случае резервный сервер
потребует повторной инициализации из новой базовой резервной копии. Этого можно избежать,
установив для wal_keep_segments достаточно большое значение, при котором сегменты WAL бу-
дут защищены от ранней очистки, либо настроив слот репликации для резервного сервера. Если
с резервного сервера доступен архив WAL, этого не требуется, так как резервный может всегда
обратиться к архиву для восполнения пропущенных сегментов.
Чтобы включить потоковую репликацию, сначала настройте резервный сервер на приём трансля-
ции журналов, как описано в Разделе  26.2. Затем сделайте следующий шаг — переключите ре-
зервный сервер в режим репликации, установив в primary_conninfo в файле recovery.conf стро-
ку подключения, указывающую на ведущий. Настройте listen_addresses и параметры аутентифи-
кации (см. pg_hba.conf) на ведущем сервере таким образом, чтобы резервный смог подключиться
к псевдобазе replication на ведущем (см. Подраздел 26.2.5.1).
В
системах,
поддерживающих
параметр
сокета
keepalive,
подходящие
значения
tcp_keepalives_idle, tcp_keepalives_interval и tcp_keepalives_count помогут ведущему вовремя заме-
тить разрыв соединения.
Установите максимальное количество одновременных соединений с резервных серверов (см. опи-
сание max_wal_senders.
При запуске резервного сервера с правильно установленным primary_conninfo резервный под-
ключится к ведущему после воспроизведения всех файлов WAL, доступных из архива. При успеш-
ном установлении соединения можно увидеть процесс walreceiver на резервном сервере и соот-
ветствующий процесс walsender на ведущем.
26.2.5.1. Аутентификация
Право использования репликации очень важно ограничить так, чтобы только доверенные поль-
зователи могли читать поток WAL, так как из него можно извлечь конфиденциальную информа-
цию. Резервный сервер должен аутентифицироваться на ведущем от имени суперпользователя или
пользователя с правом REPLICATION. Настоятельно рекомендуется создавать выделенного пользо-
вателя с правами REPLICATION и LOGIN специально для репликации. Хотя право REPLICATION даёт
очень широкие полномочия, оно не позволяет модифицировать данные в ведущей системе, тогда
как с правом SUPERUSER это можно делать.
653Отказоустойчивость, баланси-
ровка нагрузки и репликация
Список аутентификации клиентов для репликации содержится в pg_hba.conf в записях с установ-
ленным значением replication в поле database. Например, если резервный сервер запущен на
компьютере с IP-адресом 192.168.1.100 и учётная запись для репликации foo, администратор мо-
жет добавить следующую строку в файл pg_hba.conf ведущего:</p>
<h1 id="Разрешить-пользователю-foo-с-компьютера-1921681100-подключаться-к-этому">Разрешить пользователю “foo” с компьютера 192.168.1.100 подключаться к этому</h1>
<h1 id="серверу-в-качестве-партнёра-репликации-если-был-передан-правильный-пароль">серверу в качестве партнёра репликации, если был передан правильный пароль.</h1>
<p>#</p>
<h1 id="type-database">TYPE DATABASE</h1>
<p>USER
ADDRESS
METHOD
host
replication
foo
192.168.1.100/32
md5
Имя компьютера и номер порта для ведущего, имя пользователя для соединения и пароль указы-
ваются в файле recovery.conf. Пароль так же может быть задан через файл ~/.pgpass на резерв-
ном сервере (указанном в определении с replication в поле database). Например, если ведущий
принимает подключения по IP-адресу 192.168.1.50, в порту 5432, пользователя для репликации
foo с паролем foopass, то администратор может добавить следующую строку в файл recovery.conf
на резервном сервере:</p>
<h1 id="Резервный-сервер-подключается-к-ведущему-работающему-на-компьютере-192168150">Резервный сервер подключается к ведущему, работающему на компьютере 192.168.1.50</h1>
<h1 id="порт-5432-от-имени-пользователя-foo-с-паролем-foopass">(порт 5432), от имени пользователя “foo” с паролем “foopass”.</h1>
<p>primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
26.2.5.2. Наблюдение
Важным индикатором стабильности работы потоковой репликации является количество запи-
сей WAL, созданных на ведущем, но ещё не применённых на резервном сервере. Задержку мож-
но подсчитать, сравнив текущую позиции записи WAL на ведущем с последней позицией WAL,
полученной на резервном сервере. Эти позиции можно узнать, воспользовавшись функциями
pg_current_wal_lsn на ведущем и pg_last_wal_receive_lsn на резервном, соответственно (за по-
дробностями обратитесь к Таблице 9.79 и Таблице 9.80). Последняя полученная позиция WAL на
резервном сервере также выводится в состоянии процесса-приёмника WAL, которое показывает
команда ps (подробнее об этом в Разделе 28.1).
Список процессов-передатчиков WAL можно получить через представление pg_stat_replication.
Значительная разница между pg_current_wal_lsn и полем sent_lsn этого представления может
указывать на то, что главный сервер работает с большой нагрузкой, тогда как разница между
sent_lsn и pg_last_wal_receive_lsn на резервном может быть признаком задержек в сети или
большой нагрузки резервного сервера.
На сервере горячего резерва состояние процесса-приёмника WAL можно получить через
представление pg_stat_wal_receiver. Большая разница между pg_last_wal_replay_lsn и полем
received_lsn свидетельствует о том, что WAL поступает быстрее, чем удаётся его воспроизвести.
26.2.6. Слоты репликации
Слоты репликации автоматически обеспечивают механизм сохранения сегментов WAL, пока они
не будут получены всеми резервными и главный сервер не будет удалять строки, находящиеся в
статусе recovery conflict даже при отключении резервного.
Вместо использования слотов репликации для предотвращения удаления старых сегментов WAL
можно применять wal_keep_segments, или сохранять сегменты в архиве с помощью команды
archive_command. Тем не менее, эти методы часто приводят к тому, что хранится больше сегмен-
тов WAL, чем необходимо, в то время как слоты репликации оставляют только то количество сег-
ментов, которое необходимо. Преимущество этих методов состоит в том, что они чётко задают
объёмы места, необходимого для pg_wal; в то время как текущая реализация слотов репликации
не предоставляет такой возможности.
Подобным образом, параметры hot_standby_feedback и vacuum_defer_cleanup_age позволяют защи-
тить востребованные строки от удаления при очистке, но первый параметр не защищает в тот про-
межуток времени, когда резервный сервер не подключён, а для последнего часто нужно задавать
большое значение, чтобы обеспечить должную защиту. Слоты репликации решают эти проблемы.
654Отказоустойчивость, баланси-
ровка нагрузки и репликация
26.2.6.1. Запросы и действия слотов репликации
Каждый слот репликации обладает именем, состоящим из строчных букв, цифр и символов под-
чёркивания.
Имеющиеся слоты репликации
pg_replication_slots.
и
их
статус
можно
просмотреть
в
представлении
Слоты могут быть созданы и удалены как с помощью протокола потоковой репликации (см. Раз-
дел 53.4), так и посредством функций SQL (см. Подраздел 9.26.6).
26.2.6.2. Пример конфигурации
Для создания слота репликации выполните:
postgres=# SELECT * FROM pg_create_physical_replication_slot(‘node_a_slot’);
slot_name | lsn
————-+—–
node_a_slot |
postgres=# SELECT slot_name, slot_type, active FROM pg_replication_slots;
slot_name | slot_type | active
————-+———–+——–
node_a_slot | physical | f
(1 row)
Для настройки резервного сервера на использование этого слота primary_slot_name должно быть
настроено в конфигурации recovery.conf резервного. Вот простейший пример:
standby_mode = ‘on’
primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
primary_slot_name = ‘node_a_slot’
26.2.7. Каскадная репликация
Свойство каскадной репликации позволяет резервному серверу принимать соединения реплика-
ции и потоки WAL от других резервных, выступающих посредниками. Это может быть полезно для
уменьшения числа непосредственных подключений к главному серверу, а также для уменьшения
накладных расходов при передаче данных в интрасети.
Резервный сервер, выступающий как получатель и отправитель, называется каскадным резерв-
ным сервером. Резервные серверы, стоящие ближе к главному, называются серверами верхнего
уровня, а более отдалённые — серверами нижнего уровня. Каскадная репликация не накладыва-
ет ограничений на количество или организацию последующих уровней, а каждый резервный со-
единяется только с одним сервером вышестоящего уровня, который в конце концов соединяется
с единственным главным/ведущим сервером.
Резервный сервер каскадной репликации не только получает записи WAL от главного, но так же
восстанавливает их из архива. Таким образом, даже если соединение с сервером более высокого
уровня разорвётся, потоковая репликация для последующих уровней будет продолжаться до ис-
черпания доступных записей WAL.
Каскадная репликация в текущей реализации асинхронна. Параметры синхронной репликации
(см. Подраздел 26.2.8) в настоящее время не оказывают влияние на каскадную репликацию.
Распространение обратной связи горячего резерва работает от нижестоящего уровня к вышесто-
ящему уровню вне зависимости от способа организации связи.
Если резервный сервер вышестоящего уровня будет преобразован в новый главный, серве-
ры нижестоящего уровня продолжат получать поток с нового главного при условии, что
recovery_target_timeline установлен в значение ‘latest’.
655Отказоустойчивость, баланси-
ровка нагрузки и репликация
Для использования каскадной репликации необходимо настроить резервный каскадный сервер
на прием соединений репликации (то есть установить max_wal_senders и hot_standby, настроить
host-based authentication). Так же может быть необходимо настроить на нижестоящем резервном
значение primary_conninfo на каскадный резервный сервер.
26.2.8. Синхронная репликация
По умолчанию в PostgreSQL потоковая репликация асинхронна. Если ведущий сервер выходит из
строя, некоторые транзакции, которые были подтверждены, но не переданы на резервный, могут
быть потеряны. Объём потерянных данных пропорционален задержке репликации на момент от-
работки отказа.
Синхронная репликация предоставляет возможность гарантировать, что все изменения, внесён-
ные в транзакции, были переданы одному или нескольким синхронным резервным серверам. Это
увеличивает стандартный уровень надёжности, гарантируемый при фиксации транзакции. Этот
уровень защиты соответствует второму уровню безопасности репликации из теории вычислитель-
ной техники, или групповой безопасности первого уровня (безопасности групповой и уровня 1),
когда выбран режим synchronous_commit remote_write.
При синхронной репликации каждая фиксация пишущей транзакции ожидает подтверждения то-
го, что запись фиксации помещена в журнал предзаписи на диске на обоих серверах: ведущем и
резервном. При таком варианте потеря данных может произойти только в случае одновременного
выхода из строя ведущего и резервного серверов. Это обеспечивает более высокий уровень надёж-
ности, при условии продуманного подхода системного администратора к вопросам размещения и
управления этими серверами. Ожидание подтверждения увеличивает уверенность в том, что дан-
ные не будут потеряны во время сбоя сервера, но при этом увеличивает время отклика для обра-
ботки транзакции. Минимальное время ожидания равно времени передачи данных от ведущего к
резервному и обратно.
Транзакции только для чтения и откат транзакции не требуют ожидания для ответа с резервного
сервера. Промежуточные подтверждения не ожидают ответа от резервного сервера, только под-
тверждение верхнего уровня. Долгие операции вида загрузки данных или построения индекса
не ожидают финального подтверждения. Но все двухфазные подтверждения требуют ожидания,
включая подготовку и непосредственно подтверждение.
Синхронным резервным сервером может быть резервный сервер при физической репликации или
подписчик при логической репликации. Это также может быть другой потребитель потока логи-
ческой или физической репликации, способный отправлять в ответ требуемые сообщения. Поми-
мо встроенных систем логической и физической репликации, к таким потребителям относятся
специальные программы, pg_receivewal и pg_recvlogical, а также некоторые сторонние систе-
мы репликации и внешние программы. Подробнее об организации синхронной репликации с их
использованием можно узнать в соответствующей документации.
26.2.8.1. Базовая настройка
При настроенной потоковой репликации установка синхронной репликации требует только допол-
нительной настройки: необходимо выставить synchronous_standby_names в непустое значение. Так
же необходимо установить synchronous_commit в значение on, но так как это значение по умолча-
нию, обычно действий не требуется. (См. Подраздел 19.5.1 и Подраздел 19.6.2.) В такой конфигу-
рации каждая транзакция будет ожидать подтверждение того, что на резервном сервере произо-
шла запись транзакции в надёжное хранилище. Значение synchronous_commit может быть выстав-
лено для отдельного пользователя, может быть прописано в файле конфигурации, для конкретного
пользователя или БД или динамически изменено приложением для управления степенью надёж-
ности на уровне отдельных транзакций.
После сохранения записи о фиксации транзакции на диске ведущего сервера эта запись WAL пере-
даётся резервному серверу. Резервный сервер отвечает подтверждающим сообщением после со-
хранения каждого нового блока данных WAL на диске, если только wal_receiver_status_interval
на нём не равен нулю. В случае, когда выбран режим synchronous_commit remote_apply, резерв-
ный сервер передаёт подтверждение после воспроизведения записи фиксации, когда транзакция
656Отказоустойчивость, баланси-
ровка нагрузки и репликация
становится видимой. Если резервный сервер выбран на роль синхронного резервного в соответ-
ствии со значением synchronous_standby_names на ведущем, подтверждающие сообщения с этого
сервера, в совокупности с сообщениями с других синхронных серверов, будут сигналом к завер-
шению ожидания при фиксировании транзакций, требующих подтверждения сохранения записи
фиксации. Эти параметры позволяют администратору определить, какие резервные серверы будут
синхронными резервными. Заметьте, что настройка синхронной репликации в основном осуществ-
ляется на главном сервере. Перечисленные в списке резервных серверы должны быть подключе-
ны к нему непосредственно; он ничего не знает о резервных серверах, подключённых каскадно,
через промежуточные серверы.
Если synchronous_commit имеет значение remote_write, то в случае подтверждения транзакции
ответ от резервного сервера об успешном подтверждении будет передан, когда данные запишутся
в операционной системе, но не когда данные будет реально сохранены на диске. При таком зна-
чении уровень надёжности снижается по сравнению со значением on. Резервный сервер может
потерять данные в случае падения операционной системы, но не в случае падения PostgreSQL.
Тем не менее, этот вариант полезен на практике, так как позволяет сократить время отклика для
транзакции. Потеря данных может произойти только в случае одновременного сбоя ведущего и
резервного, осложнённого повреждением БД на ведущем.
Если synchronous_commit имеет значение remote_apply, то для завершения фиксирования тран-
закции потребуется дождаться, чтобы текущие синхронные резервные серверы сообщили, что они
воспроизвели транзакцию и её могут видеть запросы пользователей. В простых случаях это позво-
ляет обеспечить обычный уровень согласованности и распределение нагрузки.
Пользователи прекратят ожидание в случае запроса на быструю остановку сервера. В то время
как при использовании асинхронной репликации сервер не будет полностью остановлен, пока все
исходящие записи WAL не переместятся на текущий присоединённый резервный сервер.
26.2.8.2. Несколько синхронных резервных серверов
Синхронная репликация поддерживает применение одного или нескольких синхронных резерв-
ных серверов; транзакции будут ждать, пока все резервные серверы, считающиеся синхронными,
не подтвердят получение своих данных. Число синхронных резервных серверов, от которых тран-
закции должны ждать подтверждения, задаётся в параметре synchronous_standby_names. В этом
параметре также задаётся список имён резервных серверов и метод (FIRST или ANY) выбора син-
хронных из заданного списка.
С методом FIRST производится синхронная репликация на основе приоритетов, когда транзакции
фиксируются только после того, как их записи в WAL реплицируются на заданное число синхрон-
ных резервных серверов, выбираемых согласно приоритетам. Серверы, имена которых идут в на-
чале списка, имеют больший приоритет и выбираются на роль синхронных. Другие резервные сер-
веры, идущие в этом списке за ними, считаются потенциальными синхронными. Если один из те-
кущих синхронных резервных серверов по какой-либо причине отключается, он будет немедленно
заменён следующим по порядку резервным сервером.
Пример значения synchronous_standby_names для нескольких синхронных резервных серверов,
выбираемых по приоритетам:
synchronous_standby_names = ‘FIRST 2 (s1, s2, s3)’
В данном примере, если работают четыре резервных сервера s1, s2, s3 и s4, два сервера s1 и s2
будут выбраны на роль синхронных резервных, так как их имена идут в начале этого списка. Сервер
s3 будет потенциальным резервным и возьмёт на себя роль синхронного резервного при отказе s1
или s2. Сервер s4 будет асинхронным резервным, так как его имя в этом списке отсутствует.
С методом ANY производится синхронная репликация на основе кворума, когда транзакции фикси-
руются только после того, как их записи в WAL реплицируются на как минимум заданное число
синхронных серверов в списке.
Пример значения synchronous_standby_names для нескольких синхронных резервных серверов,
образующих кворум:
657Отказоустойчивость, баланси-
ровка нагрузки и репликация
synchronous_standby_names = ‘ANY 2 (s1, s2, s3)’
В данном примере, если работают четыре резервных сервера s1, s2, s3 и s4, транзакции будут
фиксироваться только после получения ответов как минимум от двух резервных серверов из s1, s2
и s3. Сервер s4 будет асинхронным резервным, так как его имя в этом списке отсутствует.
Состояние
синхронности
pg_stat_replication.
резервных
серверов
можно
увидеть
в
представлении
26.2.8.3. Планирование производительности
Организуя синхронную репликацию, обычно нужно обстоятельно обдумать конфигурацию и раз-
мещение резервных серверов, чтобы обеспечить приемлемую производительность приложений.
Ожидание не потребляет системные ресурсы, но блокировки транзакций будут сохраняться до
подтверждения передачи. Как следствие, непродуманное использование синхронной репликации
приведёт к снижению производительности БД из-за увеличения времени отклика и числа кон-
фликтов.
PostgreSQL позволяет разработчикам выбрать требуемый уровень надёжности, обеспечиваемый
при репликации. Он может быть установлен для системы в целом, для отдельного пользователя
или соединения или даже для отдельной транзакции.
Например, в рабочей нагрузке приложения 10% изменений могут относиться к важным данным
клиентов, а 90% — к менее критичным данным, потеряв которые, бизнес вполне сможет выжить
(например, это могут быть текущие разговоры пользователей между собой).
При настройке уровня синхронности репликации на уровне приложения (на ведущем) можно за-
дать синхронную репликацию для большинства важных изменений без замедления общего рабо-
чего ритма. Возможность настройки на уровне приложения является важным и практичным сред-
ством для получения выгод синхронной репликации при высоком быстродействии.
Следует иметь в виду, что пропускная способность сети должна быть больше скорости генериро-
вания данных WAL.
26.2.8.4. Планирование отказоустойчивости
В synchronous_standby_names задаётся количество и имена синхронных резервных серверов,
от которых будет ожидаться подтверждение при фиксировании транзакции, когда параметру
synchronous_commit присвоено значение on, remote_apply или remote_write. Фиксирование тран-
закции в таком режиме может не завершиться никогда, если один из синхронных резервных сер-
веров выйдет из строя.
Поэтому для высокой степени доступности лучше всего обеспечить наличие синхронных резерв-
ных серверов в должном количестве. Для этого можно перечислить несколько потенциальных ре-
зервных серверов в строке synchronous_standby_names.
При синхронной репликации на основе приоритетов синхронными резервными серверами станут
серверы, имена которых стоят в этом списке первыми. Следующие за ними серверы будут стано-
виться синхронными резервными при отказе одного из текущих.
При синхронной репликации на основе кворума кандидатами на роль синхронных резервных будут
все серверы в списке. И если один из них откажет, другие серверы будут продолжать исполнять
эту роль.
Когда к ведущему серверу впервые присоединяется резервный, он ещё не будет полностью синхро-
низированным. Это называется состоянием навёрстывания. Как только отставание резервного от
ведущего сервера сократится до нуля в первый раз, система перейдет в состояние потоковой пе-
редачи в реальном времени. Сразу после создания резервного сервера навёрстывание может быть
длительным. В случае выключения резервного сервера длительность этого процесса увеличится
соответственно продолжительности простоя. Резервный сервер может стать синхронным только
658Отказоустойчивость, баланси-
ровка нагрузки и репликация
по достижении состояния потоковой передачи. Это состояние можно проследить в представлении
pg_stat_replication.
Если ведущий сервер перезапускается при наличии зафиксированных транзакций, ожидающих
подтверждения, эти транзакции будут помечены как полностью зафиксированные после восста-
новления ведущего. При этом нельзя гарантировать, что все резервные серверы успели получить
все текущие данные WAL к моменту падения ведущего. Таким образом, некоторые транзакции мо-
гут считаться незафиксированными на резервном сервере, даже если они считаются зафиксиро-
ванными на ведущем. Гарантия, которую мы можем дать, состоит в том, что приложение не полу-
чит явного подтверждения успешной фиксации, пока не будет уверенности, что данные WAL по-
лучены всеми синхронными резервными серверами.
Если запустить синхронные резервные серверы в указанном количестве не удаётся, вам следует
уменьшить число синхронных серверов, подтверждения которых требуются для завершения фик-
сации транзакций, в параметре synchronous_standby_names (или вовсе отключить его) и переза-
грузить файл конфигурации на ведущем сервере.
В случае если ведущий сервер стал недоступным для оставшихся резервных, следует переклю-
читься на наиболее подходящий из имеющихся резервных серверов.
Если необходимо пересоздать резервный сервер при наличии ожидающей подтверждения тран-
закции необходимо убедиться, что команды pg_start_backup() и pg_stop_backup() запускаются в
сессии с установленным synchronous_commit = off, в противном случае эти запросы на подтвер-
ждение будут бесконечными для вновь возникшего резервного сервера.
26.2.9. Непрерывное архивирование на резервном сервере
Когда на резервном сервере применяется последовательное архивирование WAL, возможны два
различных сценария: архив WAL может быть общим для ведущего и резервного сервера, либо ре-
зервный сервер может иметь собственный архив WAL. Когда резервный работает с собственным
архивом WAL, установите в archive_mode значение always, и он будет вызывать команду архива-
ции для каждого сегмента WAL, который он получает при восстановлении из архива или потоко-
вой репликации. В случае с общим архивом можно поступить аналогично, но archive_command
должна проверять, нет ли в архиве файла, идентичного архивируемому. Таким образом, команда
archive_command должна позаботиться о том, чтобы существующий файл не был заменён файлом
с другим содержимым, а в случае попытки повторного архивирования должна сообщать об успеш-
ном выполнении. При этом все эти действия должны быть рассчитаны на условия гонки, возмож-
ные, если два сервера попытаются архивировать один и тот же файл одновременно.
Если в archive_mode установлено значение on, архивация в режиме восстановления или резерва не
производится. В случае повышения резервного сервера, он начнёт архивацию после повышения,
но в архив не попадут те файлы WAL, которые генерировал не он сам. Поэтому, чтобы в архиве
оказался полный набор файлов WAL, необходимо обеспечить архивацию всех файлов WAL до того,
как они попадут на резервный сервер. Это естественным образом происходит при трансляции фай-
лов журналов, так как резервный сервер может восстановить только файлы, которые находятся в
архиве, однако при потоковой репликации это не так. Когда сервер работает не в режиме резерва,
различий между режимами on и always нет.
26.3. Отработка отказа
Если ведущий сервер отказывает, резервный должен начать процедуры отработки отказа.
Если отказывает резервный сервер, никакие действия по отработке отказа не требуются. Если ре-
зервный сервер будет перезапущен, даже через некоторое время, немедленно начнётся операция
восстановления, благодаря возможности возобновляемого восстановления. Если вернуть резерв-
ный сервер в строй невозможно, необходимо создать полностью новый экземпляр резервного сер-
вера.
Когда ведущий сервер отказывает и резервный сервер становится новым ведущим, а затем старый
ведущий включается снова, необходим механизм для предотвращения возврата старого к роли
659Отказоустойчивость, баланси-
ровка нагрузки и репликация
ведущего. Иногда его называют STONITH (Shoot The Other Node In The Head, «Выстрелите в голову
другому узлу»), что позволяет избежать ситуации, когда обе системы считают себя ведущими, и
в результате возникают конфликты и потеря данных.
Во многих отказоустойчивых конструкциях используются всего две системы: ведущая и резервная,
с некоторым контрольным механизмом, который постоянно проверяет соединение между ними
и работоспособность ведущей. Также возможно применение третьей системы (называемой следя-
щим сервером) для исключения некоторых вариантов нежелательной отработки отказа, но эта до-
полнительная сложность оправдана, только если вся схема достаточно хорошо продумана и тща-
тельно протестирована.
PostgreSQL не предоставляет системного программного обеспечения, необходимого для опреде-
ления сбоя на ведущем и уведомления резервного сервера баз данных. Имеется множество подоб-
ных инструментов, которые хорошо интегрируются со средствами операционной системы, требу-
емыми для успешной отработки отказа, например, для миграции IP-адреса.
Когда происходит переключение на резервный сервер, только один сервер продолжает работу. Это
состояние называется ущербным. Бывший резервный сервер теперь является ведущим, а бывший
ведущий отключён и может оставаться отключённым. Для возвращения к нормальному состоянию
необходимо запустить новый резервный сервер, либо на бывшем ведущем, либо в третьей, возмож-
но, новой системе. Ускорить этот процесс в больших кластерах позволяет утилита pg_rewind. По
завершении этого процесса можно считать, что ведущий и резервный сервер поменялись ролями.
Некоторые используют третий сервер в качестве запасного для нового ведущего, пока не будет
воссоздан новый резервный сервер, хотя это, очевидно, усложняет конфигурацию системы и ра-
бочие процедуры.
Таким образом, переключение с ведущего сервера на резервный может быть быстрым, но требу-
ет некоторого времени для повторной подготовки отказоустойчивого кластера. Регулярные пере-
ключения с ведущего сервера на резервный полезны, так как при этом появляется плановое время
для отключения и проведения обслуживания. Это также позволяет убедиться в работоспособности
механизма отработки отказа и гарантировать, что он действительно будет работать, когда потре-
буется. Эти административные процедуры рекомендуется документировать письменно.
Чтобы сделать ведущим резервный сервер, принимающий журналы, выполните команду pg_ctl
promote или создайте файл-триггер с именем и путём, заданным в параметре trigger_file в фай-
ле recovery.conf. Если для переключения планируется использовать команду pg_ctl promote,
указывать trigger_file не требуется. Если резервный сервер применяется для анализа данных,
чтобы только разгрузить ведущий, выполняя запросы на чтение, а не обеспечивать отказоустой-
чивость, повышать его до ведущего не понадобится.
26.4. Другие методы трансляции журнала
Встроенному режиму резерва, описанному в предыдущем разделе, есть альтернатива — задать в
restore_command команду, следящую за содержимым архива. Эта возможность доступна только
для версии 8.4 и выше. В такой конфигурации режим standby_mode выключается, так как реали-
зуется отдельный механизм слежения за данными, требующихся для резервного сервера. См. мо-
дуль pg_standby для примера реализации такой возможности.
Необходимо отметить, что в этом режиме сервер будет применять только один файл WAL одновре-
менно, то есть если использовать резервный сервер для запросов (см. сервер горячего резерва),
будет задержка между операциями на главном и моментом видимости этой операции резервным,
соответствующей времени заполнения файла WAL. archive_timeout можно использовать для сни-
жения этой задержки. Так же необходимо отметить, что нельзя совмещать этот метод с потоковой
репликацией.
В процессе работы на ведущем сервере и резервном будет происходить обычное формирование
архивов и их восстановление. Единственной точкой соприкосновения двух серверов будут только
архивы файлов WAL на обеих сторонах: на ведущем архивы формируются, на резервном происходит
чтение данных из архивов. Следует внимательно следить за тем, чтобы архивы WAL от разных
660Отказоустойчивость, баланси-
ровка нагрузки и репликация
ведущих серверов не смешивались или не перепутывались. Архив не должен быть больше, чем это
необходимо для работы резерва.
Магия, заставляющая работать вместе два слабо связанных сервера, проста: restore_command,
выполняющаяся на резервном при запросе следующего файла WAL, ожидает его доступности
на ведущем. Команда restore_command задаётся в файле recovery.conf на резервном сервере.
Обычно процесс восстановления запрашивает файл из архива WAL, сообщая об ошибке в слу-
чае его недоступности. Для работы резервного сервера недоступность очередного файла WAL яв-
ляется обычной ситуацией, резервный просто ожидает его появления. Для файлов, оканчиваю-
щихся на .history, ожидание не требуется, поэтому возвращается ненулевой код. Ожидающая
restore_command может быть написана как пользовательский скрипт, который в цикле опрашива-
ет, не появился ли очередной файл WAL. Также должен быть способ инициировать переключение
роли, при котором цикл в restore_command должен прерваться, а резервный сервер должен полу-
чить ошибку «файл не найден». При этом восстановление завершится, и резервный сервер сможет
станет обычным.
Псевдокод для подходящей restore_command:
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
sleep(100000L);
/* ждать ~0.1 сек*/
if (CheckForExternalTrigger())
triggered = true;
}
if (!triggered)
CopyWALFileForRecovery();
Рабочий пример ожидающей restore_command представлен в модуле pg_standby. К нему следует
обратится за примером правильной реализации логики, описанной выше. Он так же может быть
расширен для поддержки особых конфигураций и окружений.
Метод вызова переключения является важной частью планирования и архитектуры. Один из воз-
можных вариантов — команда restore_command. Она исполняется единожды для каждого файла
WAL, но процесс, запускаемый restore_command, создаётся и завершается для каждого файла, так
что это не служба и не серверный процесс, и применить сигналы и реализовать их обработчик
в нём нельзя. Поэтому restore_command не подходит для отработки отказа. Можно организовать
переключение по тайм-ауту, в частности, связав его с известным значением archive_timeout на
ведущем. Однако это не очень надёжно, так как переключение может произойти и из-за проблем
в сети или загруженности ведущего сервера. В идеале для этого следует использовать механизм
уведомлений, например явно создавать файл-триггер, если это возможно.
26.4.1. Реализация
Сокращённая процедура настройки для резервного сервера с применением альтернативного ме-
тода указана ниже. Для подробностей по каждому шагу следует обратиться к указанному разделу.</p>
<ol>
  <li>Разверните ведущую и резервную системы, сделав их максимально одинаковыми, включая две
одинаковые копии PostgreSQL одного выпуска.</li>
  <li>Настройте постоянную архивацию с ведущего сервера в каталог архивов WAL на резервном. Убе-
дитесь, что archive_mode, archive_command и archive_timeout установлены в соответствующие
значения на ведущем (см. Подраздел 25.3.1).</li>
  <li>Создайте базовую копию данных ведущего сервера (см. Подраздел 25.3.2) и восстановите её на
резервном.</li>
  <li>Запустите восстановление на резервном сервере из локального архива WAL с помощью команды
restore_command из файла recovery.conf как описано выше (см. Подраздел 25.3.4).
Поток восстановления только читает архив WAL, поэтому, как только файл WAL скопирован на
резервную систему, его можно копировать на ленту в то время, как его читает резервный сервер.
661Отказоустойчивость, баланси-
ровка нагрузки и репликация
Таким образом, работа резервного сервера в целях отказоустойчивости может быть совмещена с
долговременным сохранением файлов для восстановления после катастрофических сбоев.
Для целей тестирования возможен запуск ведущего и резервного сервера в одной системе. Это не
обеспечивает надёжность серверов, так же как и не подходит под описание высокой доступности.
26.4.2. Построчная трансляция журнала
Так же возможна реализация построчной трансляции журналов с применением альтернативного
метода, хотя это требует дополнительных доработок, а изменения будут видны для запросов на
сервере горячего резерва только после передачи полного файла WAL.
Внешняя программа может вызвать функцию pg_walfile_name_offset() (см. Раздел 9.26) для по-
иска имени файла и точного смещения в нём от текущего конца WAL. Можно получить доступ
к файлу WAL напрямую и скопировать данные из последнего известного окончания WAL до теку-
щего окончания на резервном сервере. При таком подходе интервал возможной потери данных
определяется временем цикла работы программы копирования, что может составлять очень ма-
лую величину. Так же не потребуется напрасно использовать широкую полосу пропускания для
принудительного архивирования частично заполненного файла сегмента. Следует отметить, что
на резервном сервере скрипт команды restore_command работает только с файлом WAL целиком,
таким образом, копирование данных нарастающим итогом не может быть выполнено на резерв-
ном обычными средствами. Это используется только в случае отказа ведущего — когда послед-
ний частично сформированный файл WAL предоставляется резервному непосредственно перед пе-
реключением. Корректная реализация этого процесса требует взаимодействия скрипта команды
restore_command с данными из программы копирования.
Начиная с PostgreSQL версии 9.0 можно использовать потоковую репликацию (см. Подраз-
дел 26.2.5) для получения этих же преимуществ меньшими усилиями.
26.5. Горячий резерв
Термин «горячий резерв» используется для описания возможности подключаться к серверу и вы-
полнять запросы на чтение, в то время как сервер находится в режиме резерва или восстановле-
ния архива. Это полезно и для целей репликации, и для восстановления желаемого состояния из
резервной копии с высокой точностью. Так же термин «горячий резерв» описывает способность
сервера переходить из режима восстановления к обычной работе, в то время как пользователи
продолжают выполнять запросы и/или их соединения остаются открытыми.
В режиме горячего резерва запросы выполняются примерно так же, как и в обычном режиме, с
некоторыми отличиями в использовании и администрировании, описанными ниже.
26.5.1. Обзор на уровне пользователя
Когда параметр hot_standby на резервном сервере установлен в true, то он начинает принимать
соединения сразу как только система придёт в согласованное состояние в процессе восстановле-
ния. Для таких соединений будет разрешено только чтение, запись невозможна даже во времен-
ные таблицы.
Для того, чтобы данные с ведущего сервера были получены на резервном, требуется некоторое
время. Таким образом, имеется измеряемая задержка между ведущим и резервным серверами.
Поэтому запуск одинаковых запросов примерно в одно время на ведущем и резервном серверах
может вернуть разный результат. Можно сказать, что данные на резервном сервере в конечном
счёте согласуются с ведущим. После того как запись о зафиксированной транзакции воспроиз-
водится на резервном сервере, изменения, совершённые в этой транзакции, становится видны в
любых последующих снимках данных на резервном сервере. Снимок может быть сделан в начале
каждого запроса или в начале каждой транзакции в зависимости от уровня изоляции транзакции.
Более подробно см. Раздел 13.2.
Транзакции, запущенные в режиме горячего резерва, могут выполнять следующие команды:
662Отказоустойчивость, баланси-
ровка нагрузки и репликация
• Доступ к данным — SELECT, COPY TO
• Команды для работы с курсором — DECLARE, FETCH, CLOSE
• Параметры — SHOW, SET, RESET
• Команды явного управления транзакциями
• BEGIN, END, ABORT, START TRANSACTION
• SAVEPOINT, RELEASE, ROLLBACK TO SAVEPOINT
• Блок EXCEPTION и другие внутренние подчиненные транзакции
• LOCK TABLE, только когда исполняется в явном виде в следующем режиме: ACCESS SHARE, ROW
SHARE или ROW EXCLUSIVE.
• Планы и ресурсы — PREPARE, EXECUTE, DEALLOCATE, DISCARD
• Дополнения и расширения — LOAD
Транзакции, запущенные в режиме горячего резерва, никогда не получают ID транзакции и не
могут быть записаны в журнал предзаписи. Поэтому при попытке выполнить следующие действия
возникнут ошибки:
• Команды манипуляции данными (DML) — INSERT, UPDATE, DELETE, COPY FROM, TRUNCATE. Следу-
ет отметить, что нет разрешённых действий, которые приводили бы к срабатыванию тригге-
ра во время исполнения на резервном сервере. Это ограничение так же касается и временных
таблиц, так как строки таблицы не могут быть прочитаны или записаны без обращения к ID
транзакции, что в настоящее время не возможно в среде горячего резерва.
• Команды определения данных (DDL) — CREATE, DROP, ALTER, COMMENT. Эти ограничения так же
относятся и к временным таблицам, так как операции могут потребовать обновления таблиц
системных каталогов.
• SELECT … FOR SHARE | UPDATE, так как блокировка строки не может быть проведена без об-
новления соответствующих файлов данных.
• Правила для выражений SELECT, которые приводят к выполнению команд DML.
• LOCK которая явно требует режим более строгий чем ROW EXCLUSIVE MODE.
• LOCK в короткой форме с умолчаниями, так как требует ACCESS EXCLUSIVE MODE.
• Команды управления транзакциями, которые в явном виде требуют режим не только для чте-
ния
• BEGIN READ WRITE, START TRANSACTION READ WRITE
• SET TRANSACTION READ WRITE, SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE
• SET transaction_read_only = off
• Команды двухфазной фиксации — PREPARE TRANSACTION, COMMIT PREPARED, ROLLBACK PREPARED,
так как даже транзакции только для чтения нуждаются в записи в WAL на подготовительной
фазе (первая фаза двухфазной фиксации).
• Обновление последовательностей — nextval(), setval()
• LISTEN, UNLISTEN, NOTIFY
При обычной работе транзакции «только для чтения» могут использовать команды LISTEN,
UNLISTEN и NOTIFY; таким образом, сеансы горячего резерва работают с несколько большими огра-
ничениями, чем обычные только читающие сеансы. Возможно, что некоторые из этих ограниче-
ний будут ослаблены в следующих выпуска.
В режиме горячего резерва параметр transaction_read_only всегда имеет значение true и изме-
нить его нельзя. Но если не пытаться модифицировать содержимое БД, подключение к серверу в
663Отказоустойчивость, баланси-
ровка нагрузки и репликация
этом режиме не отличается от подключений к обычным базам данных. При отработке отказа или
переключении ролей база данных переходит в обычный режим работы. Когда сервер меняет ре-
жим работы, установленные сеансы остаются подключёнными. После выхода из режима горячего
резерва становится возможным запускать пишущие транзакции (даже в сеансах, начатых ещё в
режиме горячего резерва).
Пользователи могут узнать о нахождении сессии в режиме только для чтения с помощью команды
SHOW transaction_read_only. Кроме того, набор функций (Таблица 9.80) позволяет пользователям
получить доступ к информации о резервном сервере. Это позволяет создавать программы, учиты-
вающие текущий статус базы данных. Такой режим может быть полезен для мониторинга процес-
са восстановления или для написания комплексного восстановления для особенных случаев.
26.5.2. Обработка конфликтов запросов
Ведущий и резервный серверы связаны между собой многими слабыми связями. События на веду-
щем сервере оказывают влияние на резервный. В результате имеется потенциальная возможность
отрицательного влияния или конфликта между ними. Наиболее простой для понимания конфликт
— быстродействие: если на ведущем происходит загрузка очень большого объёма данных, то про-
исходит создание соответствующего потока записей WAL на резервный сервер. Таким образом,
запросы на резервном конкурируют за системные ресурсы, например, ввод-вывод.
Так же может возникнуть дополнительный тип конфликта на сервере горячего резерва. Этот кон-
фликт называется жёстким конфликтом, оказывает влияние на запросы, приводя к их отмене, а
в некоторых случаях и к обрыву сессии для разрешения конфликтов. Пользователям предоставлен
набор средств для обработки подобных конфликтов. Случаи конфликтов включают:
• Установка эксклюзивной блокировки на ведущем сервере, как с помощью явной команды
LOCK, так и при различных DDL, что приводит к конфликту доступа к таблицам на резервном.
• Удаление табличного пространства на ведущем сервере приводит к конфликту на резервном
когда запросы используют это пространство для хранения временных рабочих файлов.
• Удаление базы данных на ведущем сервере конфликтует с сессиями, подключёнными к этой
БД на резервном.
• Приложение очистки устаревших транзакций из WAL конфликтует с транзакциями на резерв-
ном сервере, которые используют снимок данных, который всё ещё видит какие-то из очищен-
ных на ведущем строк.
• Приложение очистки устаревших транзакций из WAL конфликтует с запросами к целевой
странице на резервном сервере вне зависимости от того, являются ли данные удалёнными
или видимыми.
В этих случаях на ведущем сервере просто происходит ожидание; пользователю следует выбрать
какую их конфликтующих сторон отменить. Тем не менее, на резервном нет выбора: действия из
WAL уже произошли на ведущем, поэтому резервный обязан применить их. Более того, позволять
обработчику WAL ожидать неограниченно долго может быть крайне нежелательно, так как отста-
вание резервного сервера от ведущего может всё возрастать. Таким образом, механизм обеспечи-
вает принудительную отмену запросов на резервном сервере, которые конфликтуют с применяе-
мыми записями WAL.
Примером такой проблемы может быть ситуация: администратор на ведущем сервере выполнил
команду DROP TABLE для таблицы, которая сейчас участвует в запросе на резервном. Понятно, что
этот запрос нельзя будет выполнять дальше, если команда DROP TABLE применится на резервном.
Если бы этот запрос выполнялся на ведущем, команда DROP TABLE ждала бы его окончания. Но ко-
гда на ведущем выполняется только команда DROP TABLE, ведущий сервер не знает, какие запросы
выполняются на резервном, поэтому он не может ждать завершения подобных запросов. Поэтому
если записи WAL с изменением прибудут на резервный сервер, когда запрос будет продолжать вы-
полняться, возникнет конфликт. В этом случае резервный сервер должен либо задержать приме-
нение этих записей WAL (и всех остальных, следующих за ними), либо отменить конфликтующий
запрос, чтобы можно было применить DROP TABLE.
664Отказоустойчивость, баланси-
ровка нагрузки и репликация
Если конфликтный запрос короткий, обычно желательно разрешить ему завершиться, нена-
долго задержав применение записей WAL, но слишком большая задержка в применении WAL
обычно нежелательна. Поэтому механизм отмены имеет параметры max_standby_archive_delay и
max_standby_streaming_delay, которые определяют максимально допустимое время задержки при-
менения WAL. Конфликтующие запросы будут отменены, если они длятся дольше допустимого
времени задержки применения очередных записей WAL. Два параметра существуют для того, что-
бы можно было задать разные значения для чтения записей WAL из архива (то есть при начальном
восстановлении из базовой копии либо при «навёрстывании» ведущего сервера в случае большого
отставания) и для получения записей WAL при потоковой репликации.
На резервном сервере, созданном преимущественно для отказоустойчивости, лучше выставлять
параметры задержек относительно небольшими, чтобы он не мог сильно отстать от ведущего из-
за задержек, связанных с ожиданием запросов горячего резерва. Однако если резервный сервер
предназначен для выполнения длительных запросов, то высокое значение или даже бесконечное
ожидание могут быть предпочтительнее. Тем не менее, следует иметь в виду, что длительные за-
просы могут оказать влияние на другие сессии на резервном сервере в виде отсутствия последних
изменений от ведущего из-за задержки применения записей WAL.
В
случае,
если
задержка,
определённая
max_standby_archive_delay
или
max_standby_streaming_delay будет превышена, конфликтующий запрос будет отменён. Обычно
это выражается в виде ошибки отмены, но в случае проигрывания команды DROP DATABASE обры-
вается вся конфликтная сессия. Так же, если конфликт произошел при блокировке, вызванной
транзакцией в состоянии IDLE, конфликтная сессия разрывается (это поведение может изменить
в будущем).
Отменённые запросы могут быть немедленно повторены (конечно после старта новой транзакции).
Так как причина отмены зависит от природы проигрываемых записей WAL, запрос, который был
отменён, может быть успешно выполнен вновь.
Следует учесть, что параметры задержки отсчитываются от времени получения резервным сер-
вером данных WAL. Таким образом, период дозволенной работы для запроса на резервном серве-
ре никогда не может быть длиннее параметра задержки и может быть существенно короче, если
резервный уже находится в режиме задержки в результате ожидания предыдущего запроса или
результат не доступен из-за высокой нагрузки обновлений.
Наиболее частой причиной конфликтов между запросами на резервном сервере и проигрыванием
WAL является преждевременная очистка. Обычно PostgreSQL допускает очистку старых версий
записей при условии что ни одна из транзакций их не видит согласно правилам видимости данных
для MVCC. Тем не менее, эти правила применяются только для транзакций, выполняемых на глав-
ном сервере. Таким образом, допустима ситуация, когда на главном запись уже очищена, но эта
же запись всё ещё видна для транзакций на резервном сервере.
Для опытных пользователей следует отметить, что как очистка старых версий строк, так и замо-
розка версии строки могут потенциально вызвать конфликт с запросами на резервном сервере.
Ручной запуск команды VACUUM FREEZE может привести к конфликту, даже в таблице без обнов-
ленных и удалённых строк.
Пользователи должны понимать, что регулярное и активное изменение данных в таблицах на ве-
дущем сервере чревато отменой длительных запросов на резервном. В таком случае установка
конечного значения для max_standby_archive_delay или max_standby_streaming_delay действует
подобно ограничению statement_timeout.
В случае, если количество отменённых запросов на резервном сервере получается неприемле-
мым, существует ряд дополнительных возможностей. Первая возможность — установить параметр
hot_standby_feedback, который не даёт команде VACUUM удалять записи, ставшие недействитель-
ными недавно, что предотвращает конфликты очистки. При этом следует учесть, что это вызывает
задержку очистки мёртвых строк на ведущем, что может привести к нежелательному распуханию
таблицы. Тем не менее, в итоге ситуация будет не хуже, чем если бы запросы к резервному серверу
исполнялись непосредственно на ведущем, но при этом сохранится положительный эффект от раз-
665Отказоустойчивость, баланси-
ровка нагрузки и репликация
деления нагрузки. В случае, когда соединение резервных серверов с ведущим часто разрывается,
следует скорректировать период, в течение которого обратная связь через hot_standby_feedback
не обеспечивается. Например, следует подумать об увеличении max_standby_archive_delay, что-
бы запросы отменялись не сразу при конфликтах с архивом WAL в период разъединения. Также
может иметь смысл увеличить max_standby_streaming_delay для предотвращения быстрой отме-
ны запросов из-за полученных записей WAL после восстановления соединения.
Другая возможность — увеличение vacuum_defer_cleanup_age на ведущем сервере таким об-
разом, чтобы мёртвые записи не очищались бы так быстро, как при обычном режиме рабо-
ты. Это даёт запросам на резервном сервере больше времени на выполнение, прежде чем они
могут быть отменены, без увеличения задержки max_standby_streaming_delay. Тем не менее
при таком подходе очень трудно обеспечить какое-то определённое окно по времени, так как
vacuum_defer_cleanup_age измеряется в количестве транзакций, выполняемых на ведущем серве-
ре.
Количество отменённых запросов и причины отмены можно просмотреть через системное
представление pg_stat_database_conflicts на резервном сервере. Системное представление
pg_stat_database так же содержит итоговую информацию.
26.5.3. Обзор административной части
Если в файле postgresql.conf для параметра hot_standby задано значение on (по умолчанию)
и существует файл recovery.conf, сервер запустится в режиме горячего резерва. Однако может
пройти некоторое время, прежде чем к нему можно будет подключиться, так как он не будет при-
нимать подключения, пока не произведёт восстановление до согласованного состояния, подходя-
щего для выполнения запросов. (Информация о согласованности состояния записывается на веду-
щем сервере в контрольной точке.) В течение этого периода клиенты при попытке подключения
будут получать сообщение об ошибке. Убедиться, что сервер включился в работу, можно либо по-
вторяя попытки подключения из приложения до успешного подключения, либо дождавшись по-
явления в журналах сервера этих сообщений:
LOG:
entering standby mode
… then some time later …
LOG:
LOG:
consistent recovery state reached
database system is ready to accept read only connections
Включить горячий резерв нельзя, если WAL был записан в период, когда на ведущем сервере па-
раметр wal_level имел значение не replica и не logical. Достижение согласованного состояния
также может быть отсрочено, если имеют место оба этих условия:
• Пишущая транзакция имеет более 64 подтранзакций
• Очень длительные пишущие транзакции
Если вы применяете файловую репликацию журналов («тёплый резерв»), возможно, придётся
ожидать прибытия следующего файла WAL (максимальное время ожидания задаётся параметром
archive_timeout на ведущем сервере).
Значения некоторых параметров на резервном сервере необходимо изменить при модификации
их на ведущем. Для таких параметров значения на резервном сервере должны быть не меньше
значений на ведущем. Таким образом, если вы хотите увеличить их, вы сначала должны сделать
это на резервных серверах, а затем применить изменения на ведущем. И наоборот, если вы хотите
их уменьшить, сначала сделайте это на ведущем сервере, а потом примените изменения на всех
резервных. Если параметры имеют недостаточно большие значения, резервный сервер не сможет
начать работу. В этом случае можно увеличить их и повторить попытку запуска сервера, чтобы он
возобновил восстановление. Это касается следующих параметров:
• max_connections
• max_prepared_transactions
666Отказоустойчивость, баланси-
ровка нагрузки и репликация
• max_locks_per_transaction
• max_worker_processes
Очень важно для администратора выбрать подходящие значения для max_standby_archive_delay
и max_standby_streaming_delay. Оптимальное значение зависит от приоритетов. Например, если
основное назначение сервера — обеспечение высокой степени доступности, то следует установить
короткий период, возможно даже нулевой, хотя это очень жёсткий вариант. Если резервный сер-
вер планируется как дополнительный сервер для аналитических запросов, то приемлемой будет
максимальная задержка в несколько часов или даже -1, что означает бесконечное ожидание окон-
чания запроса.
Вспомогательные биты статуса транзакций, записанные на ведущем, не попадают в WAL, так что
они, скорее всего, будут перезаписаны на нём при работе с данными. Таким образом, резервный
сервер будет производить запись на диск, даже если все пользователи только читают данные, ни-
чего не меняя. Кроме того, пользователи будут записывать временные файлы при сортировке боль-
ших объёмов и обновлять файлы кеша. Поэтому в режиме горячего резерва ни одна часть базы
данных фактически не работает в режиме «только чтение». Следует отметить, что также возможно
выполнить запись в удалённую базу данных с помощью модуля dblink и другие операции вне базы
данных с применением PL-функций, несмотря на то, что транзакции по-прежнему смогут только
читать данные.
Следующие типы административных команд недоступны в течение режима восстановления:
• Команды определения данных (DDL) — например: CREATE INDEX
• Команды выдачи привилегий и назначения владельца — GRANT, REVOKE, REASSIGN
• Команды обслуживания — ANALYZE, VACUUM, CLUSTER, REINDEX
Ещё раз следует отметить, что некоторые из этих команд фактически доступны на ведущем сер-
вере для транзакций в режиме только для чтения.
В результате нельзя создать дополнительные индексы или статистику, чтобы они существовали
только на резервном. Если подобные административные команды нужны, то их следует выполнить
на ведущем сервере, затем эти изменения будут распространены на резервные серверы.
Функции pg_cancel_backend() и pg_terminate_backend() работают на стороне пользовате-
ля, но не для процесса запуска, который обеспечивает восстановление. Представление
pg_stat_activity не показывает восстанавливаемые транзакции как активные. Поэтому представ-
ление pg_prepared_xacts всегда пусто в ходе восстановления. Если требуется разобрать сомни-
тельные подготовленные транзакции, следует обратиться к pg_prepared_xacts на ведущем и вы-
полнить команды для разбора транзакций там либо разобрать их по окончании восстановления.
pg_locks отображает блокировки, происходящие в процессе работы сервера как обычно. pg_locks
так же показывает виртуальные транзакции, обработанные процессом запуска, которому принад-
лежат все AccessExclusiveLocks, наложенные транзакциями в режиме восстановления. Следует
отметить, что процесс запуска не запрашивает блокировки, чтобы внести изменения в базу дан-
ных, поэтому блокировки, отличные от AccessExclusiveLocks не показываются в pg_locks для
процесса запуска, подразумевается их существование.
Модуль check_pgsql для Nagios будет работать, так как сервер выдаёт простую информацию, нали-
чие которой он проверяет. Скрипт мониторинга check_postgres так же работает, хотя для некото-
рых выдаваемых показателей результаты могут различаться или вводить в заблуждение. Напри-
мер, нельзя отследить время последней очистки, так как очистка не производится на резервном
сервере. Очистка запускается на ведущем сервере и результаты её работы передаются резервно-
му.
Команды управления файлами WAL, например pg_start_backup, pg_switch_wal и т. д. не будут
работать во время восстановления.
Динамически загружаемые модули работать будут, включая pg_stat_statements.
667Отказоустойчивость, баланси-
ровка нагрузки и репликация
Рекомендательная блокировка работает обычно при восстановлении, включая обнаружение вза-
имных блокировок. Следует отметить, что рекомендательная блокировка никогда не попадает в
WAL, таким образом для рекомендательной блокировки как на ведущем сервере, так и на резерв-
ном, невозможен конфликт с проигрыванием WAL. Но возможно получение рекомендательной бло-
кировки на ведущем сервере, а затем получение подобной рекомендательной блокировки на ре-
зервном. Рекомендательная блокировка относится только к серверу, на котором она получена.
Системы репликации на базе триггеров, подобные Slony, Londiste и Bucardo не могут запускаться
на резервном сервере вовсе, хотя они превосходно работают на ведущем до тех пор, пока не будет
подана команда не пересылать изменения на резервный. Проигрывание WAL не основано на триг-
герах, поэтому поток WAL нельзя транслировать с резервного сервера в другую систему, которая
требует дополнительной записи в БД или работает на основе триггеров.
Новые OID не могут быть выданы, хотя, например генераторы UUID смогут работать, если они не
пытаются записывать новое состояние в базу данных.
В настоящий момент создание временных таблиц недопустимо при транзакции только для чтения,
в некоторых случаях существующий скрипт будет работать неверно. Это ограничение может быть
ослаблено в следующих выпусках. Это одновременно требование SQL стандарта и техническое
требование.
Команда DROP TABLESPACE может быть выполнена только если табличное пространство пусто. Неко-
торые пользователи резервного сервера могут активно использовать табличное пространство че-
рез параметр temp_tablespaces. Если имеются временные файлы в табличных пространствах, все
активные запросы отменяются для обеспечения удаления временных файлов, затем табличное
пространство может быть удалено и продолжено проигрывание WAL.
Выполнение команды DROP DATABASE или ALTER DATABASE … SET TABLESPACE на ведущем сервере
приводит к созданию записи в WAL, которая вызывает принудительное отключение всех пользова-
телей, подключённых к этой базе данных на резервном. Это происходит немедленно, вне зависимо-
сти от значения max_standby_streaming_delay. Следует отметить, что команда ALTER DATABASE …
RENAME не приводит к отключению пользователей, так что обычно она действует незаметно, хотя
в некоторых случаях возможны сбои программ, которые зависят от имени базы данных.
Если вы в обычном режиме (не в режиме восстановления) выполните DROP USER или DROP ROLE
для роли с возможностью подключения, в момент, когда этот пользователь подключён, на данном
пользователе это никак не отразится — он останется подключённым. Однако переподключиться
он уже не сможет. Это же поведение действует в режиме восстановления — если выполнить DROP
USER на ведущем сервере, пользователь не будет отключён от резервного.
Сборщик статистики работает во время восстановления. Все операции сканирования, чтения, бло-
ки, использование индексов и т. п. будут записаны обычным образом на резервном сервере. Дей-
ствия, происходящие при проигрывании, не будут дублировать действия на ведущем сервере,
то есть проигрывание команды вставки не увеличит значение столбца Inserts в представлении
pg_stat_user_tables. Файлы статистики удаляются с началом восстановления, таким образом, ста-
тистика на ведущем сервере и резервном будет разной. Это является особенностью, не ошибкой.
Автоматическая очистка не работает во время восстановления. Она запустится в обычном режиме
после завершения восстановления.
Во время восстановления активен фоновый процесс записи, он обрабатывает точки перезапуска
(подобно контрольным точкам на ведущем сервере) и выполняет обычную очистку блоков. В том
числе он может обновлять вспомогательные биты, сохранённые на резервном. Во время восста-
новления принимается команда CHECKPOINT, но она производит точку перезапуска, а не создаёт
новую точку восстановления.
26.5.4. Ссылки на параметры горячего резерва
Различные параметры были упомянуты выше в Подразделе 26.5.2 и Подразделе 26.5.3.
668Отказоустойчивость, баланси-
ровка нагрузки и репликация
На ведущем могут применяться параметры wal_level и vacuum_defer_cleanup_age. Параметры
max_standby_archive_delay и max_standby_streaming_delay на ведущем не действуют.
На резервном сервере могут применяться параметры hot_standby, max_standby_archive_delay и
max_standby_streaming_delay. Параметр vacuum_defer_cleanup_age на нём не действует, пока сер-
вер остаётся в режиме резервного сервера. Но если он станет ведущим, его значение вступит в
силу.
26.5.5. Ограничения
Имеются следующие ограничения горячего резерва. Они могут и скорее всего будут исправлены
в следующих выпусках:
• Требуется информация о всех запущенных транзакциях перед тем как будет создан снимок
данных. Транзакции, использующие большое количество подтранзакций (в настоящий момент
больше 64), будут задерживать начало соединения только для чтения до завершения самой
длинной пишущей транзакции. При возникновении этой ситуации поясняющее сообщение бу-
дет записано в журнал сервера.
• Подходящие стартовые точки для запросов на резервном сервере создаются при каждой кон-
трольной точке на главном. Если резервный сервер отключается, в то время как главный был
в отключённом состоянии, может оказаться невозможным возобновить его работу в режиме
горячего резерва, до того, как запустится ведущий и добавит следующие стартовые точки в
журналы WAL. Подобная ситуация не является проблемой для большинства случаев, в кото-
рых она может произойти. Обычно, если ведущий сервер выключен и больше не доступен, это
является следствием серьёзного сбоя и в любом случае требует преобразования резервного в
новый ведущий. Так же в ситуации, когда ведущий отключён намеренно, проверка готовности
резервного к преобразованию в ведущий тоже является обычной процедурой.
• В конце восстановления блокировки AccessExclusiveLocks, вызванные подготовленными
транзакциями, требуют удвоенное, в сравнении с нормальным, количество блокировок за-
писей таблицы. Если планируется использовать либо большое количество конкурирующих
подготовленных транзакций, обычно вызывающие AccessExclusiveLocks, либо большие
транзакции с применением большого количества AccessExclusiveLocks, то рекомендует-
ся выбрать большое значение параметра max_locks_per_transaction, возможно в два ра-
за большее, чем значение параметра на ведущем сервере. Всё это не имеет значения, когда
max_prepared_transactions равно 0.
• Уровень изоляции транзакции Serializable в настоящее время недоступен в горячем резерве.
(За подробностями обратитесь к Подразделу 13.2.3 и Подразделу 13.4.1) Попытка выставить
для транзакции такой уровень изоляции в режиме горячего резерва вызовет ошибку.
669</li>
</ol>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://localhost:4000/tags/#PostgreSQL" title="Pages tagged PostgreSQL" class="tag"><span class="term">PostgreSQL</span></a><a href="http://localhost:4000/tags/#PostgreSQL_Book_11" title="Pages tagged PostgreSQL_Book_11" class="tag"><span class="term">PostgreSQL_Book_11</span></a></span>
        <span>Updated on <span class="entry-date date updated"><time datetime="2018-12-03 T15:14:43-04:00">December 03, 2018</time></span></span>
        <span class="author vcard"><span class="fn">Sergey Khatsiola</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/PostgreSQL-V11_Doc-026/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://localhost:4000/PostgreSQL-V11_Doc-026/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=http://localhost:4000/PostgreSQL-V11_Doc-026/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://localhost:4000/PostgreSQL-V11_Doc-025/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://localhost:4000/Script_Mount_FTP_Folder/" title="Монтируем папку FTP в файловую систему Bash  SCRIPT">Монтируем папку FTP в файловую систему Bash  SCRIPT</a></h3>
      <p>Bash Монтируем папку FTP в файловую систему Bash  SCRIPT: <a href="http://localhost:4000/Script_Mount_FTP_Folder/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://localhost:4000/Script_bash-copy-ftp/" title="Копируем на FTP скриптом bash">Копируем на FTP скриптом bash</a></h4>
        <span>Published on January 17, 2019</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://localhost:4000/Work-Project-Manager/" title="Коротко - работа менеджера проектов">Коротко - работа менеджера проектов</a></h4>
        <span>Published on December 04, 2018</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Sergey Khatsiola. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-130427752-1', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>


	        

</body>
</html>
