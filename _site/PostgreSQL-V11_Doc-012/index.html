<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Глава 12. Полнотекстовый поиск &#8211; Sirius Blog</title>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130427752-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130427752-1');
</script>

</head>
<meta name="description" content="">
<meta name="keywords" content="PostgreSQL, PostgreSQL_Book_11">

<!-- Twitter Cards -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/abstract-11.jpg">

<meta name="twitter:title" content="Глава 12. Полнотекстовый поиск">
<meta name="twitter:description" content="">
<meta name="twitter:creator" content="@2hotab2">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Глава 12. Полнотекстовый поиск">
<meta property="og:description" content="">
<meta property="og:url" content="http://localhost:4000/PostgreSQL-V11_Doc-012/">
<meta property="og:site_name" content="Sirius Blog">





<link rel="canonical" href="http://localhost:4000/PostgreSQL-V11_Doc-012/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Sirius Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.jpg">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.jpg">
<!-- 114x72 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x72" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.jpg">
<!-- 144x72 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x72" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.jpg">



</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://localhost:4000/images/avatar.jpg" alt="Sergey Khatsiola photo" class="author-photo">
					<h4>Sergey Khatsiola</h4>
					<p>Кратко обо мне ...</p>
				</li>
				<li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:2hotab2@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				<li>
					<a href="https://twitter.com/2hotab2"><i class="fa fa-fw fa-twitter"></i> Twitter</a>
				</li>
				<li>
					<a href="https://facebook.com/sergej.ha1"><i class="fa fa-fw fa-facebook"></i> Facebook</a>
				</li>
				
				
				<li>
					<a href="https://github.com/Sergey-sirius"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    
	    <li><a href="http://localhost:4000/handbook/" >HandBook</a></li>
	  
	    
	    <li><a href="https://github.com/Sergey-sirius" target="_blank">Main Link</a></li>
	  
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  
  <div class="entry-image">
    <img src="http://localhost:4000/images/abstract-11.jpg" alt="Глава 12. Полнотекстовый поиск">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://localhost:4000/PostgreSQL-V11_Doc-012/" rel="bookmark" title="Глава 12. Полнотекстовый поиск">Глава 12. Полнотекстовый поиск</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2018-12-01T00:00:00+02:00">December 01, 2018</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
Reading time ~63 minutes
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>Глава 12. Полнотекстовый поиск</p>

<p>12.1. Введение
Полнотекстовый поиск (или просто поиск текста) — это возможность находить документы на
естественном языке, соответствующие запросу, и, возможно, дополнительно сортировать их по
релевантности для этого запроса. Наиболее распространённая задача — найти все документы,
содержащие слова запроса, и выдать их отсортированными по степени соответствия запросу.
Понятия запроса и соответствия довольно расплывчаты и зависят от конкретного приложения.
В самом простом случае запросом считается набор слов, а соответствие определяется частотой
слов в документе.
Операторы текстового поиска существуют в СУБД уже многие годы. В PostgreSQL для текстовых
типов данных есть операторы ~, ~*, LIKE и ILIKE, но им не хватает очень важных вещей, которые
требуются сегодня от информационных систем:
• Нет поддержки лингвистического функционала, даже для английского языка. Возможности
регулярных выражений ограничены — они не рассчитаны на работу со словоформами, напри-
мер, подходят и подходить. С ними вы можете пропустить документы, которые содержат под-
ходят, но, вероятно, и они представляют интерес при поиске по ключевому слову подходить.
Конечно, можно попытаться перечислить в регулярном выражении все варианты слова, но
это будет очень трудоёмко и чревато ошибками (некоторые слова могут иметь десятки слово-
форм).
• Они не позволяют упорядочивать результаты поиска (по релевантности), а без этого поиск
неэффективен, когда находятся сотни подходящих документов.
• Они обычно выполняются медленно из-за отсутствия индексов, так как при каждом поиске
приходится просматривать все документы.
Полнотекстовая индексация заключается в предварительной обработке документов и сохране-
нии индекса для последующего быстрого поиска. Предварительная обработка включает следую-
щие операции:
Разбор документов на фрагменты. При этом полезно выделить различные классы фрагмен-
тов, например, числа, слова, словосочетания, почтовые адреса и т. д., которые будут обраба-
тываться по-разному. В принципе классы фрагментов могут зависеть от приложения, но для
большинства применений вполне подойдёт предопределённый набор классов. Эту операцию в
PostgreSQL выполняет анализатор (parser). Вы можете использовать как стандартный анали-
затор, так и создавать свои, узкоспециализированные.
Преобразование фрагментов в лексемы. Лексема — это нормализованный фрагмент, в кото-
ром разные словоформы приведены к одной. Например, при нормализации буквы верхнего ре-
гистра приводятся к нижнему, а из слов обычно убираются окончания (в частности, s или es в
английском). Благодаря этому можно находить разные формы одного слова, не вводя вручную
все возможные варианты. Кроме того, на данном шаге обычно исключаются стоп-слова, то
есть слова, настолько распространённые, что искать их нет смысла. (Другими словами, фраг-
менты представляют собой просто подстроки текста документа, а лексемы — это слова, имею-
щие ценность для индексации и поиска.) Для выполнения этого шага в PostgreSQL использу-
ются словари. Набор существующих стандартных словарей при необходимости можно расши-
рять, создавая свои собственные.
Хранение документов в форме, подготовленной для поиска. Например, каждый документ мо-
жет быть представлен в виде сортированного массива нормализованных лексем. Помимо лек-
сем часто желательно хранить информацию об их положении для ранжирования по близо-
сти, чтобы документ, в котором слова запроса расположены «плотнее», получал более высо-
кий ранг, чем документ с разбросанными словами.
Словари позволяют управлять нормализацией фрагментов с большой гибкостью. Создавая слова-
ри, можно:
• Определять стоп-слова, которые не будут индексироваться.
370Полнотекстовый поиск
•
•
•
•
Сопоставлять
Сопоставлять
Сопоставлять
Сопоставлять
Snowball.
синонимы с одним словом, используя Ispell.
словосочетания с одним словом, используя тезаурус.
различные склонения слова с канонической формой, используя словарь Ispell.
различные склонения слова с канонической формой, используя стеммер
Для хранения подготовленных документов в PostgreSQL предназначен тип данных tsvector, а для
представления обработанных запросов — тип tsquery (Раздел 8.11). С этими типами данных ра-
ботают целый ряд функций и операторов (Раздел 9.13), и наиболее важный из них — оператор со-
ответствия @@, с которым мы познакомимся в Подразделе 12.1.2. Для ускорения полнотекстового
поиска могут применяться индексы (Раздел 12.9).
12.1.1. Что такое документ?
Документ — это единица обработки в системе полнотекстового поиска; например, журнальная
статья или почтовое сообщение. Система поиска текста должна уметь разбирать документы и со-
хранять связи лексем (ключевых слов) с содержащим их документом. Впоследствии эти связи мо-
гут использоваться для поиска документов с заданными ключевыми словами.
В контексте поиска в PostgreSQL документ — это обычно содержимое текстового поля в строке
таблицы или, возможно, сочетание (объединение) таких полей, которые могут храниться в разных
таблицах или формироваться динамически. Другими словами, документ для индексации может
создаваться из нескольких частей и не храниться где-либо как единое целое. Например:
SELECT title || ‘ ‘ ||
AS document
FROM messages
WHERE mid = 12;
author || ‘ ‘ ||
abstract || ‘ ‘ || body
SELECT m.title || ‘ ‘ || m.author || ‘ ‘ || m.abstract || ‘ ‘ || d.body
AS document
FROM messages m, docs d
WHERE mid = did AND mid = 12;
Примечание
На самом деле в этих примерах запросов следует использовать функцию coalesce, что-
бы значение NULL в каком-либо одном атрибуте не привело к тому, что результирую-
щим документом окажется NULL.
Документы также можно хранить в обычных текстовых файлах в файловой системе. В этом случае
база данных может быть просто хранилищем полнотекстового индекса и исполнителем запросов,
а найденные документы будут загружаться из файловой системы по некоторым уникальным иден-
тификаторам. Однако для загрузки внешних файлов требуются права суперпользователя или под-
держка специальных функций, так что это обычно менее удобно, чем хранить все данные внутри
БД PostgreSQL. Кроме того, когда всё хранится в базе данных, это упрощает доступ к метаданным
документов при индексации и выводе результатов.
Для нужд текстового поиска каждый документ должен быть сведён к специальному формату
tsvector. Поиск и ранжирование выполняется исключительно с этим представлением документа
— исходный текст потребуется извлечь, только когда документ будет отобран для вывода пользо-
вателю. Поэтому мы часто подразумеваем под tsvector документ, тогда как этот тип, конечно,
содержит только компактное представление всего документа.
12.1.2. Простое соответствие текста
Полнотекстовый поиск в PostgreSQL реализован на базе оператора соответствия @@, который воз-
вращает true, если tsvector (документ) соответствует tsquery (запросу). Для этого оператора не
важно, какой тип записан первым:
371Полнотекстовый поиск
SELECT ‘a fat cat sat on a mat and ate a fat rat’::tsvector @@
‘cat &amp; rat’::tsquery;
?column?
———-
t
SELECT ‘fat &amp; cow’::tsquery @@
‘a fat cat sat on a mat and ate a fat rat’::tsvector;
?column?
———-
f
Как можно догадаться из этого примера, tsquery — это не просто текст, как и tsvector. Значение
типа tsquery содержит искомые слова, это должны быть уже нормализованные лексемы, возможно
объединённые в выражение операторами И, ИЛИ, НЕ и ПРЕДШЕСТВУЕТ. (Подробнее синтаксис
описан в Подразделе 8.11.2.) Вы можете воспользоваться функциями to_tsquery, plainto_tsquery
и phraseto_tsquery, которые могут преобразовать заданный пользователем текст в значение
tsquery, прежде всего нормализуя слова в этом тексте. Функция to_tsvector подобным образом
может разобрать и нормализовать текстовое содержимое документа. Так что запрос с поиском
соответствия на практике выглядит скорее так:
SELECT to_tsvector(‘fat cats ate fat rats’) @@ to_tsquery(‘fat &amp; rat’);
?column?
———-
t
Заметьте, что соответствие не будет обнаружено, если запрос записан как
SELECT ‘fat cats ate fat rats’::tsvector @@ to_tsquery(‘fat &amp; rat’);
?column?
———-
f
так как слово rats не будет нормализовано. Элементами tsvector являются лексемы, предполо-
жительно уже нормализованные, так что rats считается не соответствующим rat.
Оператор @@ также может принимать типы text, позволяя опустить явные преобразования тексто-
вых строк в типы tsvector и tsquery в простых случаях. Всего есть четыре варианта этого опера-
тора:
tsvector @@ tsquery
tsquery @@ tsvector
text @@ tsquery
text @@ text
Первые два мы уже видели раньше. Форма text@@tsquery равнозначна выражению to_tsvector(x)
@@ y, а форма text@@text — выражению to_tsvector(x) @@ plainto_tsquery(y).
В значении tsquery оператор &amp; (И) указывает, что оба его операнда должны присутствовать в до-
кументе, чтобы он удовлетворял запросу. Подобным образом, оператор | (ИЛИ) указывает, что
в документе должен присутствовать минимум один из его операндов, тогда как оператор ! (НЕ)
указывает, что его операнд не должен присутствовать, чтобы условие удовлетворялось. Например,
запросу fat &amp; ! rat соответствуют документы, содержащие fat и не содержащие rat.
Фразовый поиск возможен с использованием оператора &lt;-&gt; (ПРЕДШЕСТВУЕТ) типа tsquery, ко-
торый находит соответствие, только если его операнды расположены рядом и в заданном порядке.
Например:
SELECT to_tsvector(‘fatal error’) @@ to_tsquery(‘fatal &lt;-&gt; error’);
?column?
———-
t
372Полнотекстовый поиск
SELECT to_tsvector(‘error is not fatal’) @@ to_tsquery(‘fatal &lt;-&gt; error’);
?column?
———-
f
Более общая версия оператора ПРЕДШЕСТВУЕТ имеет вид <N>, где N — целое число, выражающее
разность между позициями найденных лексем. Запись &lt;1&gt; равнозначна &lt;-&gt;, тогда как &lt;2&gt; допуска-
ет существование ровно одной лексемы между этими лексемами и т. д. Функция phraseto_tsquery
задействует этот оператор для конструирования tsquery, который может содержать многослов-
ную фразу, включающую в себя стоп-слова. Например:
SELECT phraseto_tsquery('cats ate rats');
phraseto_tsquery
-------------------------------
'cat' &lt;-&gt; 'ate' &lt;-&gt; 'rat'
SELECT phraseto_tsquery('the cats ate the rats');
phraseto_tsquery
-------------------------------
'cat' &lt;-&gt; 'ate' &lt;2&gt; 'rat'
Особый случай, который иногда бывает полезен, представляет собой запись &lt;0&gt;, требующая, чтобы
обоим лексемам соответствовало одно слово.
Сочетанием операторов tsquery можно управлять, применяя скобки. Без скобок операторы имеют
следующие приоритеты, в порядке возрастания: |, &amp;, &lt;-&gt; и самый приоритетный — !.
Стоит отметить, что операторы И/ИЛИ/НЕ имеют несколько другое значение, когда они приме-
няются в аргументах оператора ПРЕДШЕСТВУЕТ, так как в этом случае имеет значение точная
позиция совпадения. Например, обычному !x соответствуют только документы, не содержащие
x нигде. Но условию !x &lt;-&gt; y соответствует y, если оно не следует непосредственно за x; при
вхождении x в любом другом месте документа он не будет исключаться из рассмотрения. Другой
пример: для условия x &amp; y обычно требуется, чтобы и x, и y встречались в каком-то месте доку-
мента, но для выполнения условия (x &amp; y) &lt;-&gt; z требуется, чтобы x и y располагались в одном
месте, непосредственно перед z. Таким образом, этот запрос отличается от x &lt;-&gt; z &amp; y &lt;-&gt; z,
которому удовлетворяют документы, содержащие две отдельные последовательности x z и y z.
(Этот конкретный запрос в таком виде, как он записан, не имеет смысла, так как x и y не могут
находиться в одном месте; но в более сложных ситуациях, например, с шаблонами поиска по мас-
ке, запросы этого вида могут быть полезны.)
12.1.3. Конфигурации
До этого мы рассматривали очень простые примеры поиска текста. Как было упомянуто выше, весь
функционал текстового поиска позволяет делать гораздо больше: пропускать определённые сло-
ва (стоп-слова), обрабатывать синонимы и выполнять сложный анализ слов, например, выделять
фрагменты не только по пробелам. Все эти функции управляются конфигурациями текстового
поиска. В PostgreSQL есть набор предопределённых конфигураций для многих языков, но вы также
можете создавать собственные конфигурации. (Все доступные конфигурации можно просмотреть
с помощью команды \dF в psql.)
Подходящая конфигурация для данной среды выбирается во время установки и записывается в
параметре default_text_search_config в postgresql.conf. Если вы используете для всего кластера
одну конфигурацию текстового поиска, вам будет достаточно этого параметра в postgresql.conf.
Если же требуется использовать в кластере разные конфигурации, но для каждой базы данных
одну определённую, её можно задать командой ALTER DATABASE ... SET. В противном случае
конфигурацию можно выбрать в рамках сеанса, определив параметр default_text_search_config.
У каждой функции текстового поиска, зависящей от конфигурации, есть необязательный аргу-
мент regconfig, в котором можно явно указать конфигурацию для данной функции. Значение
default_text_search_config используется, только когда этот аргумент опущен.
373Полнотекстовый поиск
Для упрощения создания конфигураций текстового поиска они строятся из более простых объек-
тов. В PostgreSQL есть четыре типа таких объектов:
• Анализаторы текстового поиска разделяют документ на фрагменты и классифицируют их
(например, как слова или числа).
• Словари текстового поиска приводят фрагменты к нормализованной форме и отбрасывают
стоп-слова.
• Шаблоны текстового поиска предоставляют функции, образующие реализацию словарей.
(При создании словаря просто задаётся шаблон и набор параметров для него.)
• Конфигурации текстового поиска выбирают анализатор и набор словарей, который будет ис-
пользоваться для нормализации фрагментов, выданных анализатором.
Анализаторы и шаблоны текстового поиска строятся из низкоуровневых функций на языке C; что-
бы создать их, нужно программировать на C, а подключить их к базе данных может только супер-
пользователь. (В подкаталоге contrib/ инсталляции PostgreSQL можно найти примеры дополни-
тельных анализаторов и шаблонов.) Так как словари и конфигурации представляют собой просто
наборы параметров, связывающие анализаторы и шаблоны, их можно создавать, не имея админи-
стративных прав. Далее в этой главе будут приведены примеры их создания.
12.2. Таблицы и индексы
В предыдущем разделе приводились примеры, которые показывали, как можно выполнить сопо-
ставление с простыми текстовыми константами. В этом разделе показывается, как находить текст
в таблице, возможно с применением индексов.
12.2.1. Поиск в таблице
Полнотекстовый поиск можно выполнить, не применяя индекс. Следующий простой запрос выво-
дит заголовок (title) каждой строки, содержащей слово friend в поле body:
SELECT title
FROM pgweb
WHERE to_tsvector('english', body) @@ to_tsquery('english', 'friend');
При этом также будут найдены связанные слова, такие как friends и friendly, так как все они
сводятся к одной нормализованной лексеме.
В показанном выше примере для разбора и нормализации строки явно выбирается конфигурация
english. Хотя параметры, задающие конфигурацию, можно опустить:
SELECT title
FROM pgweb
WHERE to_tsvector(body) @@ to_tsquery('friend');
Такой запрос будет использовать конфигурацию, заданную в параметре default_text_search_config.
В следующем более сложном примере выбираются десять документов, изменённых последними,
со словами create и table в полях title или body:
SELECT title
FROM pgweb
WHERE to_tsvector(title || ' ' || body) @@ to_tsquery('create &amp; table')
ORDER BY last_mod_date DESC
LIMIT 10;
Чтобы найти строки, содержащие NULL в одном из полей, нужно воспользоваться функцией
coalesce, но здесь мы опустили её вызовы для краткости.
Хотя такие запросы будут работать и без индекса, для большинства приложений скорость будет
неприемлемой; этот подход рекомендуется только для нерегулярного поиска и динамического со-
держимого. Для практического применения полнотекстового поиска обычно создаются индексы.
374Полнотекстовый поиск
12.2.2. Создание индексов
Для ускорения текстового поиска мы можем создать индекс GIN (см. Раздел 12.9):
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector('english', body));
Заметьте, что здесь используется функция to_tsvector с двумя аргументами. В выражениях, опре-
деляющих индексы, можно использовать только функции, в которых явно задаётся имя конфигура-
ции текстового поиска (см. Раздел 11.7). Это объясняется тем, что содержимое индекса не должно
зависеть от значения параметра default_text_search_config. В противном случае содержимое ин-
декса может быть неактуальным, если разные его элементы tsvector будут создаваться с разными
конфигурациями текстового поиска и нельзя будет понять, какую именно использовать. Выгрузить
и восстановить такой индекс будет невозможно.
Так как при создании индекса использовалась версия to_tsvector с двумя аргументами, этот ин-
декс будет использоваться только в запросах, где to_tsvector вызывается с двумя аргументами и
во втором передаётся имя той же конфигурации. То есть, WHERE to_tsvector('english', body)
@@ 'a &amp; b' сможет использовать этот индекс, а WHERE to_tsvector(body) @@ 'a &amp; b' — нет. Это
гарантирует, что индекс будет использоваться только с той конфигурацией, с которой создавались
его элементы.
Индекс можно создать более сложным образом, определив для него имя конфигурации в другом
столбце таблицы, например:
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector(config_name, body));
где config_name — имя столбца в таблице pgweb. Так можно сохранить имя конфигурации, связан-
ной с элементом индекса, и, таким образом, иметь в одном индексе элементы с разными конфигу-
рациями. Это может быть полезно, например, когда в коллекции документов хранятся документы
на разных языках. И в этом случае в запросах должен использоваться тот же индекс (с таким же
образом задаваемой конфигурацией), например, так: WHERE to_tsvector(config_name, body) @@
'a &amp; b'.
Индексы могут создаваться даже по объединению столбцов:
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector('english', title || ' ' ||
body));
Ещё один вариант — создать отдельный столбец tsvector, в котором сохранить результат
to_tsvector. Следующий пример показывает, как можно подготовить для индексации объединён-
ное содержимое столбцов title и body, применив функцию coalesce для получения желаемого
результата, даже когда один из столбцов NULL:
ALTER TABLE pgweb ADD COLUMN textsearchable_index_col tsvector;
UPDATE pgweb SET textsearchable_index_col =
to_tsvector('english', coalesce(title,'') || ' ' || coalesce(body,''));
Затем мы создаём индекс GIN для ускорения поиска:
CREATE INDEX textsearch_idx ON pgweb USING GIN (textsearchable_index_col);
Теперь мы можем быстро выполнять полнотекстовый поиск:
SELECT title
FROM pgweb
WHERE textsearchable_index_col @@ to_tsquery('create &amp; table')
ORDER BY last_mod_date DESC
LIMIT 10;
Когда представление tsvector хранится в отдельном столбце, необходимо создать триггер, кото-
рый будет поддерживать столбец с tsvector в актуальном состоянии при любых изменениях title
или body. Как это сделать, рассказывается в Подразделе 12.4.3.
Хранение вычисленного выражения индекса в отдельном столбце даёт ряд преимуществ. Во-пер-
вых, для использования индекса в запросах не нужно явно указывать имя конфигурации тексто-
375Полнотекстовый поиск
вого поиска. Как показано в вышеприведённом примере, в этом случае запрос может зависеть от
default_text_search_config. Во-вторых, поиск выполняется быстрее, так как для проверки соот-
ветствия данных индексу не нужно повторно выполнять to_tsvector. (Это актуально больше для
индексов GiST, чем для GIN; см. Раздел 12.9.) С другой стороны, схему с индексом по выражению
проще реализовать и она позволяет сэкономить место на диске, так как представление tsvector
не хранится явно.
12.3. Управление текстовым поиском
Для реализации полнотекстового поиска необходимы функции, позволяющие создать tsvector из
документа и tsquery из запроса пользователя. Кроме того, результаты нужно выдавать в удобном
порядке, так что нам потребуется функция, оценивающая релевантность документа для данно-
го запроса. Важно также иметь возможность выводить найденный текст подходящим образом. В
PostgreSQL есть все необходимые для этого функции.
12.3.1. Разбор документов
Для преобразования документа в тип tsvector PostgreSQL предоставляет функцию to_tsvector.
to_tsvector([конфигурация regconfig,] документ text) returns tsvector
to_tsvector разбирает текстовый документ на фрагменты, сводит фрагменты к лексемам и воз-
вращает значение tsvector, в котором перечисляются лексемы и их позиции в документе. При
обработке документа используется указанная конфигурация текстового поиска или конфигурация
по умолчанию. Простой пример:
SELECT to_tsvector('english', 'a fat cat sat on a mat - it ate a fat rats');
to_tsvector
-----------------------------------------------------
'ate':9 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4
В этом примере мы видим, что результирующий tsvector не содержит слова a, on и it, слово rats
превратилось rat, а знак препинания «-» был проигнорирован.
Функция to_tsvector внутри вызывает анализатор, который разбивает текст документа на фраг-
менты и классифицирует их. Для каждого фрагмента она проверяет список словарей (Раздел 12.6),
определяемый типом фрагмента. Первый же словарь, распознавший фрагмент, выдаёт одну или
несколько представляющих его лексем. Например, rats превращается в rat, так как один из сло-
варей понимает, что слово rats — это слово rat во множественном числе. Некоторое слова рас-
познаются как стоп-слова (Подраздел 12.6.1) и игнорируются как слова, фигурирующие в тексте
настолько часто, что искать их бессмысленно. В нашем примере это a, on и it. Если фрагмент не
воспринимается ни одним словарём из списка, он так же игнорируется. В данном примере это
происходит со знаком препинания -, так как с таким типом фрагмента (символы-разделители) не
связан никакой словарь и значит такие фрагменты никогда не будут индексироваться. Выбор ана-
лизатора, словарей и индексируемых типов фрагментов определяется конфигурацией текстового
поиска (Раздел 12.7). В одной базе данных можно использовать разные конфигурации, в том числе,
предопределённые конфигурации для разных языков. В нашем примере мы использовали конфи-
гурацию по умолчанию для английского языка — english.
Для назначения элементам tsvector разных весов используется функция setweight. Вес элемента
задаётся буквой A, B, C или D. Обычно это применяется для обозначения важности слов в разных
частях документа, например в заголовке или в теле документа. Затем эта информация может ис-
пользоваться при ранжировании результатов поиска.
Так как to_tsvector(NULL) вернёт NULL, мы советуем использовать coalesce везде, где соответству-
ющее поле может быть NULL. Создавать tsvector из структурированного документа рекоменду-
ется так:
UPDATE tt SET ti =
setweight(to_tsvector(coalesce(title,'')), 'A')
376
||Полнотекстовый поиск
setweight(to_tsvector(coalesce(keyword,'')), 'B') ||
setweight(to_tsvector(coalesce(abstract,'')), 'C') ||
setweight(to_tsvector(coalesce(body,'')), 'D');
Здесь мы использовали setweight для пометки происхождения каждой лексемы в сформирован-
ных значениях tsvector и объединили помеченные значения с помощью оператора конкатенации
типов tsvector ||. (Подробнее эти операции рассматриваются в Подразделе 12.4.1.)
12.3.2. Разбор запросов
PostgreSQL предоставляет функции to_tsquery, plainto_tsquery, phraseto_tsquery и
websearch_to_tsquery для приведения запроса к типу tsquery. Функция to_tsquery даёт больше
возможностей, чем plainto_tsquery или phraseto_tsquery, но более строга к входным данным.
Функция websearch_to_tsquery представляет собой упрощённую версию to_tsquery с альтерна-
тивным синтаксисом, подобным тому, что принят в поисковых системах в Интернете.
to_tsquery([конфигурация regconfig,] текст_запроса text) returns tsquery
to_tsquery создаёт значение tsquery из текста_запроса, который может состоять из простых
фрагментов, разделённых логическими операторами tsquery: &amp; (И), | (ИЛИ), ! (НЕ) и &lt;-&gt; (ПРЕД-
ШЕСТВУЕТ), возможно, сгруппированных скобками. Другими словами, входное значение для
to_tsquery должно уже соответствовать общим правилам для значений tsquery, описанным в Под-
разделе 8.11.2. Различие их состоит в том, что во вводимом в tsquery значении фрагменты вос-
принимаются буквально, тогда как to_tsquery нормализует фрагменты, приводя их к лексемам,
используя явно указанную или подразумеваемую конфигурацию, и отбрасывая стоп-слова. Напри-
мер:
SELECT to_tsquery('english', 'The &amp; Fat &amp; Rats');
to_tsquery
---------------
'fat' &amp; 'rat'
Как и при вводе значения tsquery, для каждой лексемы можно задать вес(а), чтобы при поиске
можно было выбрать из tsvector только лексемы с заданными весами. Например:
SELECT to_tsquery('english', 'Fat | Rats:AB');
to_tsquery
------------------
'fat' | 'rat':AB
К лексеме также можно добавить *, определив таким образом условие поиска по префиксу:
SELECT to_tsquery('supern:*A &amp; star:A*B');
to_tsquery
--------------------------
'supern':*A &amp; 'star':*AB
Такая лексема будет соответствовать любому слову в tsvector, начинающемуся с данной подстро-
ки.
to_tsquery может также принимать фразы в апострофах. Это полезно в основном когда конфигу-
рация включает тезаурус, который может обрабатывать такие фразы. В показанном ниже примере
предполагается, что тезаурус содержит правило supernovae stars : sn:
SELECT to_tsquery('''supernovae stars'' &amp; !crab');
to_tsquery
---------------
'sn' &amp; !'crab'
Если убрать эти апострофы, to_tsquery не примет фрагменты, не разделённые операторами И,
ИЛИ и ПРЕДШЕСТВУЕТ, и выдаст синтаксическую ошибку.
plainto_tsquery([конфигурация regconfig,] текст_запроса text) returns tsquery
377Полнотекстовый поиск
plainto_tsquery преобразует неформатированный текст_запроса в значение tsquery. Текст раз-
бирается и нормализуется подобно тому, как это делает to_tsvector, а затем между оставшимися
словами вставляются операторы &amp; (И) типа tsquery.
Пример:
SELECT plainto_tsquery('english', 'The Fat Rats');
plainto_tsquery
-----------------
'fat' &amp; 'rat'
Заметьте, что plainto_tsquery не распознает во входной строке операторы tsquery, метки весов
или обозначения префиксов:
SELECT plainto_tsquery('english', 'The Fat &amp; Rats:C');
plainto_tsquery
---------------------
'fat' &amp; 'rat' &amp; 'c'
В данном случае все знаки пунктуации были отброшены как символы-разделители.
phraseto_tsquery([конфигурация regconfig,] текст_запроса text) returns tsquery
phraseto_tsquery ведёт себя подобно plainto_tsquery, за исключением того, что она вставляет
между оставшимися словами оператор &lt;-&gt; (ПРЕДШЕСТВУЕТ) вместо оператора &amp; (И). Кроме того,
стоп-слова не просто отбрасываются, а подсчитываются, и вместо операторов &lt;-&gt; используются
операторы <N> с подсчитанным числом. Эта функция полезна при поиске точных последователь-
ностей лексем, так как операторы ПРЕДШЕСТВУЕТ проверяют не только наличие всех лексем, но
и их порядок.
Пример:
SELECT phraseto_tsquery('english', 'The Fat Rats');
phraseto_tsquery
------------------
'fat' &lt;-&gt; 'rat'
Как и plainto_tsquery, функция phraseto_tsquery не распознает во входной строке операторы
типа tsquery, метки весов или обозначения префиксов:
SELECT phraseto_tsquery('english', 'The Fat &amp; Rats:C');
phraseto_tsquery
-----------------------------
'fat' &lt;-&gt; 'rat' &lt;-&gt; 'c'
websearch_to_tsquery([конфигурация regconfig,] текст_запроса text) returns tsquery
Функция websearch_to_tsquery создаёт значение tsquery из текста_запроса, используя альтер-
нативный синтаксис, в котором запрос задаётся просто неформатированным текстом. В отличие
от plainto_tsquery и phraseto_tsquery, она также принимает определённые операторы. Более
того, эта функция не должна никогда выдавать синтаксические ошибки, что позволяет осуществ-
лять поиск по произвольному заданному пользователем запросу. Она поддерживает следующий
синтаксис:
• текст не в кавычках: текст, не заключённый в кавычки, который будет преобразован в слова,
разделяемые операторами &amp;, как его восприняла бы функция plainto_tsquery.
• "текст в кавычках": текст, заключённый в кавычки, будет преобразован в слова, разделяе-
мые операторами &lt;-&gt;, как его восприняла бы функция phraseto_tsquery.
• OR: логическая операция ИЛИ будет преобразована в оператор |.
• -: логическая операция НЕ, преобразуется в оператор !.
Примеры:
SELECT websearch_to_tsquery('english', 'The fat rats');
378Полнотекстовый поиск
websearch_to_tsquery
----------------------
'fat' &amp; 'rat'
(1 row)
SELECT websearch_to_tsquery('english', '"supernovae stars" -crab');
websearch_to_tsquery
----------------------------------
'supernova' &lt;-&gt; 'star' &amp; !'crab'
(1 row)
SELECT websearch_to_tsquery('english', '"sad cat" or "fat rat"');
websearch_to_tsquery
-----------------------------------
'sad' &lt;-&gt; 'cat' | 'fat' &lt;-&gt; 'rat'
(1 row)
SELECT websearch_to_tsquery('english', 'signal -"segmentation fault"');
websearch_to_tsquery
---------------------------------------
'signal' &amp; !( 'segment' &lt;-&gt; 'fault' )
(1 row)
SELECT websearch_to_tsquery('english', '""" )( dummy \\ query &lt;-&gt;');
websearch_to_tsquery
----------------------
'dummi' &amp; 'queri'
(1 row)
12.3.3. Ранжирование результатов поиска
Ранжирование документов можно представить как попытку оценить, насколько они релевантны
заданному запросу и отсортировать их так, чтобы наиболее релевантные выводились первыми. В
PostgreSQL встроены две функции ранжирования, принимающие во внимание лексическую, пози-
ционную и структурную информацию; то есть, они учитывают, насколько часто и насколько близ-
ко встречаются в документе ключевые слова и какова важность содержащей их части документа.
Однако само понятие релевантности довольно размытое и во многом определяется приложением.
Приложения могут использовать для ранжирования и другую информацию, например, время из-
менения документа. Встроенные функции ранжирования можно рассматривать лишь как примеры
реализации. Для своих конкретных задач вы можете разработать собственные функции ранжиро-
вания и/или учесть при обработке их результатов дополнительные факторы.
Ниже описаны две встроенные функции ранжирования:
ts_rank([веса float4[],] вектор tsvector, запрос tsquery [, нормализация integer]) returns
float4
Ранжирует векторы по частоте найденных лексем.
ts_rank_cd([веса float4[],] вектор tsvector, запрос tsquery [, нормализация integer])
returns float4
Эта функция вычисляет плотность покрытия для данного вектора документа и запроса, ис-
пользуя метод, разработанный Кларком, Кормаком и Тадхоуп и описанный в статье "Relevance
Ranking for One to Three Term Queries" в журнале "Information Processing and Management" в
1999 г. Плотность покрытия вычисляется подобно рангу ts_rank, но в расчёт берётся ещё и
близость соответствующих лексем друг к другу.
Для вычисления результата этой функции требуется информация о позиции лексем. Поэтому
она игнорируют «очищенные» от этой информации лексемы в tsvector. Если во входных дан-
379Полнотекстовый поиск
ных нет неочищенных лексем, результат будет равен нулю. (За дополнительными сведениями о
функции strip и позиционной информации в данных tsvector обратитесь к Подразделу 12.4.1.)
Для обеих этих функций аргумент веса позволяет придать больший или меньший вес словам, в
зависимости от их меток. В передаваемом массиве весов определяется, насколько весома каждая
категория слов, в следующем порядке:
{вес D, вес C, вес B, вес A}
Если этот аргумент опускается, подразумеваются следующие значения:
{0.1, 0.2, 0.4, 1.0}
Обычно весами выделяются слова из особых областей документа, например из заголовка или крат-
кого введения, с тем, чтобы эти слова считались более и менее значимыми, чем слова в основном
тексте документа.
Так как вероятность найти ключевые слова увеличивается с размером документа, при ранжиро-
вании имеет смысл учитывать его, чтобы, например, документ с сотней слов, содержащий пять
вхождений искомых слов, считался более релевантным, чем документ с тысячей слов и теми же
пятью вхождениями. Обе функции ранжирования принимают целочисленный параметр нормали-
зации, определяющий, как ранг документа будет зависеть от его размера. Этот параметр пред-
ставляет собой битовую маску и управляет несколькими режимами: вы можете включить сразу
несколько режимов, объединив значения оператором | (например так: 2|4).
•
•
•
•
0 (по умолчанию): длина документа не учитывается
1: ранг документа делится на 1 + логарифм длины документа
2: ранг документа делится на его длину
4: ранг документа делится на среднее гармоническое расстояние между блоками (это реали-
зовано только в ts_rank_cd)
• 8: ранг документа делится на число уникальных слов в документе
• 16: ранг документа делится на 1 + логарифм числа уникальных слов в документе
• 32: ранг делится своё же значение + 1
Если включены несколько флагов, соответствующие операции выполняются в показанном поряд-
ке.
Важно заметить, что функции ранжирования не используют никакую внешнюю информацию, так
что добиться нормализации до 1% или 100% невозможно, хотя иногда это желательно. Применив
параметр 32 (rank/(rank+1)), можно свести все ранги к диапазону 0..1, но это изменение будет
лишь косметическим, на порядке сортировки результатов это не отразится.
В данном примере выбираются десять найденных документов с максимальным рангом:
SELECT title, ts_rank_cd(textsearch, query) AS rank
FROM apod, to_tsquery('neutrino|(dark &amp; matter)') query
WHERE query @@ textsearch
ORDER BY rank DESC
LIMIT 10;
title
|
rank
-----------------------------------------------+----------
Neutrinos in the Sun
|
3.1
The Sudbury Neutrino Detector
|
2.4
A MACHO View of Galactic Dark Matter
| 2.01317
Hot Gas and Dark Matter
| 1.91171
The Virgo Cluster: Hot Plasma and Dark Matter | 1.90953
Rafting for Solar Neutrinos
|
1.9
NGC 4650A: Strange Galaxy and Dark Matter
| 1.85774
Hot Gas and Dark Matter
|
1.6123
Ice Fishing for Cosmic Neutrinos
|
1.6
Weak Lensing Distorts the Universe
| 0.818218
Тот же пример с нормализованным рангом:
380Полнотекстовый поиск
SELECT title, ts_rank_cd(textsearch, query, 32 /* rank/(rank+1) */ ) AS rank
FROM apod, to_tsquery('neutrino|(dark &amp; matter)') query
WHERE query @@ textsearch
ORDER BY rank DESC
LIMIT 10;
title
|
rank
-----------------------------------------------+-------------------
Neutrinos in the Sun
| 0.756097569485493
The Sudbury Neutrino Detector
| 0.705882361190954
A MACHO View of Galactic Dark Matter
| 0.668123210574724
Hot Gas and Dark Matter
| 0.65655958650282
The Virgo Cluster: Hot Plasma and Dark Matter | 0.656301290640973
Rafting for Solar Neutrinos
| 0.655172410958162
NGC 4650A: Strange Galaxy and Dark Matter
| 0.650072921219637
Hot Gas and Dark Matter
| 0.617195790024749
Ice Fishing for Cosmic Neutrinos
| 0.615384618911517
Weak Lensing Distorts the Universe
| 0.450010798361481
Ранжирование может быть довольно дорогостоящей операцией, так как для вычисления ранга
необходимо прочитать tsvector каждого подходящего документа и это займёт значительное вре-
мя, если придётся обращаться к диску. К сожалению, избежать этого вряд ли возможно, так как
на практике по многим запросам выдаётся большое количество результатов.
12.3.4. Выделение результатов
Представляя результаты поиска, в идеале нужно выделять часть документа и показывать, как он
связан с запросом. Обычно поисковые системы показывают фрагменты документа с отмеченны-
ми искомыми словами. В PostgreSQL для реализации этой возможности представлена функция
ts_headline.
ts_headline([конфигурация regconfig,] документ text, запрос tsquery [, параметры text])
returns text
ts_headline принимает документ вместе с запросом и возвращает выдержку из документа, в
которой выделяются слова из запроса. Применяемую для разбора документа конфигурацию
можно указать в параметре config; если этот параметр опущен, применяется конфигурация
default_text_search_config.
Если в параметрах передаётся строка options, она должна состоять из списка разделённых запя-
тыми пар параметр=значение. Параметры могут быть следующими:
• StartSel, StopSel: строки, которые будут разграничивать слова запроса в документе, выделяя
их среди остальных. Если эти строки содержат пробелы или запятые, их нужно заключить в
кавычки.
• MaxWords, MinWords: эти числа определяет нижний и верхний предел размера выдержки.
• ShortWord: слова такой длины или короче в начале и конце выдержки будут отбрасываться.
Значение по умолчанию, равное 3, исключает распространённые английские артикли.
• HighlightAll: логический флаг; если он равен true, выдержкой будет весь документ и три
предыдущие параметра игнорируются.
• MaxFragments: максимальное число выводимых текстовых выдержек или фрагментов. Значе-
ние по умолчанию, равное 0, выбирает метод создания выдержки без фрагментов. При значе-
нии большем 0 выбирается метод с фрагментами, когда находятся все фрагменты, содержа-
щие как можно больше слов запроса, а затем они сжимаются до слов запроса. Такие фрагмен-
ты могут содержать какие-то ключевые слова в середине и ограничиваются двумя искомыми
словами. При этом фрагменты могут содержать не больше MaxWords слов, а в начале и конце
они будут очищены от слов длины ShortWord и меньше. Если в документе найдены не все сло-
ва запроса, выводится один фрагмент, включающий первые MinWords слов в документе.
• FragmentDelimiter: Когда выводятся несколько фрагментов, они будут разделяться этой стро-
кой.
381Полнотекстовый поиск
Имена этих параметров распознаются без учёта регистра. Явно не определённые параметры по-
лучают такие значения по умолчанию:
StartSel=<b>, StopSel=</b>,
MaxWords=35, MinWords=15, ShortWord=3, HighlightAll=FALSE,
MaxFragments=0, FragmentDelimiter=" ... "
Пример использования:
SELECT ts_headline('english',
'The most common type of search
is to find all documents containing given query terms
and return them in order of their similarity to the
query.',
to_tsquery('query &amp; similarity'));
ts_headline
------------------------------------------------------------
containing given <b>query</b> terms
and return them in order of their <b>similarity</b> to the
<b>query</b>.
SELECT ts_headline('english',
'The most common type of search
is to find all documents containing given query terms
and return them in order of their similarity to the
query.',
to_tsquery('query &amp; similarity'),
'StartSel = &lt;, StopSel = &gt;');
ts_headline
-------------------------------------------------------
containing given <query> terms
and return them in order of their <similarity> to the</similarity></query></N></N></p>
<query>.
Функция ts_headline работает с оригинальным документом, а не его сжатым представлением
tsvector, так что она может быть медленной и использовать её следует осмотрительно.
12.4. Дополнительные возможности
В этом разделе описываются дополнительные функции и операторы, которые могут быть полезны
при поиске текста.
12.4.1. Обработка документов
В Подразделе 12.3.1 показывалось, как обычные текстовые документы можно преобразовать в зна-
чения tsvector. PostgreSQL предлагает также набор функций и операторов для обработки доку-
ментов, уже представленных в формате tsvector.
tsvector || tsvector
Оператор конкатенации значений tsvector возвращает вектор, объединяющий лексемы и по-
зиционную информацию двух векторов, переданных ему в аргументах. В полученном результа-
те сохраняются позиции и метки весов. При этом позиции в векторе справа сдвигаются на мак-
симальное значение позиции в векторе слева, что почти равносильно применению to_tsvector
к результату конкатенации двух исходных строк документов. (Почти, потому что стоп-слова,
исключаемые в конце левого аргумента, при конкатенации исходных строк влияют на позиции
лексем в правой части, а при конкатенации tsvector — нет.)
Преимущество же конкатенации документов в векторной форме по сравнению с конкатенаци-
ей текста до вызова to_tsvector заключается в том, что так можно разбирать разные части
документа, применяя разные конфигурации. И так как функция setweight помечает все лексе-
382Полнотекстовый поиск
мы данного вектора одинаково, разбирать текст и выполнять setweight нужно до объединения
разных частей документа с подразумеваемым разным весом.
setweight(вектор tsvector, вес "char") returns tsvector
setweight возвращает копию входного вектора, помечая в ней каждую позицию заданным ве-
сом, меткой A, B, C или D. (Метка D по умолчанию назначается всем векторам, так что при выводе
она опускается.) Эти метки сохраняются при конкатенации векторов, что позволяет придавать
разные веса словам из разных частей документа и, как следствие, ранжировать их по-разному.
Заметьте, что веса назначаются позициям, а не лексемам. Если входной вектор очищен от
позиционной информации, setweight не делает ничего.
length(вектор tsvector) returns integer
Возвращает число лексем, сохранённых в векторе.
strip(вектор tsvector) returns tsvector
Возвращает вектор с теми же лексемами, что и в данном, но без информации о позиции и весе.
Очищенный вектор обычно оказывается намного меньше исходного, но при этом и менее по-
лезным. С очищенными векторами хуже работает ранжирование, а также оператор &lt;-&gt; (ПРЕД-
ШЕСТВУЕТ) типа tsquery никогда не найдёт соответствие в них, так как не сможет определить
расстояние между вхождениями лексем.
Полный список связанных с tsvector функций приведён в Таблице 9.41.
12.4.2. Обработка запросов
В Подразделе 12.3.2 показывалось, как обычные текстовые запросы можно преобразовывать в зна-
чения tsquery. PostgreSQL предлагает также набор функций и операторов для обработки запро-
сов, уже представленных в формате tsquery.
tsquery &amp;&amp; tsquery
Возвращает логическое произведение (AND) двух данных запросов.
tsquery || tsquery
Возвращает логическое объединение (OR) двух данных запросов.
!! tsquery
Возвращает логическое отрицание (NOT) данного запроса.
tsquery &lt;-&gt; tsquery
Возвращает запрос, который ищет соответствие первому данному запросу, за которым следует
соответствие второму данному запросу, с применением оператора &lt;-&gt; (ПРЕДШЕСТВУЕТ) типа
tsquery. Например:
SELECT to_tsquery('fat') &lt;-&gt; to_tsquery('cat | rat');
?column?
-----------------------------------
'fat' &lt;-&gt; 'cat' | 'fat' &lt;-&gt; 'rat'
tsquery_phrase(запрос1 tsquery, запрос2 tsquery [, расстояние integer ]) returns tsquery
Возвращает запрос, который ищет соответствие первому данному запросу, за которым следует
соответствие второму данному запросу (число лексем между ними задаётся параметром рас-
стояние), с применением оператора <N> типа tsquery. Например:
SELECT tsquery_phrase(to_tsquery('fat'), to_tsquery('cat'), 10);
tsquery_phrase
------------------
'fat' &lt;10&gt; 'cat'
383Полнотекстовый поиск
numnode(запрос tsquery) returns integer
Возвращает число узлов (лексем и операторов) в значении tsquery. Эта функция помогает
определить, имеет ли смысл запрос (тогда её результат &gt; 0) или он содержит только стоп-слова
(тогда она возвращает 0). Примеры:
SELECT numnode(plainto_tsquery('the any'));
ЗАМЕЧАНИЕ: запрос поиска текста игнорируется, так как содержит
только стоп-слова или не содержит лексем
numnode
---------
0
SELECT numnode('foo &amp; bar'::tsquery);
numnode
---------
3
querytree(запрос tsquery) returns text
Возвращает часть tsquery, которую можно использовать для поиска по индексу. Эта функция
помогает выявить неиндексируемые запросы, например, такие, которые содержат только стоп-
слова или условия отрицания. Например:
SELECT querytree(to_tsquery('!defined'));
querytree
-----------
12.4.2.1. Перезапись запросов
Семейство запросов ts_rewrite ищет в данном tsquery вхождения целевого подзапроса и заменя-
ет каждое вхождение указанной подстановкой. По сути эта операция похожа на замену подстроки
в строке, только рассчитана на работу с tsquery. Сочетание целевого подзапроса с подстановкой
можно считать правилом перезаписи запроса. Набор таких правил перезаписи может быть очень
полезен при поиске. Например, вы можете улучшить результаты, добавив синонимы (например,
big apple, nyc и gotham для new york) или сузить область поиска, чтобы нацелить пользователя на
некоторую область. Это в некотором смысле пересекается с функциональностью тезаурусов (Под-
раздел  12.6.4). Однако, при таком подходе вы можете изменять правила перезаписи «на лету»,
тогда как при обновлении тезауруса необходима переиндексация.
ts_rewrite (запрос tsquery, цель tsquery, замена tsquery) returns tsquery
Эта форма ts_rewrite просто применяет одно правило перезаписи: цель заменяется подста-
новкой везде, где она находится в запросе. Например:
SELECT ts_rewrite('a &amp; b'::tsquery, 'a'::tsquery, 'c'::tsquery);
ts_rewrite
------------
'b' &amp; 'c'
ts_rewrite (запрос tsquery, выборка text) returns tsquery
Эта форма ts_rewrite принимает начальный запрос и SQL-команду select, которая задаётся
текстовой строкой. Команда select должна выдавать два столбца типа tsquery. Для каждой
строки результата select вхождения первого столбца (цели) заменяются значениями второго
столбца (подстановкой) в тексте запроса. Например:
CREATE TABLE aliases (t tsquery PRIMARY KEY, s tsquery);
INSERT INTO aliases VALUES('a', 'c');
SELECT ts_rewrite('a &amp; b'::tsquery, 'SELECT t,s FROM aliases');
ts_rewrite
384Полнотекстовый поиск
------------
'b' &amp; 'c'
Заметьте, что когда таким способом применяются несколько правил перезаписи, порядок их
применения может иметь значение, поэтому в исходном запросе следует добавить ORDER BY
по какому-либо ключу.
Давайте рассмотрим практический пример на тему астрономии. Мы развернём запрос supernovae,
используя правила перезаписи в таблице:
CREATE TABLE aliases (t tsquery primary key, s tsquery);
INSERT INTO aliases VALUES(to_tsquery('supernovae'),
to_tsquery('supernovae|sn'));
SELECT ts_rewrite(to_tsquery('supernovae &amp; crab'), 'SELECT * FROM aliases');
ts_rewrite
---------------------------------
'crab' &amp; ( 'supernova' | 'sn' )
Мы можем скорректировать правила перезаписи, просто изменив таблицу:
UPDATE aliases
SET s = to_tsquery('supernovae|sn &amp; !nebulae')
WHERE t = to_tsquery('supernovae');
SELECT ts_rewrite(to_tsquery('supernovae &amp; crab'), 'SELECT * FROM aliases');
ts_rewrite
---------------------------------------------
'crab' &amp; ( 'supernova' | 'sn' &amp; !'nebula' )
Перезапись может быть медленной, когда задано много правил перезаписи, так как соответствия
будут проверяться для каждого правила. Чтобы отфильтровать явно неподходящие правила, можно
использовать проверки включения для типа tsquery. В следующем примере выбираются только те
правила, которые могут соответствовать исходному запросу:
SELECT ts_rewrite('a &amp; b'::tsquery,
'SELECT t,s FROM aliases WHERE ''a &amp; b''::tsquery @&gt; t');
ts_rewrite
------------
'b' &amp; 'c'
12.4.3. Триггеры для автоматического обновления
Когда представление документа в формате tsvector хранится в отдельном столбце, необходимо
создать триггер, который будет обновлять его содержимое при изменении столбцов, из которых со-
ставляется исходный документ. Для этого можно использовать две встроенные триггерные функ-
ции или написать свои собственные.
tsvector_update_trigger(столбец_tsvector, имя_конфигурации, столбец_текста [, ...])
tsvector_update_trigger_column(столбец_tsvector, столбец_конфигурации,
столбец_текста [, ...])
Эти триггерные функции автоматически вычисляют значение для столбца tsvector из одного или
нескольких текстовых столбцов с параметрами, указанными в команде CREATE TRIGGER. Пример
их использования:
CREATE TABLE messages (
title
text,
body
text,
tsv
tsvector
);
385Полнотекстовый поиск
CREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE
ON messages FOR EACH ROW EXECUTE FUNCTION
tsvector_update_trigger(tsv, 'pg_catalog.english', title, body);
INSERT INTO messages VALUES('title here', 'the body text is here');
SELECT * FROM messages;
title
|
body
|
tsv
------------+-----------------------+----------------------------
title here | the body text is here | 'bodi':4 'text':5 'titl':1
SELECT title, body FROM messages WHERE tsv @@ to_tsquery('title &amp; body');
title
|
body
------------+-----------------------
title here | the body text is here
С таким триггером любое изменение в полях title или body будет автоматически отражаться в
содержимом tsv, так что приложению не придётся заниматься этим.
Первым аргументом этих функций должно быть имя столбца tsvector, содержимое которого будет
обновляться. Ещё один аргумент — конфигурация текстового поиска, которая будет использовать-
ся для преобразования. Для tsvector_update_trigger имя конфигурации передаётся просто как
второй аргумент триггера. Это имя должно быть определено полностью, чтобы поведение триггера
не менялось при изменениях в пути поиска (search_path). Для tsvector_update_trigger_column
во втором аргументе триггера передаётся имя другого столбца таблицы, который должен иметь
тип regconfig. Это позволяет использовать разные конфигурации для разных строк. В оставшихся
аргументах передаются имена текстовых столбцов (типа text, varchar или char). Их содержимое
будет включено в документ в заданном порядке. При этом значения NULL будут пропущены (а
другие столбцы будут индексироваться).
Ограничение этих встроенных триггеров заключается в том, что они обрабатывают все столбцы
одинаково. Чтобы столбцы обрабатывались по-разному, например для текста заголовка задавался
не тот же вес, что для тела документа, потребуется разработать свой триггер. К примеру, так это
можно сделать на языке PL/pgSQL:
CREATE FUNCTION messages_trigger() RETURNS trigger AS $$
begin
new.tsv :=
setweight(to_tsvector('pg_catalog.english', coalesce(new.title,'')), 'A') ||
setweight(to_tsvector('pg_catalog.english', coalesce(new.body,'')), 'D');
return new;
end
$$ LANGUAGE plpgsql;
CREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE
ON messages FOR EACH ROW EXECUTE FUNCTION messages_trigger();
Помните, что, создавая значения tsvector в триггерах, важно явно указывать имя конфигурации,
чтобы содержимое столбца не зависело от изменений default_text_search_config. В противном
случае могут возникнуть проблемы, например результаты поиска изменятся после выгрузки и вос-
становления данных.
12.4.4. Сбор статистики по документу
Функция ts_stat может быть полезна для проверки конфигурации и нахождения возможных стоп-
слов.
ts_stat(sql_запрос text, [веса text,]
OUT слово text, OUT число_док integer,
OUT число_вхожд integer) returns setof record
386Полнотекстовый поиск
Здесь sql_запрос — текстовая строка, содержащая SQL-запрос, который должен возвращать один
столбец tsvector. Функция ts_stat выполняет запрос и возвращает статистику по каждой отдель-
ной лексеме (слову), содержащейся в данных tsvector. Её результат представляется в столбцах
• слово text — значение лексемы
• число_док integer — число документов (значений tsvector), в которых встретилось слово
• число_вхожд integer — общее число вхождений слова
Если передаётся параметр weights, то подсчитываются только вхождения с указанными в нём
весами.
Например, найти десять наиболее часто используемых слов в коллекции документов можно так:
SELECT * FROM ts_stat('SELECT vector FROM apod')
ORDER BY nentry DESC, ndoc DESC, word
LIMIT 10;
Следующий запрос возвращает тоже десять слов, но при выборе их учитываются только вхождения
с весами A или B:
SELECT * FROM ts_stat('SELECT vector FROM apod', 'ab')
ORDER BY nentry DESC, ndoc DESC, word
LIMIT 10;
12.5. Анализаторы
Задача анализаторов текста — разделить текст документа на фрагменты и присвоить каждому
из них тип из набора, определённого в самом анализаторе. Заметьте, что анализаторы не меняют
текст — они просто выдают позиции предполагаемых слов. Вследствие такой ограниченности их
функций, собственные специфические анализаторы бывают нужны гораздо реже, чем собственные
словари. В настоящее время в PostgreSQL есть только один встроенный анализатор, который может
быть полезен для широкого круга приложений.
Этот встроенный анализатор называется pg_catalog.default. Он распознаёт 23 типа фрагментов,
перечисленные в Таблице 12.1.
Таблица 12.1. Типы фрагментов, выделяемых стандартным анализатором
Псевдоним Описание
Пример
asciiword Слово только из букв ASCII
elephant
word Слово из любых букв
mañana
numword Слово из букв и цифр
beta1
asciihword Слово только из букв ASCII с де- up-to-date
фисами
hword Слово из любых букв с дефиса- lógico-matemática
ми
numhword Слово из букв и цифр с дефиса- postgresql-beta1
ми
hword_asciipart Часть слова с дефисами, только postgresql в словосочетании
из букв ASCII
postgresql-beta1
hword_part Часть слова с дефисами, из лю- lógico или matemática в слово-
бых букв
сочетании lógico-matemática
hword_numpart Часть слова с дефисами, из букв beta1
в
словосочетании
и цифр
postgresql-beta1
email Адрес электронной почты
foo@example.com
protocol Префикс протокола
http://
387Полнотекстовый поиск
Псевдоним Описание Пример
url URL example.com/stuff/
index.html
host Имя узла example.com
url_path Путь в адресе URL /stuff/index.html, как часть
URL
file Путь или имя файла /usr/local/foo.txt, если не яв-
ляется частью URL
sfloat Научная запись числа -1.234e56
float Десятичная запись числа -1.234
int Целое со знаком -1234
uint Целое без знака 1234
version Номер версии 8.3.0
tag Тег XML &lt;a
"dictionaries.html"&gt;
entity Сущность XML &amp;
blank Символы-разделители (любые пробельные символы
или знаки препинания, не по-
павшие в другие категории)
href=
Примечание
Понятие «буквы» анализатор определяет исходя из локали, заданной для базы данных,
в частности параметра lc_ctype. Слова, содержащие только буквы из ASCII (латинские
буквы), распознаются как фрагменты отдельного типа, так как иногда бывает полезно
выделить их. Для многих европейских языков типы фрагментов word и asciiword мож-
но воспринимать как синонимы.
email принимает не все символы, которые считаются допустимыми по стандарту RFC
5322. В частности, имя почтового ящика помимо алфавитно-цифровых символов может
содержать только точку, минус и подчёркивание.
Анализатор может выделить в одном тексте несколько перекрывающихся фрагментов. Например,
слово с дефисом будет выдано как целое составное слово и по частям:
SELECT alias, description, token FROM ts_debug('foo-bar-beta1');
alias
|
description
|
token
-----------------+------------------------------------------+--------------
numhword
| Hyphenated word, letters and digits
| foo-bar-beta1
hword_asciipart | Hyphenated word part, all ASCII
| foo
blank
| Space symbols
| -
hword_asciipart | Hyphenated word part, all ASCII
| bar
blank
| Space symbols
| -
hword_numpart
| Hyphenated word part, letters and digits | beta1
Это поведение считается желательным, так как это позволяет находить при последующем поиске
и всё слово целиком, и его части. Ещё один показательный пример:
SELECT alias, description, token
FROM ts_debug('http://example.com/stuff/index.html');
alias
| description |
token
----------+---------------+------------------------------
protocol | Protocol head | http://
388Полнотекстовый поиск
url
| URL
host
| Host
url_path | URL path
| example.com/stuff/index.html
| example.com
| /stuff/index.html
12.6. Словари
Словари полнотекстового поиска предназначены для исключения стоп-слов (слов, которые не
должны учитываться при поиске) и нормализации слов, чтобы разные словоформы считались сов-
падающими. Успешно нормализованное слово называется лексемой. Нормализация и исключение
стоп-слов не только улучшает качество поиска, но и уменьшает размер представления документа в
формате tsvector, и, как следствие, увеличивает быстродействие. Нормализация не всегда имеет
лингвистический смысл, обычно она зависит от требований приложения.
Несколько примеров нормализации:
• Лингвистическая нормализация — словари Ispell пытаются свести слова на входе к нормали-
зованной форме, а стеммеры убирают окончания слов
• Адреса URL могут быть канонизированы, чтобы например следующие адреса считались оди-
наковыми:
• http://www.pgsql.ru/db/mw/index.html
• http://www.pgsql.ru/db/mw/
• http://www.pgsql.ru/db/../db/mw/index.html
• Названия цветов могут быть заменены их шестнадцатеричными значениями, например red,
green, blue, magenta -&gt; FF0000, 00FF00, 0000FF, FF00FF
• При индексировании чисел можно отбросить цифры в дробной части для сокращения множе-
ства всевозможных чисел, чтобы например 3.14159265359, 3.1415926 и 3.14 стали одинаковы-
ми после нормализации, при которой после точки останутся только две цифры.
Словарь — это программа, которая принимает на вход фрагмент и возвращает:
• массив лексем, если входной фрагмент известен в словаре (заметьте, один фрагмент может
породить несколько лексем)
• одну лексему с установленным флагом TSL_FILTER для замены исходного фрагмента новым,
чтобы следующие словари работали с новым вариантом (словарь, который делает это, называ-
ется фильтрующим словарём)
• пустой массив, если словарь воспринимает этот фрагмент, но считает его стоп-словом
• NULL, если словарь не воспринимает полученный фрагмент
В PostgreSQL встроены стандартные словари для многих языков. Есть также несколько предопре-
делённых шаблонов, на основании которых можно создавать новые словари с изменёнными пара-
метрами. Все эти шаблоны описаны ниже. Если же ни один из них не подходит, можно создать и
свои собственные шаблоны. Соответствующие примеры можно найти в каталоге contrib/ инстал-
ляции PostgreSQL.
Конфигурация текстового поиска связывает анализатор с набором словарей, которые будут обра-
батывать выделенные им фрагменты. Для каждого типа фрагментов, выданных анализатором, в
конфигурации задаётся отдельный список словарей. Найденный анализатором фрагмент проходит
через все словари по порядку, пока какой-либо словарь не увидит в нём знакомое для него слово.
Если он окажется стоп-словом или его не распознает ни один словарь, этот фрагмент не будет
учитываться при индексации и поиске. Обычно результат определяет первый же словарь, который
возвращает не NULL, и остальные словари уже не проверяются; однако фильтрующий словарь мо-
жет заменить полученное слово другим, которое и будет передано следующим словарям.
Общее правило настройки списка словарей заключается в том, чтобы поставить наиболее частные
и специфические словари в начале, затем перечислить более общие и закончить самым общим
словарём, например стеммером Snowball или словарём simple, который распознаёт всё. Напри-
мер, для поиска по теме астрономии (конфигурация astro_en) тип фрагментов asciiword (слово из
букв ASCII) можно связать со словарём синонимов астрономических терминов, затем с обычным
английским словарём и наконец со стеммером английских окончаний Snowball:
389Полнотекстовый поиск
ALTER TEXT SEARCH CONFIGURATION astro_en
ADD MAPPING FOR asciiword WITH astrosyn, english_ispell, english_stem;
Фильтрующий словарь можно включить в любом месте списка, кроме конца, где он будет бесполе-
зен. Фильтрующие словари бывают полезны для частичной нормализации слов и упрощения зада-
чи следующих словарей. Например, фильтрующий словарь может удалить из текста диакритиче-
ские знаки, как это делает модуль unaccent.
12.6.1. Стоп-слова
Стоп-словами называются слова, которые встречаются очень часто, практически в каждом доку-
менте, и поэтому не имеют различительной ценности. Таким образом, при полнотекстовом поиске
их можно игнорировать. Например, в каждом английском тексте содержатся артикли a и the, так
что хранить их в индексе бессмысленно. Однако стоп-слова влияют на позиции лексем в значении
tsvector, от чего, в свою очередь, зависит ранжирование:
SELECT to_tsvector('english','in the list of stop words');
to_tsvector
----------------------------
'list':3 'stop':5 'word':6
В результате отсутствуют позиции 1,2,4, потому что фрагменты в этих позициях оказались стоп-
словами. Ранги, вычисленные для документов со стоп-словами и без них, могут значительно раз-
личаться:
SELECT ts_rank_cd (to_tsvector('english','in the list of stop words'),
to_tsquery('list &amp; stop'));
ts_rank_cd
------------
0.05
SELECT ts_rank_cd (to_tsvector('english','list stop words'),
to_tsquery('list &amp; stop'));
ts_rank_cd
------------
0.1
Как именно обрабатывать стоп-слова, определяет сам словарь. Например, словари ispell снача-
ла нормализуют слова, а затем просматривают список стоп-слов, тогда как стеммеры Snowball
просматривают свой список стоп-слов в первую очередь. Это различие в поведении объясняется
стремлением уменьшить шум.
12.6.2. Простой словарь
Работа шаблона словарей simple сводится к преобразованию входного фрагмента в нижний ре-
гистр и проверки результата по файлу со списком стоп-слов. Если это слово находится в файле,
словарь возвращает пустой массив и фрагмент исключается из дальнейшего рассмотрения. В про-
тивном случае словарь возвращает в качестве нормализованной лексемы слово в нижнем регистре.
Этот словарь можно настроить и так, чтобы все слова, кроме стоп-слов, считались неопознанными
и передавались следующему словарю в списке.
Определить словарь на основе шаблона simple можно так:
CREATE TEXT SEARCH DICTIONARY public.simple_dict (
TEMPLATE = pg_catalog.simple,
STOPWORDS = english
);
Здесь english — базовое имя файла со стоп-словами. Полным именем файла будет $SHAREDIR/
tsearch_data/english.stop, где $SHAREDIR указывает на каталог с общими данными PostgreSQL,
часто это /usr/local/share/postgresql (точно узнать его можно с помощью команды pg_config
390Полнотекстовый поиск
--sharedir). Этот текстовый файл должен содержать просто список слов, по одному слову в строке.
Пустые строки и окружающие пробелы игнорируются, все символы переводятся в нижний регистр
и на этом обработка файла заканчивается.
Теперь мы можем проверить наш словарь:
SELECT ts_lexize('public.simple_dict','YeS');
ts_lexize
-----------
{yes}
SELECT ts_lexize('public.simple_dict','The');
ts_lexize
-----------
{}
Мы также можем настроить словарь так, чтобы он возвращал NULL вместо слова в нижнем реги-
стре, если оно не находится в файле стоп-слов. Для этого нужно присвоить параметру Accept зна-
чение false. Продолжая наш пример:
ALTER TEXT SEARCH DICTIONARY public.simple_dict ( Accept = false );
SELECT ts_lexize('public.simple_dict','YeS');
ts_lexize
-----------
SELECT ts_lexize('public.simple_dict','The');
ts_lexize
-----------
{}
Со значением Accept = true (по умолчанию) словарь simple имеет смысл включать только в конце
списка словарей, так как он никогда не передаст фрагмент следующему словарю. И напротив,
Accept = false имеет смысл, только если за ним следует ещё минимум один словарь.
Внимание
Большинство словарей работают с дополнительными файлами, например, файлами
стоп-слов. Содержимое этих файлов должно иметь кодировку UTF-8. Если база данных
работает в другой кодировке, они будут переведены в неё, когда сервер будет загру-
жать их.
Внимание
Обычно в рамках одного сеанса дополнительный файл словаря загружается только
один раз, при первом использовании. Если же вы измените его и захотите, чтобы суще-
ствующие сеансы работали с новым содержимым, выполните для этого словаря коман-
ду ALTER TEXT SEARCH DICTIONARY. Это обновление словаря может быть «фиктивным»,
фактически не меняющим значения никаких параметров.
12.6.3. Словарь синонимов
Этот шаблон словарей используется для создания словарей, заменяющих слова синонимами. Сло-
восочетания такие словари не поддерживают (используйте для этого тезаурус (Подраздел 12.6.4)).
Словарь синонимов может помочь в преодолении лингвистических проблем, например, не дать
391Полнотекстовый поиск
стеммеру английского уменьшить слово «Paris» до «pari». Для этого достаточно поместить в сло-
варь синонимов строку Paris paris и поставить этот словарь перед словарём english_stem. На-
пример:
SELECT * FROM ts_debug('english', 'Paris');
alias |
description | token| dictionaries | dictionary | lexemes
----------+----------------+------+---------------+-------------+--------
asciiword| Word, all ASCII| Paris| {english_stem}| english_stem| {pari}
CREATE TEXT SEARCH DICTIONARY my_synonym (
TEMPLATE = synonym,
SYNONYMS = my_synonyms
);
ALTER TEXT SEARCH CONFIGURATION english
ALTER MAPPING FOR asciiword
WITH my_synonym, english_stem;
SELECT * FROM ts_debug('english', 'Paris');
alias |
description | token| dictionaries | dictionary| lexemes
----------+----------------+------+--------------+-----------+--------
asciiword| Word, all ASCII| Paris| {my_synonym, | my_synonym| {paris}
|
|
| english_stem}|
|
Шаблон synonym принимает единственный параметр, SYNONYMS, в котором задаётся базовое имя
его файла конфигурации — в данном примере это my_synonyms. Полным именем файла будет
$SHAREDIR/tsearch_data/my_synonyms.syn (где $SHAREDIR указывает на каталог общих данных
PostgreSQL). Содержимое этого файла должны составлять строки с двумя словами в каждой (пер-
вое — заменяемое слово, а второе — его синоним), разделёнными пробелами. Пустые строки и
окружающие пробелы при разборе этого файла игнорируются.
Шаблон synonym также принимает необязательный параметр CaseSensitive, который по умолча-
нию имеет значение false. Когда CaseSensitive равен false, слова в файле синонимов перево-
дятся в нижний регистр, вместе с проверяемыми фрагментами. Если же он не равен true, регистр
слов в файле и проверяемых фрагментов не меняются, они сравниваются «как есть».
В конце синонима в этом файле можно добавить звёздочку (*), тогда этот синоним будет рассмат-
риваться как префикс. Эта звёздочка будет игнорироваться в to_tsvector(), но to_tsquery() из-
менит результат, добавив в него маркер сопоставления префикса (см. Подраздел 12.3.2). Напри-
мер, предположим, что файл $SHAREDIR/tsearch_data/synonym_sample.syn имеет следующее со-
держание:
postgres
postgresql
postgre pgsql
gogle
googl
indices index*
pgsql
pgsql
С ним мы получим такие результаты:
mydb=# CREATE TEXT SEARCH DICTIONARY
syn (template=synonym, synonyms='synonym_sample');
mydb=# SELECT ts_lexize('syn','indices');
ts_lexize
-----------
{index}
(1 row)
mydb=# CREATE TEXT SEARCH CONFIGURATION tst (copy=simple);
mydb=# ALTER TEXT SEARCH CONFIGURATION tst ALTER MAPPING FOR asciiword
392Полнотекстовый поиск
WITH syn;
mydb=# SELECT to_tsvector('tst','indices');
to_tsvector
-------------
'index':1
(1 row)
mydb=# SELECT to_tsquery('tst','indices');
to_tsquery
------------
'index':*
(1 row)
mydb=# SELECT 'indexes are very useful'::tsvector;
tsvector
---------------------------------
'are' 'indexes' 'useful' 'very'
(1 row)
mydb=# SELECT 'indexes are very useful'::tsvector @@
to_tsquery('tst','indices');
?column?
----------
t
(1 row)
12.6.4. Тезаурус
Тезаурус (или сокращённо TZ) содержит набор слов и информацию о связях слов и словосочета-
ний, то есть более широкие понятия (Broader Terms, BT), более узкие понятия (Narrow Terms, NT),
предпочитаемые названия, исключаемые названия, связанные понятия и т. д.
В основном тезаурус заменяет исключаемые слова и словосочетания предпочитаемыми и может
также сохранить исходные слова для индексации. Текущая реализация тезауруса в PostgreSQL
представляет собой расширение словаря синонимов с поддержкой фраз. Конфигурация тезауруса
определяется файлом следующего формата:
# это комментарий
образец слов(а) : индексируемые слова
другой образец слов(а) : другие индексируемые слова
...
Здесь двоеточие (:) служит разделителем между исходной фразой и её заменой.
Прежде чем проверять соответствие фраз, тезаурус нормализует файл конфигурации, используя
внутренний словарь (который указывается в конфигурации словаря-тезауруса). Этот внутренний
словарь для тезауруса может быть только одним. Если он не сможет распознать какое-либо слово,
произойдёт ошибка. В этом случае необходимо либо исключить это слово, либо добавить его во
внутренний словарь. Также можно добавить звёздочку (*) перед индексируемыми словами, чтобы
они не проверялись по внутреннему словарю, но все слова-образцы должны быть известны внут-
реннему словарю.
Если входному фрагменту соответствуют несколько фраз в этом списке, тезаурус выберет самое
длинное определение, а если таких окажется несколько, самое последнее из них.
Выделить во фразе какие-то стоп-слова нельзя; вместо этого можно вставить ? в том месте, где
может оказаться стоп-слово. Например, в предположении, что a и the — стоп-слова по внутреннему
словарю:
? one ? two : swsw
393Полнотекстовый поиск
соответствует входным строкам a one the two и the one a two, так что обе эти строки будут
заменены на swsw.
Как и обычный словарь, тезаурус должен привязываться к лексемам определённых типов. Так как
тезаурус может распознавать фразы, он должен запоминать своё состояние и взаимодействовать с
анализатором. Учитывая свои привязки, он может либо обрабатывать следующий фрагмент, либо
прекратить накопление фразы. Поэтому настройка тезаурусов в системе требует особого внима-
ния. Например, если привязать тезаурус только к типу фрагментов asciiword, тогда определение
в тезаурусе one 7 не будет работать, так как этот тезаурус не связан с типом uint.
Внимание
Тезаурусы используются при индексации, поэтому при любом изменении параметров
или содержимого тезауруса необходима переиндексация. Для большинства других ти-
пов словарей при небольших изменениях, таких как удаление и добавление стоп-слов,
переиндексация не требуется.
12.6.4.1. Конфигурация тезауруса
Для создания нового словаря-тезауруса используется шаблон thesaurus. Например:
CREATE TEXT SEARCH DICTIONARY thesaurus_simple (
TEMPLATE = thesaurus,
DictFile = mythesaurus,
Dictionary = pg_catalog.english_stem
);
Здесь:
• thesaurus_simple — имя нового словаря
• mythesaurus — базовое имя файла конфигурации тезауруса. (Полным путём к файлу будет
$SHAREDIR/tsearch_data/mythesaurus.ths, где $SHAREDIR указывает на каталог общих данных
PostgreSQL.)
• pg_catalog.english_stem — внутренний словарь (в данном случае, это стеммер Snowball для
английского) для нормализации тезауруса. Заметьте, что внутренний словарь имеет собствен-
ную конфигурацию (например, список стоп-слов), но здесь она не рассматривается.
Теперь тезаурус thesaurus_simple можно связать с желаемыми типами фрагментов в конфигура-
ции, например так:
ALTER TEXT SEARCH CONFIGURATION english
ALTER MAPPING FOR asciiword, asciihword, hword_asciipart
WITH thesaurus_simple;
12.6.4.2. Пример тезауруса
Давайте рассмотрим простой астрономический тезаурус thesaurus_astro, содержащий несколько
астрономических терминов:
supernovae stars : sn
crab nebulae : crab
Ниже мы создадим словарь и привяжем некоторые типы фрагментов к астрономическому тезау-
русу и английскому стеммеру:
CREATE TEXT SEARCH DICTIONARY thesaurus_astro (
TEMPLATE = thesaurus,
DictFile = thesaurus_astro,
Dictionary = english_stem
);
ALTER TEXT SEARCH CONFIGURATION russian
394Полнотекстовый поиск
ALTER MAPPING FOR asciiword, asciihword, hword_asciipart
WITH thesaurus_astro, english_stem;
Теперь можно проверить, как он работает. Функция ts_lexize не очень полезна для проверки теза-
уруса, так как она обрабатывает входную строку как один фрагмент. Вместо неё мы можем исполь-
зовать функции plainto_tsquery и to_tsvector, которые разбивают входную строку на несколько
фрагментов:
SELECT plainto_tsquery('supernova star');
plainto_tsquery
-----------------
'sn'
SELECT to_tsvector('supernova star');
to_tsvector
-------------
'sn':1
В принципе так же можно использовать to_tsquery, если заключить аргумент в кавычки:
SELECT to_tsquery(' ''supernova star''');
to_tsquery
------------
'sn'
Заметьте, что supernova star совпадает с supernovae stars в thesaurus_astro, так как мы под-
ключили стеммер english_stem в определении тезауруса. Этот стеммер удалил конечные буквы
e и s.
Чтобы проиндексировать исходную фразу вместе с заменой, её нужно просто добавить в правую
часть соответствующего определения:
supernovae stars : sn supernovae stars
SELECT plainto_tsquery('supernova star');
plainto_tsquery
-----------------------------
'sn' &amp; 'supernova' &amp; 'star'
12.6.5. Словарь Ispell
Шаблон словарей Ispell поддерживает морфологические словари, которые могут сводить множе-
ство разных лингвистических форм слова к одной лексеме. Например, английский словарь Ispell
может связать вместе все склонения и спряжения ключевого слова bank: banking, banked, banks,
banks',bank's и т. п.
Стандартный дистрибутив PostgreSQL не включает файлы конфигурации Ispell. Загрузить словари
для множества языков можно со страницы Ispell. Кроме того, поддерживаются и другие современ-
ные форматы словарей: MySpell (OO &lt; 2.0.1) и Hunspell (OO &gt;= 2.0.2). Большой набор соответству-
ющих словарей можно найти на странице OpenOffice Wiki.
Чтобы создать словарь Ispell, выполните следующие действия:
• загрузите файлы конфигурации словаря. Пакет с дополнительным словарём OpenOffice имеет
расширение .oxt. Из него необходимо извлечь файлы .aff и .dic, и сменить их расширения
на .affix и .dict, соответственно. Для некоторых файлов словарей также необходимо преоб-
разовать символы в кодировку UTF-8 с помощью, например, таких команд (для норвежского
языка):
iconv -f ISO_8859-1 -t UTF-8 -o nn_no.affix nn_NO.aff
iconv -f ISO_8859-1 -t UTF-8 -o nn_no.dict nn_NO.dic
• скопируйте файлы в каталог $SHAREDIR/tsearch_data
395Полнотекстовый поиск
• загрузите эти файлы в PostgreSQL следующей командой:
CREATE TEXT SEARCH DICTIONARY english_hunspell (
TEMPLATE = ispell,
DictFile = en_us,
AffFile = en_us,
Stopwords = english);
Здесь параметры DictFile, AffFile и StopWords определяют базовые имена файлов словаря, аф-
фиксов и стоп-слов. Файл стоп-слов должен иметь тот же формат, что рассматривался выше в опи-
сании словаря simple. Формат других файлов здесь не рассматривается, но его можно узнать по
вышеуказанным веб-адресам.
Словари Ispell обычно воспринимают ограниченный набор слов, так что за ними следует подклю-
чить другой, более общий словарь, например, Snowball, который принимает всё.
Файл .affix для Ispell имеет такую структуру:
prefixes
flag *A:
.
suffixes
flag T:
E
[^AEIOU]Y
[AEIOU]Y
[^EY]
&gt; RE # As in enter &gt; reenter
&gt;
&gt;
&gt;
&gt; ST
-Y,IEST
EST
EST #
#
#
#
As
As
As
As
in
in
in
in
late &gt; latest
dirty &gt; dirtiest
gray &gt; grayest
small &gt; smallest
А файл .dict — такую:
lapse/ADGRS
lard/DGRS
large/PRTY
lark/MRS
Формат файла .dict следующий:
basic_form/affix_class_name
В файле .affix каждый флаг аффиксов описывается в следующем формате:
условие &gt; [-отсекаемые_буквы,] добавляемый_аффикс
Здесь условие записывается в формате, подобном формату регулярных выражений. В нём возмож-
но описать группы [...] и [^...]. Например, запись [AEIOU]Y означает, что последняя буква сло-
ва — "y", а предпоследней может быть "a", "e", "i", "o" или "u". Запись [^EY] означает, что по-
следняя буква не "e" и не "y".
Словари Ispell поддерживают разделение составных слов, что бывает полезно. Заметьте, что для
этого в файле аффиксов нужно пометить специальным оператором compoundwords controlled сло-
ва, которые могут участвовать в составных образованиях:
compoundwords
controlled z
Вот как это работает для норвежского языка:
SELECT ts_lexize('norwegian_ispell',
'overbuljongterningpakkmesterassistent');
{over,buljong,terning,pakk,mester,assistent}
SELECT ts_lexize('norwegian_ispell', 'sjokoladefabrikk');
{sjokoladefabrikk,sjokolade,fabrikk}
Формат MySpell представляет собой подмножество формата Hunspell. Файл .affix словаря
Hunspell имеет следующую структуру:
396Полнотекстовый поиск
PFX
PFX
SFX
SFX
SFX
SFX
SFX
A Y 1
A
0
T N 4
T
0
T
y
T
0
T
0
re .
st
iest
est
est e
[^aeiou]y
[aeiou]y
[^ey]
Первая строка класса аффиксов — заголовок. Поля правил аффиксов указываются после заголов-
ка:
•
•
•
•
•
имя параметра (PFX или SFX)
флаг (имя класса аффиксов)
отсекаемые символы в начале (в префиксе) или в конце (в суффиксе) слова
добавляемый аффикс
условие в формате, подобном регулярным выражениям.
Файл .dict подобен файлу .dict словаря Ispell:
larder/M
lardy/RT
large/RSPMYT
largehearted
Примечание
Словарь MySpell не поддерживает составные слова. С другой стороны, Hunspell под-
держивает множество операции с ними, но в настоящее время PostgreSQL использует
только самые простые из этого множества.
12.6.6. Словарь Snowball
Шаблон словарей Snowball основан на проекте Мартина Потера, изобретателя популярного алго-
ритма стемминга для английского языка. Сейчас Snowball предлагает алгоритмы и для многих
других языков (за подробностями обратитесь на сайт Snowball). Каждый алгоритм знает, как для
данного языка свести распространённые словоформы к начальной форме. Для словаря Snowball
задаётся обязательный параметр language, определяющий, какой именно стеммер использовать,
и может задаваться параметр stopword, указывающий файл со списком исключаемых слов. (Стан-
дартные списки стоп-слов PostgreSQL используется также в и проекте Snowball.) Например, встро-
енное определение выглядит так
CREATE TEXT SEARCH DICTIONARY english_stem (
TEMPLATE = snowball,
Language = english,
StopWords = english
);
Формат файла стоп-слов не отличается от рассмотренного ранее.
Словарь Snowball распознаёт любые фрагменты, даже если он не может упростить слова, так что
он должен быть самым последним в списке словарей. Помещать его перед другими словарями нет
смысла, так как после него никакой фрагмент не будет передан следующему словарю.
12.7. Пример конфигурации
Конфигурация текстового поиска определяет всё, что необходимо для преобразования документа
в формат tsvector: анализатор, который будет разбивать текст на фрагменты, и словари, которые
будут преобразовывать фрагменты в лексемы. При каждом вызове to_tsvector или to_tsquery
обязательно используется конфигурация текстового поиска. В конфигурации сервера есть пара-
397Полнотекстовый поиск
метр default_text_search_config, задающий имя конфигурации текстового поиска по умолчанию,
которая будет использоваться, когда при вызове функций поиска соответствующий аргумент не
определён. Этот параметр можно задать в postgresql.conf или установить в рамках отдельного
сеанса с помощью команды SET.
В системе есть несколько встроенных конфигураций текстового поиска и вы можете легко допол-
нить их своими. Для удобства управления объектами текстового поиска в PostgreSQL реализованы
соответствующие SQL-команды и специальные команды в psql, выводящие информацию об этих
объектах (Раздел 12.10).
В качестве примера использования этих команд мы создадим конфигурацию pg, взяв за основу
встроенную конфигурацию english:
CREATE TEXT SEARCH CONFIGURATION public.pg ( COPY = pg_catalog.english );
Мы будем использовать список синонимов, связанных с PostgreSQL, в файле $SHAREDIR/
tsearch_data/pg_dict.syn. Этот файл содержит строки:
postgres
pgsql
postgresql
pg
pg
pg
Мы определим словарь синонимов следующим образом:
CREATE TEXT SEARCH DICTIONARY pg_dict (
TEMPLATE = synonym,
SYNONYMS = pg_dict
);
Затем мы зарегистрируем словарь Ispell english_ispell, у которого есть собственные файлы кон-
фигурации:
CREATE TEXT SEARCH DICTIONARY english_ispell (
TEMPLATE = ispell,
DictFile = english,
AffFile = english,
StopWords = english
);
Теперь мы можем настроить сопоставления для слов в конфигурации pg:
ALTER TEXT SEARCH CONFIGURATION pg
ALTER MAPPING FOR asciiword, asciihword, hword_asciipart,
word, hword, hword_part
WITH pg_dict, english_ispell, english_stem;
Мы решили не индексировать и не учитывать при поиске некоторые типы фрагментов, которые не
обрабатываются встроенной конфигурацией:
ALTER TEXT SEARCH CONFIGURATION pg
DROP MAPPING FOR email, url, url_path, sfloat, float;
Теперь мы можем протестировать нашу конфигурацию:
SELECT * FROM ts_debug('public.pg', '
PostgreSQL, the highly scalable, SQL compliant, open source
object-relational database management system, is now undergoing
beta testing of the next version of our software.
');
И наконец мы выбираем в текущем сеансе эту конфигурацию, созданную в схеме public:
=&gt; \dF
List of text search configurations
398Полнотекстовый поиск
Schema | Name | Description
---------+------+-------------
public | pg
|
SET default_text_search_config = 'public.pg';
SET
SHOW default_text_search_config;
default_text_search_config
----------------------------
public.pg
12.8. Тестирование и отладка текстового поиска
Поведение нестандартной конфигурации текстового поиска по мере её усложнения может стать
непонятным. В этом разделе описаны функции, полезные для тестирования объектов текстового
поиска. Вы можете тестировать конфигурацию как целиком, так и по частям, отлаживая анализа-
торы и словари по отдельности.
12.8.1. Тестирование конфигурации
Созданную конфигурацию текстового поиска можно легко протестировать с помощью функции
ts_debug.
ts_debug([конфигурация regconfig,] документ text,
OUT псевдоним text,
OUT описание text,
OUT фрагмент text,
OUT словари regdictionary[],
OUT словарь regdictionary,
OUT лексемы text[])
returns setof record
ts_debug выводит информацию обо всех фрагментах данного документа, которые были выданы
анализатором и обработаны настроенными словарями. Она использует конфигурацию, указанную
в аргументе config, или default_text_search_config, если этот аргумент опущен.
ts_debug возвращает по одной строке для каждого фрагмента, найденного в тексте анализатором.
Эта строка содержит следующие столбцы:
•
•
•
•
синоним text — краткое имя типа фрагмента
описание text — описание типа фрагмента
фрагмент text — текст фрагмента
словари regdictionary[] — словари, назначенные в конфигурации для фрагментов такого ти-
па
• словарь regdictionary — словарь, распознавший этот фрагмент, или NULL, если подходящего
словаря не нашлось
• лексемы text[] — лексемы, выданные словарём, распознавшим фрагмент, или NULL, если под-
ходящий словарь не нашёлся; может быть также пустым массивом ({}), если фрагмент распо-
знан как стоп-слово
Простой пример:
SELECT * FROM ts_debug('english',
'a fat cat sat on a mat - it ate a fat rats');
alias |
description | token| dictionaries | dictionary |lexemes
----------+----------------+------+---------------+-------------+-------
asciiword| Word, all ASCII| a
| {english_stem}| english_stem| {}
blank
| Space symbols |
| {}
|
|
399Полнотекстовый поиск
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
blank
|
asciiword|
Word,
Space
Word,
Space
Word,
Space
Word,
Space
Word,
Space
Word,
Space
Space
Word,
Space
Word,
Space
Word,
Space
Word,
Space
Word,
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
symbols |
all ASCII|
fat
|
|
cat |
|
sat |
|
on
|
|
a
|
|
mat |
|
-
|
it
|
|
ate |
|
a
|
|
fat |
|
rats |
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
{}
|
{english_stem}|
english_stem|
|
english_stem|
|
english_stem|
|
english_stem|
|
english_stem|
|
english_stem|
|
|
english_stem|
|
english_stem|
|
english_stem|
|
english_stem|
|
english_stem|
{fat}
{cat}
{sat}
{}
{}
{mat}
{}
{ate}
{}
{fat}
{rat}
Для более полной демонстрации мы сначала создадим конфигурацию public.english и словарь
Ispell для английского языка:
CREATE TEXT SEARCH CONFIGURATION public.english
( COPY = pg_catalog.english );
CREATE TEXT SEARCH DICTIONARY english_ispell (
TEMPLATE = ispell,
DictFile = english,
AffFile = english,
StopWords = english
);
ALTER TEXT SEARCH CONFIGURATION public.english
ALTER MAPPING FOR asciiword WITH english_ispell, english_stem;
SELECT * FROM ts_debug('public.english','The Brightest supernovaes');
alias | description |
token
| dictionaries |dictionary| lexemes
---------+-------------+-----------+----------- ---+----------+-----------
asciiword|Word,
|The
|{english_ispell|english_ |{}
| all ASCII
|
|,english_stem} |ispell
|
blank
|Space symbols|
|{}
|
|
|
|
|
|
|
asciiword|Word,
|Brightest |{english_ispell|english_ |{bright}
|all ASCII
|
|,english_stem} |ispell
|
blank
|Space symbols|
| {}
|
|
|
|
|
|
|
asciiword|Word,
|supernovaes|{english_ispell|english_ |{supernova}
|all ASCII
|
|,english_stem} |stem
|
В этом примере слово Brightest было воспринято анализатором как фрагмент ASCII word
(синоним asciiword). Для этого типа фрагментов список словарей включает english_ispell и
english_stem. Данное слово было распознано словарём english_ispell, который свёл его к bright.
Слово supernovaes оказалось незнакомо словарю english_ispell, так что оно было передано сле-
дующему словарю, который его благополучно распознал (на самом деле english_stem — это стем-
мер Snowball, который распознаёт всё, поэтому он включён в список словарей последним).
400Полнотекстовый поиск
Слово The было распознано словарём english_ispell как стоп-слово (см. Подраздел 12.6.1) и по-
этому не будет индексироваться. Пробелы тоже отбрасываются, так как в данной конфигурации
для них нет словарей.
Вы можете уменьшить ширину вывода, явно перечислив только те столбцы, которые вы хотите
видеть:
SELECT alias, token, dictionary, lexemes
FROM ts_debug('public.english','The Brightest supernovaes');
alias
|
token
|
dictionary
|
lexemes
-----------+-------------+----------------+-------------
asciiword | The
| english_ispell | {}
blank
|
|
|
asciiword | Brightest
| english_ispell | {bright}
blank
|
|
|
asciiword | supernovaes | english_stem
| {supernova}
12.8.2. Тестирование анализатора
Следующие функции позволяют непосредственно протестировать анализатор текстового поиска.
ts_parse(имя_анализатора text, документ text,
OUT код_фрагмента integer, OUT фрагмент text) returns setof record
ts_parse(oid_анализатора oid, документ text,
OUT код_фрагмента integer, OUT фрагмент text) returns setof record
ts_parse разбирает данный документ и возвращает набор записей, по одной для каждого извле-
чённого фрагмента. Каждая запись содержит код_фрагмента, код назначенного типа фрагмента,
и фрагмент, собственно текст фрагмента. Например:
SELECT * FROM ts_parse('default', '123 - a number');
tokid | token
-------+--------
22 | 123
12 |
12 | -
1 | a
12 |
1 | number
ts_token_type(имя_анализатора text, OUT код_фрагмента integer,
OUT псевдоним text, OUT описание text) returns setof record
ts_token_type(oid_анализатора oid, OUT код_фрагмента integer,
OUT псевдоним text, OUT описание text) returns setof record
ts_token_type возвращает таблицу, описывающую все типы фрагментов, которые может распо-
знать анализатор. Для каждого типа в этой таблице указывается его целочисленный код_фрагмен-
та, псевдоним , с которым этот тип фигурирует в командах, и краткое description. Например:
SELECT * FROM ts_token_type('default');
tokid |
alias
|
description
-------+-----------------+------------------------------------------
1 | asciiword
| Word, all ASCII
2 | word
| Word, all letters
3 | numword
| Word, letters and digits
4 | email
| Email address
5 | url
| URL
6 | host
| Host
7 | sfloat
| Scientific notation
8 | version
| Version number
401Полнотекстовый поиск
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
hword_numpart
hword_part
hword_asciipart
blank
tag
protocol
numhword
asciihword
hword
url_path
file
float
int
uint
entity
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
Hyphenated word part, letters and digits
Hyphenated word part, all letters
Hyphenated word part, all ASCII
Space symbols
XML tag
Protocol head
Hyphenated word, letters and digits
Hyphenated word, all ASCII
Hyphenated word, all letters
URL path
File or path name
Decimal notation
Signed integer
Unsigned integer
XML entity
12.8.3. Тестирование словаря
Для тестирования словаря предназначена функция ts_lexize.
ts_lexize(словарь regdictionary, фрагмент text) returns text[]
ts_lexize возвращает массив лексем, если входной фрагмент известен словарю, либо пустой мас-
сив, если этот фрагмент считается в словаре стоп-словом, либо NULL, если он не был распознан.
Примеры:
SELECT ts_lexize('english_stem', 'stars');
ts_lexize
-----------
{star}
SELECT ts_lexize('english_stem', 'a');
ts_lexize
-----------
{}
Примечание
Функция ts_lexize принимает одиночный фрагмент, а не просто текст. Вот пример
возможного заблуждения:
SELECT ts_lexize('thesaurus_astro','supernovae stars') is null;
?column?
----------
t
Хотя фраза supernovae stars есть в тезаурусе thesaurus_astro, ts_lexize не работа-
ет, так как она не разбирает входной текст, а воспринимает его как один фрагмент.
Поэтому для проверки тезаурусов следует использовать функции plainto_tsquery и
to_tsvector, например:
SELECT plainto_tsquery('supernovae stars');
plainto_tsquery
-----------------
'sn'
12.9. Типы индексов GIN и GiST
402Полнотекстовый поиск
Для ускорения полнотекстового поиска можно использовать индексы двух видов. Заметьте, что эти
индексы не требуются для поиска, но если по какому-то столбцу поиск выполняется регулярно,
обычно желательно её индексировать.
CREATE INDEX имя ON таблица USING GIN (столбец);
Создаёт индекс на базе GIN (Generalized Inverted Index, Обобщённый Инвертированный Ин-
декс). Столбец должен иметь тип tsvector.
CREATE INDEX имя ON таблица USING GIST (столбец);
Создаёт индекс на базе GiST (Generalized Search Tree, Обобщённое дерево поиска). Здесь стол-
бец может иметь тип tsvector или tsquery.
Более предпочтительными для текстового поиска являются индексы GIN. Будучи инвертирован-
ными индексами, они содержат записи для всех отдельных слов (лексем) с компактным списком
мест их вхождений. При поиске нескольких слов можно найти первое, а затем воспользоваться
индексом и исключить строки, в которых дополнительные слова отсутствуют. Индексы GIN хранят
только слова (лексемы) из значений tsvector, и теряют информацию об их весах. Таким образом
для выполнения запроса с весами потребуется перепроверить строки в таблице.
Индекс GiST допускает неточности, то есть он допускает ложные попадания и поэтому их нужно
исключать дополнительно, сверяя результат с фактическими данными таблицы. (PostgreSQL дела-
ет это автоматически.) Индексы GiST являются неточными, так как все документы в них представ-
ляются сигнатурой фиксированной длины. Эта сигнатура создаётся в результате представления
присутствия каждого слова как одного бита в строке из n-бит, а затем логического объединения
этих битовых строк. Если двум словам будет соответствовать одна битовая позиция, попадание
оказывается ложным. Если для всех слов оказались установлены соответствующие биты (в случае
фактического или ложного попадания), для проверки правильности предположения о совпадении
слов необходимо прочитать строку таблицы.
Неточность индекса приводит к снижению производительности из-за дополнительных обращений
к записям таблицы, для которых предположение о совпадении оказывается ложным. Так как про-
извольный доступ к таблице обычно не бывает быстрым, это ограничивает применимость индексов
GiST. Вероятность ложных попаданий зависит от ряда факторов, например от количества уникаль-
ных слов, так что его рекомендуется сокращать, применяя словари.
Заметьте, что построение индекса GIN часто можно ускорить, увеличив maintenance_work_mem,
тогда как время построения индекса GiST не зависит от этого параметра.
Правильно используя индексы GIN и GiST и разделяя большие коллекции документов на секции,
можно реализовать очень быстрый поиск с возможностью обновления «на лету». Секционировать
данные можно как на уровне базы, с использованием наследования таблиц, так и распределив до-
кументы по разным серверам и затем собирая внешние результаты, например, средствами доступа
к сторонним данным. Последний вариант возможен благодаря тому, что функции ранжирования
используют только локальную информацию.
12.10. Поддержка psql
Информацию об объектах конфигурации текстового поиска можно получить в psql с помощью сле-
дующего набора команд:
\dF{d,p,t}[+] [ШАБЛОН]
Необязательный + в этих командах включает более подробный вывод.
В необязательном параметре ШАБЛОН может указываться имя объекта текстового поиска, возможно
дополненное именем схемы. Если ШАБЛОН не указан, выводится информация обо всех видимых объ-
ектах. ШАБЛОН может содержать регулярное выражение с разными масками для схемы и объекта.
Это иллюстрируют следующие примеры:
=&gt; \dF *fulltext*
403Полнотекстовый поиск
List of text search configurations
Schema | Name
| Description
--------+--------------+-------------
public | fulltext_cfg |
=&gt; \dF *.fulltext*
List of text search configurations
Schema
| Name
| Description
----------+----------------------------
fulltext | fulltext_cfg |
public
| fulltext_cfg |
Возможны следующие команды:
\dF[+] [ШАБЛОН]
Список конфигураций текстового поиска (добавьте + для дополнительных сведений).
=&gt; \dF russian
List of text search configurations
Schema
| Name
|
Description
------------+---------+------------------------------------
pg_catalog | russian | configuration for russian language
=&gt; \dF+ russian
Text search configuration "pg_catalog.russian"
Parser: "pg_catalog.default"
Token
| Dictionaries
-----------------+--------------
asciihword
| english_stem
asciiword
| english_stem
email
| simple
file
| simple
float
| simple
host
| simple
hword
| russian_stem
hword_asciipart | english_stem
hword_numpart
| simple
hword_part
| russian_stem
int
| simple
numhword
| simple
numword
| simple
sfloat
| simple
uint
| simple
url
| simple
url_path
| simple
version
| simple
word
| russian_stem
\dFd[+] [ШАБЛОН]
Список словарей текстового поиска (добавьте + для дополнительных сведений).
=&gt; \dFd
List of text search dictionaries
Schema
|
Name
|
Description
-----------+----------------+-------------------------------------------
pg_catalog | danish_stem
| snowball stemmer for danish language
pg_catalog | dutch_stem
| snowball stemmer for dutch language
pg_catalog | english_stem
| snowball stemmer for english language
pg_catalog | finnish_stem
| snowball stemmer for finnish language
404Полнотекстовый поиск
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
pg_catalog
|
|
|
|
|
|
|
|
|
|
|
|
french_stem
|
german_stem
|
hungarian_stem |
italian_stem
|
norwegian_stem |
portuguese_stem|
romanian_stem |
russian_stem
|
simple
|
spanish_stem
|
swedish_stem
|
turkish_stem
|
snowball stemmer for french language
snowball stemmer for german language
snowball stemmer for hungarian language
snowball stemmer for italian language
snowball stemmer for norwegian language
snowball stemmer for portuguese language
snowball stemmer for romanian language
snowball stemmer for russian language
simple dictionary: just lower case and ...
snowball stemmer for spanish language
snowball stemmer for swedish language
snowball stemmer for turkish language
\dFp[+] [ШАБЛОН]
Список анализаторов текстового поиска (добавьте + для дополнительных сведений).
=&gt; \dFp
List of text search parsers
Schema
| Name
|
Description
------------+---------+---------------------
pg_catalog | default | default word parser
=&gt; \dFp+
Text search parser "pg_catalog.default"
Method
|
Function
| Description
-----------------+----------------+-------------
Start parse
| prsd_start
|
Get next token | prsd_nexttoken |
End parse
| prsd_end
|
Get headline
| prsd_headline |
Get token types | prsd_lextype
|
Token types for parser "pg_catalog.default"
Token name
|
Description
-----------------+------------------------------------------
asciihword
| Hyphenated word, all ASCII
asciiword
| Word, all ASCII
blank
| Space symbols
email
| Email address
entity
| XML entity
file
| File or path name
float
| Decimal notation
host
| Host
hword
| Hyphenated word, all letters
hword_asciipart | Hyphenated word part, all ASCII
hword_numpart
| Hyphenated word part, letters and digits
hword_part
| Hyphenated word part, all letters
int
| Signed integer
numhword
| Hyphenated word, letters and digits
numword
| Word, letters and digits
protocol
| Protocol head
sfloat
| Scientific notation
tag
| XML tag
uint
| Unsigned integer
url
| URL
url_path
| URL path
version
| Version number
word
| Word, all letters
(23 rows)
405Полнотекстовый поиск
\dFt[+] [ШАБЛОН]
Список шаблонов текстового поиска (добавьте + для дополнительных сведений).
=&gt; \dFt
List of text search templates
Schema | Name
|
Description
----------+---------+----------------------------------------------------
pg_catalog|ispell
|ispell dictionary
pg_catalog|simple
|simple dictionary: just lower case and check for ...
pg_catalog|snowball |snowball stemmer
pg_catalog|synonym |synonym dictionary: replace word by its synonym
pg_catalog|thesaurus|thesaurus dictionary: phrase by phrase substitution
12.11. Ограничения
Текущая реализация текстового поиска в PostgreSQL имеет следующие ограничения:
•
•
•
•
•
•
•
Длина лексемы не может превышать 2 килобайта
Длина значения tsvector (лексемы и их позиции) не может превышать 1 мегабайт
64
Число лексем должно быть меньше 2
Значения позиций в tsvector должны быть от 0 до 16383
Расстояние в операторе <N> (ПРЕДШЕСТВУЕТ) типа tsquery не может быть больше 16384
Не больше 256 позиций для одной лексемы
Число узлов (лексемы + операторы) в значении tsquery должно быть меньше 32768
Для сравнения, документация PostgreSQL 8.1 содержала 335 420 слов, из них 10 441 уникальных, а
наиболее часто употребляющееся в ней слово «postgresql» встречается 6 127 раз в 655 документах.
Другой пример — архивы списков рассылки PostgreSQL содержали 910  989 уникальных слов в
57 491 343 лексемах в 461 020 сообщениях.
</N></N></query>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://localhost:4000/tags/#PostgreSQL" title="Pages tagged PostgreSQL" class="tag"><span class="term">PostgreSQL</span></a><a href="http://localhost:4000/tags/#PostgreSQL_Book_11" title="Pages tagged PostgreSQL_Book_11" class="tag"><span class="term">PostgreSQL_Book_11</span></a></span>
        <span>Updated on <span class="entry-date date updated"><time datetime="2018-12-03 T15:14:43-04:00">December 03, 2018</time></span></span>
        <span class="author vcard"><span class="fn">Sergey Khatsiola</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/PostgreSQL-V11_Doc-012/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://localhost:4000/PostgreSQL-V11_Doc-012/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=http://localhost:4000/PostgreSQL-V11_Doc-012/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://localhost:4000/PostgreSQL-V11_Doc-011/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://localhost:4000/Script_bash-copy-ftp/" title="Копируем на FTP скриптом bash">Копируем на FTP скриптом bash</a></h3>
      <p>Bash FTP script copy <a href="http://localhost:4000/Script_bash-copy-ftp/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://localhost:4000/Work-Project-Manager/" title="Коротко - работа менеджера проектов">Коротко - работа менеджера проектов</a></h4>
        <span>Published on December 04, 2018</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://localhost:4000/PostgreSQL-V11_Doc-077/" title="Приложение G. Дополнительно поставляемые программы">Приложение G. Дополнительно поставляемые программы</a></h4>
        <span>Published on December 01, 2018</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Sergey Khatsiola. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-130427752-1', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>


	        

</body>
</html>
