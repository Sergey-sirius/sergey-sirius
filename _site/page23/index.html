<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Последние посты &#8211; Sirius Blog</title>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-130427752-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-130427752-1');
</script>

</head>
<meta name="description" content="Describtion ..">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- Twitter Cards -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/abstract-1.jpg">

<meta name="twitter:title" content="Последние посты">
<meta name="twitter:description" content="Describtion ..">
<meta name="twitter:creator" content="@2hotab2">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Последние посты">
<meta property="og:description" content="Describtion ..">
<meta property="og:url" content="http://localhost:4000/page23/">
<meta property="og:site_name" content="Sirius Blog">





<link rel="canonical" href="http://localhost:4000/page23/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Sirius Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.jpg">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.jpg">
<!-- 114x72 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x72" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.jpg">
<!-- 144x72 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x72" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.jpg">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://localhost:4000/images/avatar.jpg" alt="Sergey Khatsiola photo" class="author-photo">
					<h4>Sergey Khatsiola</h4>
					<p>Кратко обо мне ...</p>
				</li>
				<li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:2hotab2@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				<li>
					<a href="https://twitter.com/2hotab2"><i class="fa fa-fw fa-twitter"></i> Twitter</a>
				</li>
				<li>
					<a href="https://facebook.com/sergej.ha1"><i class="fa fa-fw fa-facebook"></i> Facebook</a>
				</li>
				
				
				<li>
					<a href="https://github.com/Sergey-sirius"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    
	    <li><a href="http://localhost:4000/handbook/" >HandBook</a></li>
	  
	    
	    <li><a href="https://github.com/Sergey-sirius" target="_blank">Main Link</a></li>
	  
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  
    <div class="entry-image">
      <img src="http://localhost:4000/images/abstract-1.jpg" alt="Последние посты">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Sirius Blog</h1>
      <h2>Последние посты</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/PostgreSQL-V11_Doc-026/" title="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация"><img src="http://localhost:4000/images/abstract-11.jpg" alt="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-12-01T00:00:00+02:00"><a href="http://localhost:4000/PostgreSQL-V11_Doc-026/">December 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Sergey Khatsiola">Sergey Khatsiola</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~57 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/PostgreSQL-V11_Doc-026/" rel="bookmark" title="Глава 26. Отказоустойчивость,балансировка нагрузки и репликация" itemprop="url">Глава 26. Отказоустойчивость,балансировка нагрузки и репликация</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Глава 26. Отказоустойчивость,балансировка нагрузки и репликация</p>

<p>Серверы базы данных могут работать совместно для обеспечения возможности быстрого переклю-
чения на другой сервер в случае отказа первого (отказоустойчивость) или для обеспечения воз-
можности нескольким серверам БД обрабатывать один набор данных (балансировка нагрузки). В
идеале, серверы БД могут работать вместе прозрачно для клиента. Веб-серверы, обрабатывающие
статические страницы, можно совместить достаточно легко посредством простого распределения
запросов на несколько машин. Фактически серверы баз данных только для чтения тоже могут быть
совмещены достаточно легко. К сожалению, большинство серверов баз данных получают смешан-
ные запросы на чтение/запись, а серверы с доступом на чтение/запись совместить гораздо слож-
нее. Это объясняется тем, что данные только для чтения достаточно единожды разместить на каж-
дом сервере, а запись на любой из серверов должна распространиться на все остальные серверы,
чтобы будущие запросы на чтение возвращали согласованные результаты.
Проблема синхронизации является главным препятствием для совместной работы серверов. Так
как единственного решения, устраняющего проблему синхронизации во всех случаях, не суще-
ствует, предлагается несколько решений. Разные решения подходят к проблеме по-разному и ми-
нимизируют её влияние в разных рабочих условиях.
Некоторые решения применяют синхронизацию, позволяя только одному серверу изменять дан-
ные. Сервер, который может изменять данные, называется сервером чтения/записи, ведущим или
главным сервером. Сервер, который отслеживает изменения на ведущем, называется ведомым
или резервным сервером. Резервный сервер, к которому нельзя подключаться до тех пор, пока
он не будет повышен до главного, называется сервером тёплого резерва, а тот, который может
принимать соединения и обрабатывать запросы только на чтение, называется сервером горячего
резерва.
Некоторые решения являются синхронными, при которых транзакция, модифицирующая данные,
не считается подтверждённой, пока все серверы не подтвердят транзакцию. Это гарантирует, что
при отработке отказа не произойдёт потеря данных и что все балансирующие серверы возвращают
целостные данные вне зависимости от того, к какому серверу был запрос. Асинхронное решение,
напротив, допускает некоторую задержку между временем подтверждения транзакции и её пере-
дачей на другие серверы, допуская возможность, что некоторые транзакции могут быть потеряны
в момент переключения на резервный сервер и что балансирующие серверы могут вернуть слегка
устаревшие данные. Асинхронная передача используется, когда синхронная будет слишком мед-
ленной.
Решения могут так же разделяться по степени детализации. Некоторые решения работают только
на уровне всего сервера БД целиком, в то время как другие позволяют работать на уровне таблиц
или уровне БД.
В любом случае необходимо принимать во внимание быстродействие. Обычно выбирается компро-
мисс между функциональностью и производительностью. Например, полностью синхронное реше-
ние в медленной сети может снизить производительность больше чем наполовину, в то время как
асинхронное решение будет оказывать минимальное воздействие.
В продолжении этого раздела рассматриваются различные решения по организации отказоустой-
чивости, репликации и балансировки нагрузки.
26.1. Сравнение различных решений
Отказоустойчивость на разделяемых дисках
Отказоустойчивость на разделяемых дисках позволяет избежать избыточности синхронизации
путём задействования только одной копии базы данных. Она использует единственный диско-
вый массив, который разделяется между несколькими серверами. Если основной сервер БД
откажет, резервный сервер может подключиться и запустить базу данных, что позволит вос-
становить БД после аварии. Это обеспечивает быстрое переключение без потери данных.
646Отказоустойчивость, баланси-
ровка нагрузки и репликация
Функциональность разделяемого оборудования обычно реализована в сетевых устройствах хра-
нения. Так же возможно применение сетевой файловой системы; особое внимание следует уде-
лить тому, чтобы поведение системы полностью соответствовало POSIX (см. Подраздел 18.2.2).
Существенное ограничение этого метода состоит в том, что в случае отказа или порчи разде-
ляемого дискового массива оба сервера: главный и резервный — станут нерабочими. Другая
особенность — резервный сервер никогда не получает доступ к разделяемым дискам во время
работы главного.
Репликация на уровне файловой системы (блочного устройства)
Видоизменённая версия функциональности разделяемого оборудования представлена в виде
репликации на уровне файловой системы, когда все изменения в файловой системе отражают-
ся в файловой системе другого компьютера. Единственное ограничение: синхронизация долж-
на выполняться методом, гарантирующим целостность копии файловой системы на резервном
сервере — в частности, запись на резервном сервере должна происходить в том же порядке,
что и на главном. DRBD является популярным решением на основе репликации файловой си-
стемы для Linux.
Трансляция журнала предзаписи
Серверы тёплого и горячего резерва могут так же поддерживаться актуальными путём чтения
потока записей из журнала изменений (WAL). Если основной сервер отказывает, резервный
содержит почти все данные с него и может быть быстро преобразован в новый главный сервер
БД. Это можно сделать синхронно или асинхронно, но может быть выполнено только на уровне
сервера БД целиком.
Резервный сервер может быть реализован с применением трансляции файлов журналов (см.
Раздел  26.2), или потоковой репликации (см. Подраздел  26.2.5), или их комбинацией. За ин-
формацией о горячем резерве обратитесь к Разделу 26.5.
Логическая репликация
В схеме с логической репликацией сервер баз данных может передавать поток изменений дан-
ных на другой сервер. Механизм логической репликации в PostgreSQL формирует поток логи-
ческих изменений данных, обрабатывая WAL. Логическая репликация позволяет переносить
изменения, происходящие только в отдельных таблицах. Для логической репликации не требу-
ется, чтобы за определённым сервером закреплялась роль главного или реплицирующего; на-
против, данные могут передаваться в разных направлениях. За дополнительными сведениями
о логической репликации обратитесь к Главе 31. Используя интерфейс логического декодиро-
вания (Глава 49), подобную функциональность могут предоставлять и сторонние расширения.
Репликация главный-резервный на основе триггеров
При репликации главный-резервный все запросы, изменяющие данные, пересылаются главно-
му серверу. Главный сервер, в свою очередь, асинхронно пересылает изменённые данные ре-
зервному. Резервный сервер может обрабатывать запросы только на чтение при работающем
главном. Такой резервный сервер идеален для обработки запросов к хранилищам данных.
Slony-I является примером подобного типа репликации, действующей на уровне таблиц, и под-
держивает множество резервных серверов. Так как обновления на резервных серверах проис-
ходят асинхронно (в пакетах), возможна потеря данных во время отказа.
Репликация запросов в среднем слое
В схеме с репликацией запросов в среднем слое, средний слой перехватывает каждый SQL-за-
прос и пересылает его на один или все серверы. Каждый сервер работает независимо. Модифи-
цирующие запросы должны быть направлены всем серверам, чтобы каждый из них получал все
изменения. Но читающие запросы могут быть посланы только на один сервер, что позволяет
перераспределить читающую нагрузку между всеми серверами.
Если запросы просто перенаправлять без изменений, функции подобные random(),
CURRENT_TIMESTAMP и последовательности могут получить различные значения на разных сер-
верах. Это происходит потому что каждый сервер работает независимо, а эти запросы неиз-
бирательные (и действительно не изменяют строки). Если такая ситуация недопустима, или
647Отказоустойчивость, баланси-
ровка нагрузки и репликация
средний слой, или приложение должно запросить подобные значения с одного сервера, затем
использовать его в других пишущих запросах. Другим способом является применения этого
вида репликации совместно с другим традиционным набором репликации главный-резервный,
то есть изменяющие данные запросы посылаются только на главный сервер, а затем применя-
ются на резервном в процессе этой репликации, но не с помощью реплицирующего среднего
слоя. Следует иметь в виду, что все транзакции фиксируются или прерываются на всех серве-
рах, возможно с применением двухфазной фиксации (см. PREPARE TRANSACTION и COMMIT
PREPARED). Репликацию такого типа реализуют, например Pgpool-II и Continuent Tungsten.
Асинхронная репликация с несколькими главными серверами
Если серверы не находятся постоянно в единой сети, как например, ноутбуки или удалённые
серверы, обеспечение согласованности данных между ними представляет проблему. Когда ис-
пользуется асинхронная репликация с несколькими главными серверами, каждый из них ра-
ботает независимо и периодически связывается с другими серверами для определения кон-
фликтующих транзакций. Конфликты могут урегулироваться пользователем или по правилам
их разрешения. Примером такого типа репликации является Bucardo.
Синхронная репликация с несколькими главными серверами
При синхронной репликации с несколькими главными серверами каждый сервер может прини-
мать запросы на запись, а изменённые данные передаются с начального сервера всем осталь-
ным, прежде чем транзакция будет подтверждена. Если запись производится интенсивно, это
может провоцировать избыточные блокировки, что приводит к снижению производительности.
На самом деле производительность при записи часто бывает хуже, чем с одним сервером. За-
просы на чтение также могут быть обработаны любым сервером. В некоторых конфигурациях
для более эффективного взаимодействия серверов применяются разделяемые диски. Синхрон-
ная репликация с несколькими главными серверами лучше всего работает, когда преобладают
операции чтения, хотя её большой плюс в том, что любой сервер может принимать запросы на
запись — нет необходимости искусственно разделять нагрузку между главным и резервными
серверами, а так как изменения передаются от одного сервера другим, не возникает проблем
с недетерминированными функциями вроде random().
PostgreSQL не предоставляет данный тип репликации, но так как PostgreSQL поддерживает
двухфазное подтверждение транзакции (PREPARE TRANSACTION и COMMIT PREPARED) такое
поведение может быть реализовано в коде приложения или среднего слоя.
Коммерческие решения
Так как PostgreSQL обладает открытым кодом и легко расширяется, некоторые компании взяли
за основу PostgreSQL и создали коммерческие решения с закрытым кодом со своими реализа-
циями свойств отказоустойчивости, репликации и балансировки нагрузки.
Таблица 26.1 итоговая таблица возможностей различных решений приведена ниже.
Таблица 26.1. Таблица свойств отказоустойчивости, балансировки нагрузки и репликации
Тип
Наиболее
типичные
реализа-
ции
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы
NAS DRBD
Трансля-
ция жур-
нала
предза-
писи
Логиче-
ская ре-
плика-
ция
Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров
встроен- встроен- Londiste,
ная пото- ная логи-
Slony
ковая ре-
ческая
пликация репли-
кация,
pglogical
648
Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
pgpool-II Bucardo  Отказоустойчивость, баланси-
ровка нагрузки и репликация
Тип
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы Трансля-
ция жур-
нала
предза-
писи Логиче-
ская ре-
плика-
ция Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
Метод
взаимо-
действия разде-
ляемые
диски дисковые
блоки WAL логиче-
ское де-
кодиро-
вание Строки
таблицы SQL Строки
таблицы Строки
табли-
цы и бло-
киров-
ки строк
Не требу-
ется спе-
циально-
го обору-
дования   • • • • • • •
Допуска-
ется
несколь-
ко глав-
ных сер-
веров       •   • • •
Нет избы-
точности
главного
сервера •   • •   •  <br />
Нет
за-
держки
при
несколь-
ких серве-
рах •   без
синхр. без
синхр. •   •<br />
Отказ
главного
сервера
не
мо-
жет при-
вести
к
потере
данных • • с синхр. с синхр.   •   •
Сервер
реплики
принима-
ет чита-
ющие за-
просы     с горя-
чим ре-
зервом • • • • •
Реплика-
ция
на
уровне
таблиц       • •   • •
649Отказоустойчивость, баланси-
ровка нагрузки и репликация
Тип
Не требу-
ется раз-
решение
конфлик-
тов
Отказо-
устойчи-
вость че-
рез раз-
деляе-
мые дис-
ки Реплика-
ция фай-
ловой
системы Трансля-
ция жур-
нала
предза-
писи Логиче-
ская ре-
плика-
ция Реплика-
ция
глав-
ный-ре-
зервный
на осно-
ве триг-
геров Реплика-
ция за-
просов в
среднем
слое Асин-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами Син-
хронная
реплика-
ция
с
несколь-
кими
главны-
ми сер-
верами
• • •   •     •
Несколько решений, которые не подпадают под указанные выше категории:
Секционирование данных
При секционировании таблицы расщепляются на наборы данных. Каждый из наборов может
быть изменён только на одном сервере. Например, данные могут быть секционированы по офи-
сам, например, Лондон и Париж, с сервером в каждом офисе. В случае необходимости обра-
щения одновременно к данным Лондона и Парижа, приложение может запросить оба сервера,
или может быть применена репликация главный-резервный для предоставления копии только
для чтения в другом офисе для каждого из серверов.
Выполнение параллельных запросов на нескольких серверах
Многие из указанных выше решений позволяют обрабатывать несколько запросов на несколь-
ких серверах, но ни один из них не может обрабатывать один запрос с применением нескольких
серверов для уменьшения общего времени выполнения. Подобное решение позволяет несколь-
ким серверам обрабатывать один запрос одновременно. Такое обычно достигается путём раз-
деления данных между серверами, обработкой на сервере своей части запроса с возвратом ре-
зультата на центральный сервер. Там данные проходят окончательную обработку и возвраща-
ются пользователю. Pgpool-II предоставляет такую возможность. Так же это может быть реа-
лизовано с применением набора средств PL/Proxy.
26.2. Трансляция журналов на резервные серверы
Постоянная архивация может использоваться для создания кластерной конфигурации высокой
степени доступности (HA) с одним или несколькими резервными серверами, способными заме-
нить ведущий сервер в случае выхода его из строя. Такую реализацию отказоустойчивости часто
называют тёплый резерв или трансляция журналов.
Ведущий и резервный серверы работают совместно для обеспечения этой возможности, при этом
они связаны опосредованно. Ведущий сервер работает в режиме постоянной архивации измене-
ний, в то время как каждый резервный сервер работает в режиме постоянного приема архивных
изменений, зачитывая WAL-файлы с ведущего. Для обеспечения этой возможности не требуется
вносить изменения в таблицы БД, что требует существенно меньших административных издержек
в сравнении с некоторыми другими решениями репликации. Так же такая конфигурация относи-
тельно слабо влияет на производительность ведущего сервера.
Непосредственную передачу записей WAL с одного сервера БД на другой обычно называют транс-
ляцией журналов (или доставкой журналов). PostgreSQL реализует трансляцию журналов на уров-
не файлов, передавая записи WAL по одному файлу (сегменту WAL) единовременно. Файлы WAL
(размером 16 МБ) можно легко и эффективно передать на любое расстояние, будь то соседний
сервер, другая система в местной сети или сервер на другом краю света. Требуемая пропускная
способность при таком подходе определяется скоростью записи транзакций на ведущем сервере.
Трансляция журналов на уровне записей более фрагментарная операция, при которой изменения
WAL передаются последовательно через сетевое соединение (см. Подраздел 26.2.5).
650Отказоустойчивость, баланси-
ровка нагрузки и репликация
Следует отметить, что трансляция журналов асинхронна, то есть записи WAL доставляются после
завершения транзакции. В результате образуется окно, когда возможна потеря данных при отказе
сервера: будут утеряны ещё не переданные транзакции. Размер этого окна при трансляции файлов
журналов может быть ограничен параметром archive_timeout, который может принимать значе-
ние меньше нескольких секунд. Тем не менее подобные заниженные значения могут потребовать
существенного увеличения пропускной способности, необходимой для трансляции файлов. При
потоковой репликации (см. Подраздел 26.2.5) окно возможности потери данных гораздо меньше.
Скорость восстановления достаточно высока, обычно резервный сервер становится полностью до-
ступным через мгновение после активации. В результате такое решение называется тёплым ре-
зервом, что обеспечивает отличную отказоустойчивость. Восстановление сервера из архивной ко-
пии базы и применение изменений обычно происходит существенно дольше. Поэтому такие дей-
ствия обычно требуются при восстановлении после аварии, не для отказоустойчивости. Так же
резервный сервер может обрабатывать читающие запросы. В этом случае он называется сервером
горячего резерва. См. Раздел 26.5 для подробной информации.
26.2.1. Планирование
Обычно разумно подбирать ведущий и резервный серверы так, чтобы они были максимально по-
хожи, как минимум с точки зрения базы данных. Тогда в частности, пути, связанные с табличны-
ми пространствами, могут передаваться без изменений. Таким образом, как на ведущем, так и на
резервных серверах должны быть одинаковые пути монтирования для табличных пространств при
использовании этой возможности БД. Учитывайте, что если CREATE TABLESPACE выполнена на
ведущем сервере, новая точка монтирования для этой команды уже должна существовать на ре-
зервных серверах до её выполнения. Аппаратная часть не должна быть в точности одинаковой, но
опыт показывает, что сопровождать идентичные системы легче, чем две различные на протяже-
нии жизненного цикла приложения и системы. В любом случае архитектура оборудования должна
быть одинаковой — например, трансляция журналов с 32-битной на 64-битную систему не будет
работать.
В общем случае трансляция журналов между серверами с различными основными версиями
PostgreSQL невозможна. Политика главной группы разработки PostgreSQL состоит в том, чтобы
не вносить изменения в дисковые форматы при обновлениях корректирующей версии, таким об-
разом, ведущий и резервный серверы, имеющие разные корректирующие версии, могут работать
успешно. Тем не менее, формально такая возможность не поддерживается и рекомендуется под-
держивать одинаковую версию ведущего и резервных серверов, насколько это возможно. При об-
новлении корректирующей версии безопаснее будет в первую очередь обновить резервные серве-
ры — новая корректирующая версия с большей вероятностью прочитает файл WAL предыдущей
корректирующей версии, чем наоборот.
26.2.2. Работа резервного сервера
Сервер, работающий в режиме резервного, последовательно применяет файлы WAL, полученные
от главного. Резервный сервер может читать файлы WAL из архива WAL (см. restore_command)
или напрямую с главного сервера по соединению TCP (потоковая репликация). Резервный сервер
также будет пытаться восстановить любой файл WAL, найденный в кластере резервного в каталоге
pg_wal. Это обычно происходит после перезапуска сервера, когда он применяет заново файлы WAL,
полученные от главного сервера перед перезапуском. Но можно и вручную скопировать файлы в
каталог pg_wal, чтобы применить их в любой момент времени.
В момент запуска резервный сервер начинает восстанавливать все доступные файлы WAL, раз-
мещённые в архивном каталоге, указанном в команде restore_command. По достижении конца до-
ступных файлов WAL или при сбое команды restore_command сервер пытается восстановить все
файлы WAL, доступные в каталоге pg_wal. Если это не удаётся и потоковая репликация настрое-
на, резервный сервер пытается присоединиться к ведущему и начать закачивать поток WAL с по-
следней подтверждённой записи, найденной в архиве или pg_wal. Если это действие закончилось
неудачей, или потоковая репликация не настроена, или соединение позднее разорвалось, резерв-
ный сервер возвращается к шагу 1 и пытается восстановить файлы из архива вновь. Цикл обраще-
651Отказоустойчивость, баланси-
ровка нагрузки и репликация
ния за файлами WAL к архиву, pg_wal, и через потоковую репликацию продолжается до остановки
сервера или переключения его роли, вызванного файлом-триггером.
Режим резерва завершается и сервер переключается в обычный рабочий режим при получении
команды pg_ctl promote или при обнаружении файла-триггера (trigger_file). Перед переклю-
чением сервер восстановит все файлы WAL, непосредственно доступные из архива или pg_wal, но
пытаться подключиться к главному серверу он больше не будет.
26.2.3. Подготовка главного сервера для работы с резервными
Настройка постоянного архивирования на ведущем сервере в архивный каталог, доступный с ре-
зервного, описана в Разделе 25.3. Расположение архива должно быть доступно с резервного сер-
вера даже при отключении главного, то есть его следует разместить на резервном или другом до-
веренном, но не на главном сервере.
При использовании потоковой репликации следует настроить режим аутентификации на ведущем
сервере, чтобы разрешить соединения с резервных. Для этого создать роль и обеспечить подходя-
щую запись в файле pg_hba.conf в разделе доступа к БД replication. Так же следует убедиться,
что для параметра max_wal_senders задаётся достаточно большое значение в конфигурационном
файле ведущего сервера. При использовании слотов для репликации также достаточно большое
значение нужно задать для max_replication_slots.
Создание базовой резервной копии, необходимой для запуска резервного сервера, описано в Под-
разделе 25.3.2.
26.2.4. Настройка резервного сервера
Для запуска резервного сервера нужно восстановить резервную копию, снятую с ведущего (см.
Подраздел 25.3.4). Затем нужно создать файл команд восстановления recovery.conf в каталоге
данных кластера резервного сервера и включить режим standby_mode. Задайте в restore_command
обычную команду копирования файлов из архива WAL. Если планируется несколько резервных
серверов в целях отказоустойчивости, установите для recovery_target_timeline значение latest,
чтобы резервный сервер переходил на новую линию времени, образуемую при отработке отказа
и переключении на другой сервер.
Примечание
Не используйте pg_standby или подобные средства со встроенным режимом резервно-
го сервера, описанным здесь. restore_command должна немедленно прекратиться при
отсутствии файла; сервер повторит команду вновь при необходимости. Использование
средств, подобных pg_standby, описано в Разделе 26.4.
При необходимости потоковой репликации заполните primary_conninfo параметрами строки со-
единения для libpq, включая имя (или IP-адрес) сервера и все остальные необходимые данные для
соединения с ведущим сервером. Если ведущий требует пароль для аутентификации, пароль мо-
жет быть так же передан в primary_conninfo.
Если резервный сервер настраивается в целях отказоустойчивости, на нём следует настроить ар-
хивацию WAL, соединения и аутентификацию, как на ведущем сервере, потому что резервный сер-
вер станет ведущим после отработки отказа.
При использовании архива WAL его размер может быть уменьшен с помощью команды в па-
раметре archive_cleanup_command, которая удаляет файлы уже не нужные для дальнейшей ра-
боты резервного сервера. Утилита pg_archivecleanup разработана специально для использова-
ния в archive_cleanup_command при типичной конфигурации с одним резервным сервером (см.
pg_archivecleanup). Следует отметить, что если архив используется в целях резервирования, сле-
дует сохранять все файлы необходимые для восстановления как минимум с последней базовой ре-
зервной копии, даже если они не нужны для резервного сервера.
652Отказоустойчивость, баланси-
ровка нагрузки и репликация
Простой пример recovery.conf:
standby_mode = ‘on’
primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
restore_command = ‘cp /path/to/archive/%f %p’
archive_cleanup_command = ‘pg_archivecleanup /path/to/archive %r’
Можно поддерживать любое количество резервных серверов, но при применении потоковой ре-
пликации необходимо убедиться, что значение max_wal_senders на ведущем достаточно большое,
чтобы все они могли подключиться одновременно.
26.2.5. Потоковая репликация
При потоковой репликации резервный сервер может работать с меньшей задержкой, чем при
трансляции файлов. Резервный сервер подключается к ведущему, который передаёт поток запи-
сей WAL резервному в момент их добавления, не дожидаясь окончания заполнения файла WAL.
Потоковая репликация асинхронна по умолчанию (см. Подраздел 26.2.8), то есть имеется неболь-
шая задержка между подтверждением транзакции на ведущем сервере и появлением этих изме-
нений на резервном. Тем не менее, эта задержка гораздо меньше, чем при трансляции файлов
журналов, обычно в пределах одной секунды, если резервный сервер достаточно мощный и справ-
ляется с нагрузкой. При потоковой репликации настраивать archive_timeout для уменьшения ок-
на потенциальной потери данных не требуется.
При потоковой репликации без постоянной архивации на уровне файлов, сервер может избавить-
ся от старых сегментов WAL до того, как резервный получит их. В этом случае резервный сервер
потребует повторной инициализации из новой базовой резервной копии. Этого можно избежать,
установив для wal_keep_segments достаточно большое значение, при котором сегменты WAL бу-
дут защищены от ранней очистки, либо настроив слот репликации для резервного сервера. Если
с резервного сервера доступен архив WAL, этого не требуется, так как резервный может всегда
обратиться к архиву для восполнения пропущенных сегментов.
Чтобы включить потоковую репликацию, сначала настройте резервный сервер на приём трансля-
ции журналов, как описано в Разделе  26.2. Затем сделайте следующий шаг — переключите ре-
зервный сервер в режим репликации, установив в primary_conninfo в файле recovery.conf стро-
ку подключения, указывающую на ведущий. Настройте listen_addresses и параметры аутентифи-
кации (см. pg_hba.conf) на ведущем сервере таким образом, чтобы резервный смог подключиться
к псевдобазе replication на ведущем (см. Подраздел 26.2.5.1).
В
системах,
поддерживающих
параметр
сокета
keepalive,
подходящие
значения
tcp_keepalives_idle, tcp_keepalives_interval и tcp_keepalives_count помогут ведущему вовремя заме-
тить разрыв соединения.
Установите максимальное количество одновременных соединений с резервных серверов (см. опи-
сание max_wal_senders.
При запуске резервного сервера с правильно установленным primary_conninfo резервный под-
ключится к ведущему после воспроизведения всех файлов WAL, доступных из архива. При успеш-
ном установлении соединения можно увидеть процесс walreceiver на резервном сервере и соот-
ветствующий процесс walsender на ведущем.
26.2.5.1. Аутентификация
Право использования репликации очень важно ограничить так, чтобы только доверенные поль-
зователи могли читать поток WAL, так как из него можно извлечь конфиденциальную информа-
цию. Резервный сервер должен аутентифицироваться на ведущем от имени суперпользователя или
пользователя с правом REPLICATION. Настоятельно рекомендуется создавать выделенного пользо-
вателя с правами REPLICATION и LOGIN специально для репликации. Хотя право REPLICATION даёт
очень широкие полномочия, оно не позволяет модифицировать данные в ведущей системе, тогда
как с правом SUPERUSER это можно делать.
653Отказоустойчивость, баланси-
ровка нагрузки и репликация
Список аутентификации клиентов для репликации содержится в pg_hba.conf в записях с установ-
ленным значением replication в поле database. Например, если резервный сервер запущен на
компьютере с IP-адресом 192.168.1.100 и учётная запись для репликации foo, администратор мо-
жет добавить следующую строку в файл pg_hba.conf ведущего:</p>
<h1 id="Разрешить-пользователю-foo-с-компьютера-1921681100-подключаться-к-этому">Разрешить пользователю “foo” с компьютера 192.168.1.100 подключаться к этому</h1>
<h1 id="серверу-в-качестве-партнёра-репликации-если-был-передан-правильный-пароль">серверу в качестве партнёра репликации, если был передан правильный пароль.</h1>
<p>#</p>
<h1 id="type-database">TYPE DATABASE</h1>
<p>USER
ADDRESS
METHOD
host
replication
foo
192.168.1.100/32
md5
Имя компьютера и номер порта для ведущего, имя пользователя для соединения и пароль указы-
ваются в файле recovery.conf. Пароль так же может быть задан через файл ~/.pgpass на резерв-
ном сервере (указанном в определении с replication в поле database). Например, если ведущий
принимает подключения по IP-адресу 192.168.1.50, в порту 5432, пользователя для репликации
foo с паролем foopass, то администратор может добавить следующую строку в файл recovery.conf
на резервном сервере:</p>
<h1 id="Резервный-сервер-подключается-к-ведущему-работающему-на-компьютере-192168150">Резервный сервер подключается к ведущему, работающему на компьютере 192.168.1.50</h1>
<h1 id="порт-5432-от-имени-пользователя-foo-с-паролем-foopass">(порт 5432), от имени пользователя “foo” с паролем “foopass”.</h1>
<p>primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
26.2.5.2. Наблюдение
Важным индикатором стабильности работы потоковой репликации является количество запи-
сей WAL, созданных на ведущем, но ещё не применённых на резервном сервере. Задержку мож-
но подсчитать, сравнив текущую позиции записи WAL на ведущем с последней позицией WAL,
полученной на резервном сервере. Эти позиции можно узнать, воспользовавшись функциями
pg_current_wal_lsn на ведущем и pg_last_wal_receive_lsn на резервном, соответственно (за по-
дробностями обратитесь к Таблице 9.79 и Таблице 9.80). Последняя полученная позиция WAL на
резервном сервере также выводится в состоянии процесса-приёмника WAL, которое показывает
команда ps (подробнее об этом в Разделе 28.1).
Список процессов-передатчиков WAL можно получить через представление pg_stat_replication.
Значительная разница между pg_current_wal_lsn и полем sent_lsn этого представления может
указывать на то, что главный сервер работает с большой нагрузкой, тогда как разница между
sent_lsn и pg_last_wal_receive_lsn на резервном может быть признаком задержек в сети или
большой нагрузки резервного сервера.
На сервере горячего резерва состояние процесса-приёмника WAL можно получить через
представление pg_stat_wal_receiver. Большая разница между pg_last_wal_replay_lsn и полем
received_lsn свидетельствует о том, что WAL поступает быстрее, чем удаётся его воспроизвести.
26.2.6. Слоты репликации
Слоты репликации автоматически обеспечивают механизм сохранения сегментов WAL, пока они
не будут получены всеми резервными и главный сервер не будет удалять строки, находящиеся в
статусе recovery conflict даже при отключении резервного.
Вместо использования слотов репликации для предотвращения удаления старых сегментов WAL
можно применять wal_keep_segments, или сохранять сегменты в архиве с помощью команды
archive_command. Тем не менее, эти методы часто приводят к тому, что хранится больше сегмен-
тов WAL, чем необходимо, в то время как слоты репликации оставляют только то количество сег-
ментов, которое необходимо. Преимущество этих методов состоит в том, что они чётко задают
объёмы места, необходимого для pg_wal; в то время как текущая реализация слотов репликации
не предоставляет такой возможности.
Подобным образом, параметры hot_standby_feedback и vacuum_defer_cleanup_age позволяют защи-
тить востребованные строки от удаления при очистке, но первый параметр не защищает в тот про-
межуток времени, когда резервный сервер не подключён, а для последнего часто нужно задавать
большое значение, чтобы обеспечить должную защиту. Слоты репликации решают эти проблемы.
654Отказоустойчивость, баланси-
ровка нагрузки и репликация
26.2.6.1. Запросы и действия слотов репликации
Каждый слот репликации обладает именем, состоящим из строчных букв, цифр и символов под-
чёркивания.
Имеющиеся слоты репликации
pg_replication_slots.
и
их
статус
можно
просмотреть
в
представлении
Слоты могут быть созданы и удалены как с помощью протокола потоковой репликации (см. Раз-
дел 53.4), так и посредством функций SQL (см. Подраздел 9.26.6).
26.2.6.2. Пример конфигурации
Для создания слота репликации выполните:
postgres=# SELECT * FROM pg_create_physical_replication_slot(‘node_a_slot’);
slot_name | lsn
————-+—–
node_a_slot |
postgres=# SELECT slot_name, slot_type, active FROM pg_replication_slots;
slot_name | slot_type | active
————-+———–+——–
node_a_slot | physical | f
(1 row)
Для настройки резервного сервера на использование этого слота primary_slot_name должно быть
настроено в конфигурации recovery.conf резервного. Вот простейший пример:
standby_mode = ‘on’
primary_conninfo = ‘host=192.168.1.50 port=5432 user=foo password=foopass’
primary_slot_name = ‘node_a_slot’
26.2.7. Каскадная репликация
Свойство каскадной репликации позволяет резервному серверу принимать соединения реплика-
ции и потоки WAL от других резервных, выступающих посредниками. Это может быть полезно для
уменьшения числа непосредственных подключений к главному серверу, а также для уменьшения
накладных расходов при передаче данных в интрасети.
Резервный сервер, выступающий как получатель и отправитель, называется каскадным резерв-
ным сервером. Резервные серверы, стоящие ближе к главному, называются серверами верхнего
уровня, а более отдалённые — серверами нижнего уровня. Каскадная репликация не накладыва-
ет ограничений на количество или организацию последующих уровней, а каждый резервный со-
единяется только с одним сервером вышестоящего уровня, который в конце концов соединяется
с единственным главным/ведущим сервером.
Резервный сервер каскадной репликации не только получает записи WAL от главного, но так же
восстанавливает их из архива. Таким образом, даже если соединение с сервером более высокого
уровня разорвётся, потоковая репликация для последующих уровней будет продолжаться до ис-
черпания доступных записей WAL.
Каскадная репликация в текущей реализации асинхронна. Параметры синхронной репликации
(см. Подраздел 26.2.8) в настоящее время не оказывают влияние на каскадную репликацию.
Распространение обратной связи горячего резерва работает от нижестоящего уровня к вышесто-
ящему уровню вне зависимости от способа организации связи.
Если резервный сервер вышестоящего уровня будет преобразован в новый главный, серве-
ры нижестоящего уровня продолжат получать поток с нового главного при условии, что
recovery_target_timeline установлен в значение ‘latest’.
655Отказоустойчивость, баланси-
ровка нагрузки и репликация
Для использования каскадной репликации необходимо настроить резервный каскадный сервер
на прием соединений репликации (то есть установить max_wal_senders и hot_standby, настроить
host-based authentication). Так же может быть необходимо настроить на нижестоящем резервном
значение primary_conninfo на каскадный резервный сервер.
26.2.8. Синхронная репликация
По умолчанию в PostgreSQL потоковая репликация асинхронна. Если ведущий сервер выходит из
строя, некоторые транзакции, которые были подтверждены, но не переданы на резервный, могут
быть потеряны. Объём потерянных данных пропорционален задержке репликации на момент от-
работки отказа.
Синхронная репликация предоставляет возможность гарантировать, что все изменения, внесён-
ные в транзакции, были переданы одному или нескольким синхронным резервным серверам. Это
увеличивает стандартный уровень надёжности, гарантируемый при фиксации транзакции. Этот
уровень защиты соответствует второму уровню безопасности репликации из теории вычислитель-
ной техники, или групповой безопасности первого уровня (безопасности групповой и уровня 1),
когда выбран режим synchronous_commit remote_write.
При синхронной репликации каждая фиксация пишущей транзакции ожидает подтверждения то-
го, что запись фиксации помещена в журнал предзаписи на диске на обоих серверах: ведущем и
резервном. При таком варианте потеря данных может произойти только в случае одновременного
выхода из строя ведущего и резервного серверов. Это обеспечивает более высокий уровень надёж-
ности, при условии продуманного подхода системного администратора к вопросам размещения и
управления этими серверами. Ожидание подтверждения увеличивает уверенность в том, что дан-
ные не будут потеряны во время сбоя сервера, но при этом увеличивает время отклика для обра-
ботки транзакции. Минимальное время ожидания равно времени передачи данных от ведущего к
резервному и обратно.
Транзакции только для чтения и откат транзакции не требуют ожидания для ответа с резервного
сервера. Промежуточные подтверждения не ожидают ответа от резервного сервера, только под-
тверждение верхнего уровня. Долгие операции вида загрузки данных или построения индекса
не ожидают финального подтверждения. Но все двухфазные подтверждения требуют ожидания,
включая подготовку и непосредственно подтверждение.
Синхронным резервным сервером может быть резервный сервер при физической репликации или
подписчик при логической репликации. Это также может быть другой потребитель потока логи-
ческой или физической репликации, способный отправлять в ответ требуемые сообщения. Поми-
мо встроенных систем логической и физической репликации, к таким потребителям относятся
специальные программы, pg_receivewal и pg_recvlogical, а также некоторые сторонние систе-
мы репликации и внешние программы. Подробнее об организации синхронной репликации с их
использованием можно узнать в соответствующей документации.
26.2.8.1. Базовая настройка
При настроенной потоковой репликации установка синхронной репликации требует только допол-
нительной настройки: необходимо выставить synchronous_standby_names в непустое значение. Так
же необходимо установить synchronous_commit в значение on, но так как это значение по умолча-
нию, обычно действий не требуется. (См. Подраздел 19.5.1 и Подраздел 19.6.2.) В такой конфигу-
рации каждая транзакция будет ожидать подтверждение того, что на резервном сервере произо-
шла запись транзакции в надёжное хранилище. Значение synchronous_commit может быть выстав-
лено для отдельного пользователя, может быть прописано в файле конфигурации, для конкретного
пользователя или БД или динамически изменено приложением для управления степенью надёж-
ности на уровне отдельных транзакций.
После сохранения записи о фиксации транзакции на диске ведущего сервера эта запись WAL пере-
даётся резервному серверу. Резервный сервер отвечает подтверждающим сообщением после со-
хранения каждого нового блока данных WAL на диске, если только wal_receiver_status_interval
на нём не равен нулю. В случае, когда выбран режим synchronous_commit remote_apply, резерв-
ный сервер передаёт подтверждение после воспроизведения записи фиксации, когда транзакция
656Отказоустойчивость, баланси-
ровка нагрузки и репликация
становится видимой. Если резервный сервер выбран на роль синхронного резервного в соответ-
ствии со значением synchronous_standby_names на ведущем, подтверждающие сообщения с этого
сервера, в совокупности с сообщениями с других синхронных серверов, будут сигналом к завер-
шению ожидания при фиксировании транзакций, требующих подтверждения сохранения записи
фиксации. Эти параметры позволяют администратору определить, какие резервные серверы будут
синхронными резервными. Заметьте, что настройка синхронной репликации в основном осуществ-
ляется на главном сервере. Перечисленные в списке резервных серверы должны быть подключе-
ны к нему непосредственно; он ничего не знает о резервных серверах, подключённых каскадно,
через промежуточные серверы.
Если synchronous_commit имеет значение remote_write, то в случае подтверждения транзакции
ответ от резервного сервера об успешном подтверждении будет передан, когда данные запишутся
в операционной системе, но не когда данные будет реально сохранены на диске. При таком зна-
чении уровень надёжности снижается по сравнению со значением on. Резервный сервер может
потерять данные в случае падения операционной системы, но не в случае падения PostgreSQL.
Тем не менее, этот вариант полезен на практике, так как позволяет сократить время отклика для
транзакции. Потеря данных может произойти только в случае одновременного сбоя ведущего и
резервного, осложнённого повреждением БД на ведущем.
Если synchronous_commit имеет значение remote_apply, то для завершения фиксирования тран-
закции потребуется дождаться, чтобы текущие синхронные резервные серверы сообщили, что они
воспроизвели транзакцию и её могут видеть запросы пользователей. В простых случаях это позво-
ляет обеспечить обычный уровень согласованности и распределение нагрузки.
Пользователи прекратят ожидание в случае запроса на быструю остановку сервера. В то время
как при использовании асинхронной репликации сервер не будет полностью остановлен, пока все
исходящие записи WAL не переместятся на текущий присоединённый резервный сервер.
26.2.8.2. Несколько синхронных резервных серверов
Синхронная репликация поддерживает применение одного или нескольких синхронных резерв-
ных серверов; транзакции будут ждать, пока все резервные серверы, считающиеся синхронными,
не подтвердят получение своих данных. Число синхронных резервных серверов, от которых тран-
закции должны ждать подтверждения, задаётся в параметре synchronous_standby_names. В этом
параметре также задаётся список имён резервных серверов и метод (FIRST или ANY) выбора син-
хронных из заданного списка.
С методом FIRST производится синхронная репликация на основе приоритетов, когда транзакции
фиксируются только после того, как их записи в WAL реплицируются на заданное число синхрон-
ных резервных серверов, выбираемых согласно приоритетам. Серверы, имена которых идут в на-
чале списка, имеют больший приоритет и выбираются на роль синхронных. Другие резервные сер-
веры, идущие в этом списке за ними, считаются потенциальными синхронными. Если один из те-
кущих синхронных резервных серверов по какой-либо причине отключается, он будет немедленно
заменён следующим по порядку резервным сервером.
Пример значения synchronous_standby_names для нескольких синхронных резервных серверов,
выбираемых по приоритетам:
synchronous_standby_names = ‘FIRST 2 (s1, s2, s3)’
В данном примере, если работают четыре резервных сервера s1, s2, s3 и s4, два сервера s1 и s2
будут выбраны на роль синхронных резервных, так как их имена идут в начале этого списка. Сервер
s3 будет потенциальным резервным и возьмёт на себя роль синхронного резервного при отказе s1
или s2. Сервер s4 будет асинхронным резервным, так как его имя в этом списке отсутствует.
С методом ANY производится синхронная репликация на основе кворума, когда транзакции фикси-
руются только после того, как их записи в WAL реплицируются на как минимум заданное число
синхронных серверов в списке.
Пример значения synchronous_standby_names для нескольких синхронных резервных серверов,
образующих кворум:
657Отказоустойчивость, баланси-
ровка нагрузки и репликация
synchronous_standby_names = ‘ANY 2 (s1, s2, s3)’
В данном примере, если работают четыре резервных сервера s1, s2, s3 и s4, транзакции будут
фиксироваться только после получения ответов как минимум от двух резервных серверов из s1, s2
и s3. Сервер s4 будет асинхронным резервным, так как его имя в этом списке отсутствует.
Состояние
синхронности
pg_stat_replication.
резервных
серверов
можно
увидеть
в
представлении
26.2.8.3. Планирование производительности
Организуя синхронную репликацию, обычно нужно обстоятельно обдумать конфигурацию и раз-
мещение резервных серверов, чтобы обеспечить приемлемую производительность приложений.
Ожидание не потребляет системные ресурсы, но блокировки транзакций будут сохраняться до
подтверждения передачи. Как следствие, непродуманное использование синхронной репликации
приведёт к снижению производительности БД из-за увеличения времени отклика и числа кон-
фликтов.
PostgreSQL позволяет разработчикам выбрать требуемый уровень надёжности, обеспечиваемый
при репликации. Он может быть установлен для системы в целом, для отдельного пользователя
или соединения или даже для отдельной транзакции.
Например, в рабочей нагрузке приложения 10% изменений могут относиться к важным данным
клиентов, а 90% — к менее критичным данным, потеряв которые, бизнес вполне сможет выжить
(например, это могут быть текущие разговоры пользователей между собой).
При настройке уровня синхронности репликации на уровне приложения (на ведущем) можно за-
дать синхронную репликацию для большинства важных изменений без замедления общего рабо-
чего ритма. Возможность настройки на уровне приложения является важным и практичным сред-
ством для получения выгод синхронной репликации при высоком быстродействии.
Следует иметь в виду, что пропускная способность сети должна быть больше скорости генериро-
вания данных WAL.
26.2.8.4. Планирование отказоустойчивости
В synchronous_standby_names задаётся количество и имена синхронных резервных серверов,
от которых будет ожидаться подтверждение при фиксировании транзакции, когда параметру
synchronous_commit присвоено значение on, remote_apply или remote_write. Фиксирование тран-
закции в таком режиме может не завершиться никогда, если один из синхронных резервных сер-
веров выйдет из строя.
Поэтому для высокой степени доступности лучше всего обеспечить наличие синхронных резерв-
ных серверов в должном количестве. Для этого можно перечислить несколько потенциальных ре-
зервных серверов в строке synchronous_standby_names.
При синхронной репликации на основе приоритетов синхронными резервными серверами станут
серверы, имена которых стоят в этом списке первыми. Следующие за ними серверы будут стано-
виться синхронными резервными при отказе одного из текущих.
При синхронной репликации на основе кворума кандидатами на роль синхронных резервных будут
все серверы в списке. И если один из них откажет, другие серверы будут продолжать исполнять
эту роль.
Когда к ведущему серверу впервые присоединяется резервный, он ещё не будет полностью синхро-
низированным. Это называется состоянием навёрстывания. Как только отставание резервного от
ведущего сервера сократится до нуля в первый раз, система перейдет в состояние потоковой пе-
редачи в реальном времени. Сразу после создания резервного сервера навёрстывание может быть
длительным. В случае выключения резервного сервера длительность этого процесса увеличится
соответственно продолжительности простоя. Резервный сервер может стать синхронным только
658Отказоустойчивость, баланси-
ровка нагрузки и репликация
по достижении состояния потоковой передачи. Это состояние можно проследить в представлении
pg_stat_replication.
Если ведущий сервер перезапускается при наличии зафиксированных транзакций, ожидающих
подтверждения, эти транзакции будут помечены как полностью зафиксированные после восста-
новления ведущего. При этом нельзя гарантировать, что все резервные серверы успели получить
все текущие данные WAL к моменту падения ведущего. Таким образом, некоторые транзакции мо-
гут считаться незафиксированными на резервном сервере, даже если они считаются зафиксиро-
ванными на ведущем. Гарантия, которую мы можем дать, состоит в том, что приложение не полу-
чит явного подтверждения успешной фиксации, пока не будет уверенности, что данные WAL по-
лучены всеми синхронными резервными серверами.
Если запустить синхронные резервные серверы в указанном количестве не удаётся, вам следует
уменьшить число синхронных серверов, подтверждения которых требуются для завершения фик-
сации транзакций, в параметре synchronous_standby_names (или вовсе отключить его) и переза-
грузить файл конфигурации на ведущем сервере.
В случае если ведущий сервер стал недоступным для оставшихся резервных, следует переклю-
читься на наиболее подходящий из имеющихся резервных серверов.
Если необходимо пересоздать резервный сервер при наличии ожидающей подтверждения тран-
закции необходимо убедиться, что команды pg_start_backup() и pg_stop_backup() запускаются в
сессии с установленным synchronous_commit = off, в противном случае эти запросы на подтвер-
ждение будут бесконечными для вновь возникшего резервного сервера.
26.2.9. Непрерывное архивирование на резервном сервере
Когда на резервном сервере применяется последовательное архивирование WAL, возможны два
различных сценария: архив WAL может быть общим для ведущего и резервного сервера, либо ре-
зервный сервер может иметь собственный архив WAL. Когда резервный работает с собственным
архивом WAL, установите в archive_mode значение always, и он будет вызывать команду архива-
ции для каждого сегмента WAL, который он получает при восстановлении из архива или потоко-
вой репликации. В случае с общим архивом можно поступить аналогично, но archive_command
должна проверять, нет ли в архиве файла, идентичного архивируемому. Таким образом, команда
archive_command должна позаботиться о том, чтобы существующий файл не был заменён файлом
с другим содержимым, а в случае попытки повторного архивирования должна сообщать об успеш-
ном выполнении. При этом все эти действия должны быть рассчитаны на условия гонки, возмож-
ные, если два сервера попытаются архивировать один и тот же файл одновременно.
Если в archive_mode установлено значение on, архивация в режиме восстановления или резерва не
производится. В случае повышения резервного сервера, он начнёт архивацию после повышения,
но в архив не попадут те файлы WAL, которые генерировал не он сам. Поэтому, чтобы в архиве
оказался полный набор файлов WAL, необходимо обеспечить архивацию всех файлов WAL до того,
как они попадут на резервный сервер. Это естественным образом происходит при трансляции фай-
лов журналов, так как резервный сервер может восстановить только файлы, которые находятся в
архиве, однако при потоковой репликации это не так. Когда сервер работает не в режиме резерва,
различий между режимами on и always нет.
26.3. Отработка отказа
Если ведущий сервер отказывает, резервный должен начать процедуры отработки отказа.
Если отказывает резервный сервер, никакие действия по отработке отказа не требуются. Если ре-
зервный сервер будет перезапущен, даже через некоторое время, немедленно начнётся операция
восстановления, благодаря возможности возобновляемого восстановления. Если вернуть резерв-
ный сервер в строй невозможно, необходимо создать полностью новый экземпляр резервного сер-
вера.
Когда ведущий сервер отказывает и резервный сервер становится новым ведущим, а затем старый
ведущий включается снова, необходим механизм для предотвращения возврата старого к роли
659Отказоустойчивость, баланси-
ровка нагрузки и репликация
ведущего. Иногда его называют STONITH (Shoot The Other Node In The Head, «Выстрелите в голову
другому узлу»), что позволяет избежать ситуации, когда обе системы считают себя ведущими, и
в результате возникают конфликты и потеря данных.
Во многих отказоустойчивых конструкциях используются всего две системы: ведущая и резервная,
с некоторым контрольным механизмом, который постоянно проверяет соединение между ними
и работоспособность ведущей. Также возможно применение третьей системы (называемой следя-
щим сервером) для исключения некоторых вариантов нежелательной отработки отказа, но эта до-
полнительная сложность оправдана, только если вся схема достаточно хорошо продумана и тща-
тельно протестирована.
PostgreSQL не предоставляет системного программного обеспечения, необходимого для опреде-
ления сбоя на ведущем и уведомления резервного сервера баз данных. Имеется множество подоб-
ных инструментов, которые хорошо интегрируются со средствами операционной системы, требу-
емыми для успешной отработки отказа, например, для миграции IP-адреса.
Когда происходит переключение на резервный сервер, только один сервер продолжает работу. Это
состояние называется ущербным. Бывший резервный сервер теперь является ведущим, а бывший
ведущий отключён и может оставаться отключённым. Для возвращения к нормальному состоянию
необходимо запустить новый резервный сервер, либо на бывшем ведущем, либо в третьей, возмож-
но, новой системе. Ускорить этот процесс в больших кластерах позволяет утилита pg_rewind. По
завершении этого процесса можно считать, что ведущий и резервный сервер поменялись ролями.
Некоторые используют третий сервер в качестве запасного для нового ведущего, пока не будет
воссоздан новый резервный сервер, хотя это, очевидно, усложняет конфигурацию системы и ра-
бочие процедуры.
Таким образом, переключение с ведущего сервера на резервный может быть быстрым, но требу-
ет некоторого времени для повторной подготовки отказоустойчивого кластера. Регулярные пере-
ключения с ведущего сервера на резервный полезны, так как при этом появляется плановое время
для отключения и проведения обслуживания. Это также позволяет убедиться в работоспособности
механизма отработки отказа и гарантировать, что он действительно будет работать, когда потре-
буется. Эти административные процедуры рекомендуется документировать письменно.
Чтобы сделать ведущим резервный сервер, принимающий журналы, выполните команду pg_ctl
promote или создайте файл-триггер с именем и путём, заданным в параметре trigger_file в фай-
ле recovery.conf. Если для переключения планируется использовать команду pg_ctl promote,
указывать trigger_file не требуется. Если резервный сервер применяется для анализа данных,
чтобы только разгрузить ведущий, выполняя запросы на чтение, а не обеспечивать отказоустой-
чивость, повышать его до ведущего не понадобится.
26.4. Другие методы трансляции журнала
Встроенному режиму резерва, описанному в предыдущем разделе, есть альтернатива — задать в
restore_command команду, следящую за содержимым архива. Эта возможность доступна только
для версии 8.4 и выше. В такой конфигурации режим standby_mode выключается, так как реали-
зуется отдельный механизм слежения за данными, требующихся для резервного сервера. См. мо-
дуль pg_standby для примера реализации такой возможности.
Необходимо отметить, что в этом режиме сервер будет применять только один файл WAL одновре-
менно, то есть если использовать резервный сервер для запросов (см. сервер горячего резерва),
будет задержка между операциями на главном и моментом видимости этой операции резервным,
соответствующей времени заполнения файла WAL. archive_timeout можно использовать для сни-
жения этой задержки. Так же необходимо отметить, что нельзя совмещать этот метод с потоковой
репликацией.
В процессе работы на ведущем сервере и резервном будет происходить обычное формирование
архивов и их восстановление. Единственной точкой соприкосновения двух серверов будут только
архивы файлов WAL на обеих сторонах: на ведущем архивы формируются, на резервном происходит
чтение данных из архивов. Следует внимательно следить за тем, чтобы архивы WAL от разных
660Отказоустойчивость, баланси-
ровка нагрузки и репликация
ведущих серверов не смешивались или не перепутывались. Архив не должен быть больше, чем это
необходимо для работы резерва.
Магия, заставляющая работать вместе два слабо связанных сервера, проста: restore_command,
выполняющаяся на резервном при запросе следующего файла WAL, ожидает его доступности
на ведущем. Команда restore_command задаётся в файле recovery.conf на резервном сервере.
Обычно процесс восстановления запрашивает файл из архива WAL, сообщая об ошибке в слу-
чае его недоступности. Для работы резервного сервера недоступность очередного файла WAL яв-
ляется обычной ситуацией, резервный просто ожидает его появления. Для файлов, оканчиваю-
щихся на .history, ожидание не требуется, поэтому возвращается ненулевой код. Ожидающая
restore_command может быть написана как пользовательский скрипт, который в цикле опрашива-
ет, не появился ли очередной файл WAL. Также должен быть способ инициировать переключение
роли, при котором цикл в restore_command должен прерваться, а резервный сервер должен полу-
чить ошибку «файл не найден». При этом восстановление завершится, и резервный сервер сможет
станет обычным.
Псевдокод для подходящей restore_command:
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
sleep(100000L);
/* ждать ~0.1 сек*/
if (CheckForExternalTrigger())
triggered = true;
}
if (!triggered)
CopyWALFileForRecovery();
Рабочий пример ожидающей restore_command представлен в модуле pg_standby. К нему следует
обратится за примером правильной реализации логики, описанной выше. Он так же может быть
расширен для поддержки особых конфигураций и окружений.
Метод вызова переключения является важной частью планирования и архитектуры. Один из воз-
можных вариантов — команда restore_command. Она исполняется единожды для каждого файла
WAL, но процесс, запускаемый restore_command, создаётся и завершается для каждого файла, так
что это не служба и не серверный процесс, и применить сигналы и реализовать их обработчик
в нём нельзя. Поэтому restore_command не подходит для отработки отказа. Можно организовать
переключение по тайм-ауту, в частности, связав его с известным значением archive_timeout на
ведущем. Однако это не очень надёжно, так как переключение может произойти и из-за проблем
в сети или загруженности ведущего сервера. В идеале для этого следует использовать механизм
уведомлений, например явно создавать файл-триггер, если это возможно.
26.4.1. Реализация
Сокращённая процедура настройки для резервного сервера с применением альтернативного ме-
тода указана ниже. Для подробностей по каждому шагу следует обратиться к указанному разделу.</p>
<ol>
  <li>Разверните ведущую и резервную системы, сделав их максимально одинаковыми, включая две
одинаковые копии PostgreSQL одного выпуска.</li>
  <li>Настройте постоянную архивацию с ведущего сервера в каталог архивов WAL на резервном. Убе-
дитесь, что archive_mode, archive_command и archive_timeout установлены в соответствующие
значения на ведущем (см. Подраздел 25.3.1).</li>
  <li>Создайте базовую копию данных ведущего сервера (см. Подраздел 25.3.2) и восстановите её на
резервном.</li>
  <li>Запустите восстановление на резервном сервере из локального архива WAL с помощью команды
restore_command из файла recovery.conf как описано выше (см. Подраздел 25.3.4).
Поток восстановления только читает архив WAL, поэтому, как только файл WAL скопирован на
резервную систему, его можно копировать на ленту в то время, как его читает резервный сервер.
661Отказоустойчивость, баланси-
ровка нагрузки и репликация
Таким образом, работа резервного сервера в целях отказоустойчивости может быть совмещена с
долговременным сохранением файлов для восстановления после катастрофических сбоев.
Для целей тестирования возможен запуск ведущего и резервного сервера в одной системе. Это не
обеспечивает надёжность серверов, так же как и не подходит под описание высокой доступности.
26.4.2. Построчная трансляция журнала
Так же возможна реализация построчной трансляции журналов с применением альтернативного
метода, хотя это требует дополнительных доработок, а изменения будут видны для запросов на
сервере горячего резерва только после передачи полного файла WAL.
Внешняя программа может вызвать функцию pg_walfile_name_offset() (см. Раздел 9.26) для по-
иска имени файла и точного смещения в нём от текущего конца WAL. Можно получить доступ
к файлу WAL напрямую и скопировать данные из последнего известного окончания WAL до теку-
щего окончания на резервном сервере. При таком подходе интервал возможной потери данных
определяется временем цикла работы программы копирования, что может составлять очень ма-
лую величину. Так же не потребуется напрасно использовать широкую полосу пропускания для
принудительного архивирования частично заполненного файла сегмента. Следует отметить, что
на резервном сервере скрипт команды restore_command работает только с файлом WAL целиком,
таким образом, копирование данных нарастающим итогом не может быть выполнено на резерв-
ном обычными средствами. Это используется только в случае отказа ведущего — когда послед-
ний частично сформированный файл WAL предоставляется резервному непосредственно перед пе-
реключением. Корректная реализация этого процесса требует взаимодействия скрипта команды
restore_command с данными из программы копирования.
Начиная с PostgreSQL версии 9.0 можно использовать потоковую репликацию (см. Подраз-
дел 26.2.5) для получения этих же преимуществ меньшими усилиями.
26.5. Горячий резерв
Термин «горячий резерв» используется для описания возможности подключаться к серверу и вы-
полнять запросы на чтение, в то время как сервер находится в режиме резерва или восстановле-
ния архива. Это полезно и для целей репликации, и для восстановления желаемого состояния из
резервной копии с высокой точностью. Так же термин «горячий резерв» описывает способность
сервера переходить из режима восстановления к обычной работе, в то время как пользователи
продолжают выполнять запросы и/или их соединения остаются открытыми.
В режиме горячего резерва запросы выполняются примерно так же, как и в обычном режиме, с
некоторыми отличиями в использовании и администрировании, описанными ниже.
26.5.1. Обзор на уровне пользователя
Когда параметр hot_standby на резервном сервере установлен в true, то он начинает принимать
соединения сразу как только система придёт в согласованное состояние в процессе восстановле-
ния. Для таких соединений будет разрешено только чтение, запись невозможна даже во времен-
ные таблицы.
Для того, чтобы данные с ведущего сервера были получены на резервном, требуется некоторое
время. Таким образом, имеется измеряемая задержка между ведущим и резервным серверами.
Поэтому запуск одинаковых запросов примерно в одно время на ведущем и резервном серверах
может вернуть разный результат. Можно сказать, что данные на резервном сервере в конечном
счёте согласуются с ведущим. После того как запись о зафиксированной транзакции воспроиз-
водится на резервном сервере, изменения, совершённые в этой транзакции, становится видны в
любых последующих снимках данных на резервном сервере. Снимок может быть сделан в начале
каждого запроса или в начале каждой транзакции в зависимости от уровня изоляции транзакции.
Более подробно см. Раздел 13.2.
Транзакции, запущенные в режиме горячего резерва, могут выполнять следующие команды:
662Отказоустойчивость, баланси-
ровка нагрузки и репликация
• Доступ к данным — SELECT, COPY TO
• Команды для работы с курсором — DECLARE, FETCH, CLOSE
• Параметры — SHOW, SET, RESET
• Команды явного управления транзакциями
• BEGIN, END, ABORT, START TRANSACTION
• SAVEPOINT, RELEASE, ROLLBACK TO SAVEPOINT
• Блок EXCEPTION и другие внутренние подчиненные транзакции
• LOCK TABLE, только когда исполняется в явном виде в следующем режиме: ACCESS SHARE, ROW
SHARE или ROW EXCLUSIVE.
• Планы и ресурсы — PREPARE, EXECUTE, DEALLOCATE, DISCARD
• Дополнения и расширения — LOAD
Транзакции, запущенные в режиме горячего резерва, никогда не получают ID транзакции и не
могут быть записаны в журнал предзаписи. Поэтому при попытке выполнить следующие действия
возникнут ошибки:
• Команды манипуляции данными (DML) — INSERT, UPDATE, DELETE, COPY FROM, TRUNCATE. Следу-
ет отметить, что нет разрешённых действий, которые приводили бы к срабатыванию тригге-
ра во время исполнения на резервном сервере. Это ограничение так же касается и временных
таблиц, так как строки таблицы не могут быть прочитаны или записаны без обращения к ID
транзакции, что в настоящее время не возможно в среде горячего резерва.
• Команды определения данных (DDL) — CREATE, DROP, ALTER, COMMENT. Эти ограничения так же
относятся и к временным таблицам, так как операции могут потребовать обновления таблиц
системных каталогов.
• SELECT … FOR SHARE | UPDATE, так как блокировка строки не может быть проведена без об-
новления соответствующих файлов данных.
• Правила для выражений SELECT, которые приводят к выполнению команд DML.
• LOCK которая явно требует режим более строгий чем ROW EXCLUSIVE MODE.
• LOCK в короткой форме с умолчаниями, так как требует ACCESS EXCLUSIVE MODE.
• Команды управления транзакциями, которые в явном виде требуют режим не только для чте-
ния
• BEGIN READ WRITE, START TRANSACTION READ WRITE
• SET TRANSACTION READ WRITE, SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE
• SET transaction_read_only = off
• Команды двухфазной фиксации — PREPARE TRANSACTION, COMMIT PREPARED, ROLLBACK PREPARED,
так как даже транзакции только для чтения нуждаются в записи в WAL на подготовительной
фазе (первая фаза двухфазной фиксации).
• Обновление последовательностей — nextval(), setval()
• LISTEN, UNLISTEN, NOTIFY
При обычной работе транзакции «только для чтения» могут использовать команды LISTEN,
UNLISTEN и NOTIFY; таким образом, сеансы горячего резерва работают с несколько большими огра-
ничениями, чем обычные только читающие сеансы. Возможно, что некоторые из этих ограниче-
ний будут ослаблены в следующих выпуска.
В режиме горячего резерва параметр transaction_read_only всегда имеет значение true и изме-
нить его нельзя. Но если не пытаться модифицировать содержимое БД, подключение к серверу в
663Отказоустойчивость, баланси-
ровка нагрузки и репликация
этом режиме не отличается от подключений к обычным базам данных. При отработке отказа или
переключении ролей база данных переходит в обычный режим работы. Когда сервер меняет ре-
жим работы, установленные сеансы остаются подключёнными. После выхода из режима горячего
резерва становится возможным запускать пишущие транзакции (даже в сеансах, начатых ещё в
режиме горячего резерва).
Пользователи могут узнать о нахождении сессии в режиме только для чтения с помощью команды
SHOW transaction_read_only. Кроме того, набор функций (Таблица 9.80) позволяет пользователям
получить доступ к информации о резервном сервере. Это позволяет создавать программы, учиты-
вающие текущий статус базы данных. Такой режим может быть полезен для мониторинга процес-
са восстановления или для написания комплексного восстановления для особенных случаев.
26.5.2. Обработка конфликтов запросов
Ведущий и резервный серверы связаны между собой многими слабыми связями. События на веду-
щем сервере оказывают влияние на резервный. В результате имеется потенциальная возможность
отрицательного влияния или конфликта между ними. Наиболее простой для понимания конфликт
— быстродействие: если на ведущем происходит загрузка очень большого объёма данных, то про-
исходит создание соответствующего потока записей WAL на резервный сервер. Таким образом,
запросы на резервном конкурируют за системные ресурсы, например, ввод-вывод.
Так же может возникнуть дополнительный тип конфликта на сервере горячего резерва. Этот кон-
фликт называется жёстким конфликтом, оказывает влияние на запросы, приводя к их отмене, а
в некоторых случаях и к обрыву сессии для разрешения конфликтов. Пользователям предоставлен
набор средств для обработки подобных конфликтов. Случаи конфликтов включают:
• Установка эксклюзивной блокировки на ведущем сервере, как с помощью явной команды
LOCK, так и при различных DDL, что приводит к конфликту доступа к таблицам на резервном.
• Удаление табличного пространства на ведущем сервере приводит к конфликту на резервном
когда запросы используют это пространство для хранения временных рабочих файлов.
• Удаление базы данных на ведущем сервере конфликтует с сессиями, подключёнными к этой
БД на резервном.
• Приложение очистки устаревших транзакций из WAL конфликтует с транзакциями на резерв-
ном сервере, которые используют снимок данных, который всё ещё видит какие-то из очищен-
ных на ведущем строк.
• Приложение очистки устаревших транзакций из WAL конфликтует с запросами к целевой
странице на резервном сервере вне зависимости от того, являются ли данные удалёнными
или видимыми.
В этих случаях на ведущем сервере просто происходит ожидание; пользователю следует выбрать
какую их конфликтующих сторон отменить. Тем не менее, на резервном нет выбора: действия из
WAL уже произошли на ведущем, поэтому резервный обязан применить их. Более того, позволять
обработчику WAL ожидать неограниченно долго может быть крайне нежелательно, так как отста-
вание резервного сервера от ведущего может всё возрастать. Таким образом, механизм обеспечи-
вает принудительную отмену запросов на резервном сервере, которые конфликтуют с применяе-
мыми записями WAL.
Примером такой проблемы может быть ситуация: администратор на ведущем сервере выполнил
команду DROP TABLE для таблицы, которая сейчас участвует в запросе на резервном. Понятно, что
этот запрос нельзя будет выполнять дальше, если команда DROP TABLE применится на резервном.
Если бы этот запрос выполнялся на ведущем, команда DROP TABLE ждала бы его окончания. Но ко-
гда на ведущем выполняется только команда DROP TABLE, ведущий сервер не знает, какие запросы
выполняются на резервном, поэтому он не может ждать завершения подобных запросов. Поэтому
если записи WAL с изменением прибудут на резервный сервер, когда запрос будет продолжать вы-
полняться, возникнет конфликт. В этом случае резервный сервер должен либо задержать приме-
нение этих записей WAL (и всех остальных, следующих за ними), либо отменить конфликтующий
запрос, чтобы можно было применить DROP TABLE.
664Отказоустойчивость, баланси-
ровка нагрузки и репликация
Если конфликтный запрос короткий, обычно желательно разрешить ему завершиться, нена-
долго задержав применение записей WAL, но слишком большая задержка в применении WAL
обычно нежелательна. Поэтому механизм отмены имеет параметры max_standby_archive_delay и
max_standby_streaming_delay, которые определяют максимально допустимое время задержки при-
менения WAL. Конфликтующие запросы будут отменены, если они длятся дольше допустимого
времени задержки применения очередных записей WAL. Два параметра существуют для того, что-
бы можно было задать разные значения для чтения записей WAL из архива (то есть при начальном
восстановлении из базовой копии либо при «навёрстывании» ведущего сервера в случае большого
отставания) и для получения записей WAL при потоковой репликации.
На резервном сервере, созданном преимущественно для отказоустойчивости, лучше выставлять
параметры задержек относительно небольшими, чтобы он не мог сильно отстать от ведущего из-
за задержек, связанных с ожиданием запросов горячего резерва. Однако если резервный сервер
предназначен для выполнения длительных запросов, то высокое значение или даже бесконечное
ожидание могут быть предпочтительнее. Тем не менее, следует иметь в виду, что длительные за-
просы могут оказать влияние на другие сессии на резервном сервере в виде отсутствия последних
изменений от ведущего из-за задержки применения записей WAL.
В
случае,
если
задержка,
определённая
max_standby_archive_delay
или
max_standby_streaming_delay будет превышена, конфликтующий запрос будет отменён. Обычно
это выражается в виде ошибки отмены, но в случае проигрывания команды DROP DATABASE обры-
вается вся конфликтная сессия. Так же, если конфликт произошел при блокировке, вызванной
транзакцией в состоянии IDLE, конфликтная сессия разрывается (это поведение может изменить
в будущем).
Отменённые запросы могут быть немедленно повторены (конечно после старта новой транзакции).
Так как причина отмены зависит от природы проигрываемых записей WAL, запрос, который был
отменён, может быть успешно выполнен вновь.
Следует учесть, что параметры задержки отсчитываются от времени получения резервным сер-
вером данных WAL. Таким образом, период дозволенной работы для запроса на резервном серве-
ре никогда не может быть длиннее параметра задержки и может быть существенно короче, если
резервный уже находится в режиме задержки в результате ожидания предыдущего запроса или
результат не доступен из-за высокой нагрузки обновлений.
Наиболее частой причиной конфликтов между запросами на резервном сервере и проигрыванием
WAL является преждевременная очистка. Обычно PostgreSQL допускает очистку старых версий
записей при условии что ни одна из транзакций их не видит согласно правилам видимости данных
для MVCC. Тем не менее, эти правила применяются только для транзакций, выполняемых на глав-
ном сервере. Таким образом, допустима ситуация, когда на главном запись уже очищена, но эта
же запись всё ещё видна для транзакций на резервном сервере.
Для опытных пользователей следует отметить, что как очистка старых версий строк, так и замо-
розка версии строки могут потенциально вызвать конфликт с запросами на резервном сервере.
Ручной запуск команды VACUUM FREEZE может привести к конфликту, даже в таблице без обнов-
ленных и удалённых строк.
Пользователи должны понимать, что регулярное и активное изменение данных в таблицах на ве-
дущем сервере чревато отменой длительных запросов на резервном. В таком случае установка
конечного значения для max_standby_archive_delay или max_standby_streaming_delay действует
подобно ограничению statement_timeout.
В случае, если количество отменённых запросов на резервном сервере получается неприемле-
мым, существует ряд дополнительных возможностей. Первая возможность — установить параметр
hot_standby_feedback, который не даёт команде VACUUM удалять записи, ставшие недействитель-
ными недавно, что предотвращает конфликты очистки. При этом следует учесть, что это вызывает
задержку очистки мёртвых строк на ведущем, что может привести к нежелательному распуханию
таблицы. Тем не менее, в итоге ситуация будет не хуже, чем если бы запросы к резервному серверу
исполнялись непосредственно на ведущем, но при этом сохранится положительный эффект от раз-
665Отказоустойчивость, баланси-
ровка нагрузки и репликация
деления нагрузки. В случае, когда соединение резервных серверов с ведущим часто разрывается,
следует скорректировать период, в течение которого обратная связь через hot_standby_feedback
не обеспечивается. Например, следует подумать об увеличении max_standby_archive_delay, что-
бы запросы отменялись не сразу при конфликтах с архивом WAL в период разъединения. Также
может иметь смысл увеличить max_standby_streaming_delay для предотвращения быстрой отме-
ны запросов из-за полученных записей WAL после восстановления соединения.
Другая возможность — увеличение vacuum_defer_cleanup_age на ведущем сервере таким об-
разом, чтобы мёртвые записи не очищались бы так быстро, как при обычном режиме рабо-
ты. Это даёт запросам на резервном сервере больше времени на выполнение, прежде чем они
могут быть отменены, без увеличения задержки max_standby_streaming_delay. Тем не менее
при таком подходе очень трудно обеспечить какое-то определённое окно по времени, так как
vacuum_defer_cleanup_age измеряется в количестве транзакций, выполняемых на ведущем серве-
ре.
Количество отменённых запросов и причины отмены можно просмотреть через системное
представление pg_stat_database_conflicts на резервном сервере. Системное представление
pg_stat_database так же содержит итоговую информацию.
26.5.3. Обзор административной части
Если в файле postgresql.conf для параметра hot_standby задано значение on (по умолчанию)
и существует файл recovery.conf, сервер запустится в режиме горячего резерва. Однако может
пройти некоторое время, прежде чем к нему можно будет подключиться, так как он не будет при-
нимать подключения, пока не произведёт восстановление до согласованного состояния, подходя-
щего для выполнения запросов. (Информация о согласованности состояния записывается на веду-
щем сервере в контрольной точке.) В течение этого периода клиенты при попытке подключения
будут получать сообщение об ошибке. Убедиться, что сервер включился в работу, можно либо по-
вторяя попытки подключения из приложения до успешного подключения, либо дождавшись по-
явления в журналах сервера этих сообщений:
LOG:
entering standby mode
… then some time later …
LOG:
LOG:
consistent recovery state reached
database system is ready to accept read only connections
Включить горячий резерв нельзя, если WAL был записан в период, когда на ведущем сервере па-
раметр wal_level имел значение не replica и не logical. Достижение согласованного состояния
также может быть отсрочено, если имеют место оба этих условия:
• Пишущая транзакция имеет более 64 подтранзакций
• Очень длительные пишущие транзакции
Если вы применяете файловую репликацию журналов («тёплый резерв»), возможно, придётся
ожидать прибытия следующего файла WAL (максимальное время ожидания задаётся параметром
archive_timeout на ведущем сервере).
Значения некоторых параметров на резервном сервере необходимо изменить при модификации
их на ведущем. Для таких параметров значения на резервном сервере должны быть не меньше
значений на ведущем. Таким образом, если вы хотите увеличить их, вы сначала должны сделать
это на резервных серверах, а затем применить изменения на ведущем. И наоборот, если вы хотите
их уменьшить, сначала сделайте это на ведущем сервере, а потом примените изменения на всех
резервных. Если параметры имеют недостаточно большие значения, резервный сервер не сможет
начать работу. В этом случае можно увеличить их и повторить попытку запуска сервера, чтобы он
возобновил восстановление. Это касается следующих параметров:
• max_connections
• max_prepared_transactions
666Отказоустойчивость, баланси-
ровка нагрузки и репликация
• max_locks_per_transaction
• max_worker_processes
Очень важно для администратора выбрать подходящие значения для max_standby_archive_delay
и max_standby_streaming_delay. Оптимальное значение зависит от приоритетов. Например, если
основное назначение сервера — обеспечение высокой степени доступности, то следует установить
короткий период, возможно даже нулевой, хотя это очень жёсткий вариант. Если резервный сер-
вер планируется как дополнительный сервер для аналитических запросов, то приемлемой будет
максимальная задержка в несколько часов или даже -1, что означает бесконечное ожидание окон-
чания запроса.
Вспомогательные биты статуса транзакций, записанные на ведущем, не попадают в WAL, так что
они, скорее всего, будут перезаписаны на нём при работе с данными. Таким образом, резервный
сервер будет производить запись на диск, даже если все пользователи только читают данные, ни-
чего не меняя. Кроме того, пользователи будут записывать временные файлы при сортировке боль-
ших объёмов и обновлять файлы кеша. Поэтому в режиме горячего резерва ни одна часть базы
данных фактически не работает в режиме «только чтение». Следует отметить, что также возможно
выполнить запись в удалённую базу данных с помощью модуля dblink и другие операции вне базы
данных с применением PL-функций, несмотря на то, что транзакции по-прежнему смогут только
читать данные.
Следующие типы административных команд недоступны в течение режима восстановления:
• Команды определения данных (DDL) — например: CREATE INDEX
• Команды выдачи привилегий и назначения владельца — GRANT, REVOKE, REASSIGN
• Команды обслуживания — ANALYZE, VACUUM, CLUSTER, REINDEX
Ещё раз следует отметить, что некоторые из этих команд фактически доступны на ведущем сер-
вере для транзакций в режиме только для чтения.
В результате нельзя создать дополнительные индексы или статистику, чтобы они существовали
только на резервном. Если подобные административные команды нужны, то их следует выполнить
на ведущем сервере, затем эти изменения будут распространены на резервные серверы.
Функции pg_cancel_backend() и pg_terminate_backend() работают на стороне пользовате-
ля, но не для процесса запуска, который обеспечивает восстановление. Представление
pg_stat_activity не показывает восстанавливаемые транзакции как активные. Поэтому представ-
ление pg_prepared_xacts всегда пусто в ходе восстановления. Если требуется разобрать сомни-
тельные подготовленные транзакции, следует обратиться к pg_prepared_xacts на ведущем и вы-
полнить команды для разбора транзакций там либо разобрать их по окончании восстановления.
pg_locks отображает блокировки, происходящие в процессе работы сервера как обычно. pg_locks
так же показывает виртуальные транзакции, обработанные процессом запуска, которому принад-
лежат все AccessExclusiveLocks, наложенные транзакциями в режиме восстановления. Следует
отметить, что процесс запуска не запрашивает блокировки, чтобы внести изменения в базу дан-
ных, поэтому блокировки, отличные от AccessExclusiveLocks не показываются в pg_locks для
процесса запуска, подразумевается их существование.
Модуль check_pgsql для Nagios будет работать, так как сервер выдаёт простую информацию, нали-
чие которой он проверяет. Скрипт мониторинга check_postgres так же работает, хотя для некото-
рых выдаваемых показателей результаты могут различаться или вводить в заблуждение. Напри-
мер, нельзя отследить время последней очистки, так как очистка не производится на резервном
сервере. Очистка запускается на ведущем сервере и результаты её работы передаются резервно-
му.
Команды управления файлами WAL, например pg_start_backup, pg_switch_wal и т. д. не будут
работать во время восстановления.
Динамически загружаемые модули работать будут, включая pg_stat_statements.
667Отказоустойчивость, баланси-
ровка нагрузки и репликация
Рекомендательная блокировка работает обычно при восстановлении, включая обнаружение вза-
имных блокировок. Следует отметить, что рекомендательная блокировка никогда не попадает в
WAL, таким образом для рекомендательной блокировки как на ведущем сервере, так и на резерв-
ном, невозможен конфликт с проигрыванием WAL. Но возможно получение рекомендательной бло-
кировки на ведущем сервере, а затем получение подобной рекомендательной блокировки на ре-
зервном. Рекомендательная блокировка относится только к серверу, на котором она получена.
Системы репликации на базе триггеров, подобные Slony, Londiste и Bucardo не могут запускаться
на резервном сервере вовсе, хотя они превосходно работают на ведущем до тех пор, пока не будет
подана команда не пересылать изменения на резервный. Проигрывание WAL не основано на триг-
герах, поэтому поток WAL нельзя транслировать с резервного сервера в другую систему, которая
требует дополнительной записи в БД или работает на основе триггеров.
Новые OID не могут быть выданы, хотя, например генераторы UUID смогут работать, если они не
пытаются записывать новое состояние в базу данных.
В настоящий момент создание временных таблиц недопустимо при транзакции только для чтения,
в некоторых случаях существующий скрипт будет работать неверно. Это ограничение может быть
ослаблено в следующих выпусках. Это одновременно требование SQL стандарта и техническое
требование.
Команда DROP TABLESPACE может быть выполнена только если табличное пространство пусто. Неко-
торые пользователи резервного сервера могут активно использовать табличное пространство че-
рез параметр temp_tablespaces. Если имеются временные файлы в табличных пространствах, все
активные запросы отменяются для обеспечения удаления временных файлов, затем табличное
пространство может быть удалено и продолжено проигрывание WAL.
Выполнение команды DROP DATABASE или ALTER DATABASE … SET TABLESPACE на ведущем сервере
приводит к созданию записи в WAL, которая вызывает принудительное отключение всех пользова-
телей, подключённых к этой базе данных на резервном. Это происходит немедленно, вне зависимо-
сти от значения max_standby_streaming_delay. Следует отметить, что команда ALTER DATABASE …
RENAME не приводит к отключению пользователей, так что обычно она действует незаметно, хотя
в некоторых случаях возможны сбои программ, которые зависят от имени базы данных.
Если вы в обычном режиме (не в режиме восстановления) выполните DROP USER или DROP ROLE
для роли с возможностью подключения, в момент, когда этот пользователь подключён, на данном
пользователе это никак не отразится — он останется подключённым. Однако переподключиться
он уже не сможет. Это же поведение действует в режиме восстановления — если выполнить DROP
USER на ведущем сервере, пользователь не будет отключён от резервного.
Сборщик статистики работает во время восстановления. Все операции сканирования, чтения, бло-
ки, использование индексов и т. п. будут записаны обычным образом на резервном сервере. Дей-
ствия, происходящие при проигрывании, не будут дублировать действия на ведущем сервере,
то есть проигрывание команды вставки не увеличит значение столбца Inserts в представлении
pg_stat_user_tables. Файлы статистики удаляются с началом восстановления, таким образом, ста-
тистика на ведущем сервере и резервном будет разной. Это является особенностью, не ошибкой.
Автоматическая очистка не работает во время восстановления. Она запустится в обычном режиме
после завершения восстановления.
Во время восстановления активен фоновый процесс записи, он обрабатывает точки перезапуска
(подобно контрольным точкам на ведущем сервере) и выполняет обычную очистку блоков. В том
числе он может обновлять вспомогательные биты, сохранённые на резервном. Во время восста-
новления принимается команда CHECKPOINT, но она производит точку перезапуска, а не создаёт
новую точку восстановления.
26.5.4. Ссылки на параметры горячего резерва
Различные параметры были упомянуты выше в Подразделе 26.5.2 и Подразделе 26.5.3.
668Отказоустойчивость, баланси-
ровка нагрузки и репликация
На ведущем могут применяться параметры wal_level и vacuum_defer_cleanup_age. Параметры
max_standby_archive_delay и max_standby_streaming_delay на ведущем не действуют.
На резервном сервере могут применяться параметры hot_standby, max_standby_archive_delay и
max_standby_streaming_delay. Параметр vacuum_defer_cleanup_age на нём не действует, пока сер-
вер остаётся в режиме резервного сервера. Но если он станет ведущим, его значение вступит в
силу.
26.5.5. Ограничения
Имеются следующие ограничения горячего резерва. Они могут и скорее всего будут исправлены
в следующих выпусках:
• Требуется информация о всех запущенных транзакциях перед тем как будет создан снимок
данных. Транзакции, использующие большое количество подтранзакций (в настоящий момент
больше 64), будут задерживать начало соединения только для чтения до завершения самой
длинной пишущей транзакции. При возникновении этой ситуации поясняющее сообщение бу-
дет записано в журнал сервера.
• Подходящие стартовые точки для запросов на резервном сервере создаются при каждой кон-
трольной точке на главном. Если резервный сервер отключается, в то время как главный был
в отключённом состоянии, может оказаться невозможным возобновить его работу в режиме
горячего резерва, до того, как запустится ведущий и добавит следующие стартовые точки в
журналы WAL. Подобная ситуация не является проблемой для большинства случаев, в кото-
рых она может произойти. Обычно, если ведущий сервер выключен и больше не доступен, это
является следствием серьёзного сбоя и в любом случае требует преобразования резервного в
новый ведущий. Так же в ситуации, когда ведущий отключён намеренно, проверка готовности
резервного к преобразованию в ведущий тоже является обычной процедурой.
• В конце восстановления блокировки AccessExclusiveLocks, вызванные подготовленными
транзакциями, требуют удвоенное, в сравнении с нормальным, количество блокировок за-
писей таблицы. Если планируется использовать либо большое количество конкурирующих
подготовленных транзакций, обычно вызывающие AccessExclusiveLocks, либо большие
транзакции с применением большого количества AccessExclusiveLocks, то рекомендует-
ся выбрать большое значение параметра max_locks_per_transaction, возможно в два ра-
за большее, чем значение параметра на ведущем сервере. Всё это не имеет значения, когда
max_prepared_transactions равно 0.
• Уровень изоляции транзакции Serializable в настоящее время недоступен в горячем резерве.
(За подробностями обратитесь к Подразделу 13.2.3 и Подразделу 13.4.1) Попытка выставить
для транзакции такой уровень изоляции в режиме горячего резерва вызовет ошибку.
669</li>
</ol>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/PostgreSQL-V11_Doc-025/" title="Глава 25. Резервное копирование и восстановление"><img src="http://localhost:4000/images/abstract-11.jpg" alt="Глава 25. Резервное копирование и восстановление"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-12-01T00:00:00+02:00"><a href="http://localhost:4000/PostgreSQL-V11_Doc-025/">December 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Sergey Khatsiola">Sergey Khatsiola</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~41 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/PostgreSQL-V11_Doc-025/" rel="bookmark" title="Глава 25. Резервное копирование и восстановление" itemprop="url">Глава 25. Резервное копирование и восстановление</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Глава 25. Резервное копирование и восстановление</p>

<p>Как и всё, что содержит важные данные, базы данных PostgreSQL следует регулярно сохранять
в резервной копии. Хотя эта процедура по существу проста, важно чётко понимать лежащие в её
основе приёмы и положения.
Существует три фундаментально разных подхода к резервному копированию данных в PostgreSQL:
• Выгрузка в SQL
• Копирование на уровне файлов
• Непрерывное архивирование
Каждый из них имеет свои сильные и слабые стороны; все они обсуждаются в следующих разделах.
25.1. Выгрузка в SQL
Идея, стоящая за этим методом, заключается в генерации текстового файла с командами SQL,
которые при выполнении на сервере пересоздадут базу данных в том же самом состоянии, в ко-
тором она была на момент выгрузки. PostgreSQL предоставляет для этой цели вспомогательную
программу pg_dump. Простейшее применение этой программы выглядит так:
pg_dump имя_базы &gt; файл_дампа
Как видите, pg_dump записывает результаты своей работы в устройство стандартного вывода. Да-
лее будет рассмотрено, чем это может быть полезно. В то время как вышеупомянутая команда
создаёт текстовый файл, pg_dump может создать файлы и в других форматах, которые допускают
параллельную обработку и более гибкое управление восстановлением объектов.
Программа pg_dump является для PostgreSQL обычным клиентским приложением (хотя и весьма
умным). Это означает, что вы можете выполнять процедуру резервного копирования с любого уда-
лённого компьютера, если имеете доступ к нужной базе данных. Но помните, что pg_dump не ис-
пользует для своей работы какие-то специальные привилегии. В частности, ей обычно требуется
доступ на чтение всех таблиц, которые вы хотите выгрузить, так что для копирования всей базы
данных практически всегда её нужно запускать с правами суперпользователя СУБД. (Если у вас
нет достаточных прав для резервного копирования всей базы данных, вы, тем не менее, можете
сделать резервную копию той части базы, доступ к которой у вас есть, используя такие параметры,
как -n схема или -t таблица.)
Указать, к какому серверу должна подключаться программа pg_dump, можно с помощью аргумен-
тов командной строки -h сервер и -p порт. По умолчанию в качестве сервера выбирается localhost
или значение, указанное в переменной окружения PGHOST. Подобным образом, по умолчанию ис-
пользуется порт, заданный в переменной окружения PGPORT, а если она не задана, то порт, ука-
занный по умолчанию при компиляции. (Для удобства при компиляции сервера обычно устанав-
ливается то же значение по умолчанию.)
Как и любое другое клиентское приложение PostgreSQL, pg_dump по умолчанию будет подклю-
чаться к базе данных с именем пользователя, совпадающим с именем текущего пользователя опе-
рационной системы. Чтобы переопределить имя, либо добавьте параметр -U, либо установите пе-
ременную окружения PGUSER. Помните, что pg_dump подключается к серверу через обычные ме-
ханизмы проверки подлинности клиента (которые описываются в Главе 20).
Важное преимущество pg_dump в сравнении с другими методами резервного копирования, опи-
санными далее, состоит в том, что вывод pg_dump обычно можно загрузить в более новые версии
PostgreSQL, в то время как резервная копия на уровне файловой системы и непрерывное архиви-
рование жёстко зависят от версии сервера. Также, только метод с применением pg_dump будет
работать при переносе базы данных на другую машинную архитектуру, например, при переносе
с 32-битной на 64-битную версию сервера.
629Резервное копирова-
ние и восстановление
Дампы, создаваемые pg_dump, являются внутренне согласованными, то есть, дамп представляет
собой снимок базы данных на момент начала запуска pg_dump. pg_dump не блокирует другие опе-
рации с базой данных во время своей работы. (Исключение составляют операции, которым нужна
исключительная блокировка, как например, большинство форм команды ALTER TABLE.)
25.1.1. Восстановление дампа
Текстовые файлы, созданные pg_dump, предназначаются для последующего чтения программой
psql. Общий вид команды для восстановления дампа:
psql имя_базы &lt; файл_дампа
где файл_дампа — это файл, содержащий вывод команды pg_dump. База данных, заданная пара-
метром имя_базы, не будет создана данной командой, так что вы должны создать её сами из базы
template0 перед запуском psql (например, с помощью команды createdb -T template0 имя_ба-
зы). Программа psql принимает параметры, указывающие сервер, к которому осуществляется под-
ключение, и имя пользователя, подобно pg_dump. За дополнительными сведениями обратитесь
к справке по psql. Дампы, выгруженные не в текстовом формате, восстанавливаются утилитой
pg_restore.
Перед восстановлением SQL-дампа все пользователи, которые владели объектами или имели пра-
ва на объекты в выгруженной базе данных, должны уже существовать. Если их нет, при восстанов-
лении будут ошибки пересоздания объектов с изначальными владельцами и/или правами. (Иногда
это желаемый результат, но обычно нет).
По умолчанию, если происходит ошибка SQL, программа psql продолжает выполнение. Если же
запустить psql с установленной переменной ON_ERROR_STOP, это поведение поменяется и psql за-
вершится с кодом 3 в случае возникновения ошибки SQL:
psql –set ON_ERROR_STOP=on имя_базы &lt; файл_дампа
В любом случае, вы получите только частично восстановленную базу данных. В качестве альтер-
нативы можно указать, что весь дамп должен быть восстановлен в одной транзакции, так что вос-
становление либо полностью выполнится, либо полностью отменится. Включить данный режим
можно, передав psql аргумент -1 или –single-transaction. Выбирая этот режим, учтите, что да-
же незначительная ошибка может привести к откату восстановления, которое могло продолжать-
ся несколько часов. Однако, это всё же может быть предпочтительней, чем вручную вычищать
сложную базу данных после частично восстановленного дампа.
Благодаря способности pg_dump и psql писать и читать каналы ввода/вывода, можно скопировать
базу данных непосредственно с одного сервера на другой, например:
pg_dump -h host1 имя_базы | psql -h host2 имя_базы
Важно
Дампы, которые выдаёт pg_dump, содержат определения относительно template0. Это
означает, что любые языки, процедуры и т. п., добавленные в базу через template1,
pg_dump также выгрузит в дамп. Как следствие, если при восстановлении вы ис-
пользуете модифицированный template1, вы должны создать пустую базу данных из
template0, как показано в примере выше.
После восстановления резервной копии имеет смысл запустить ANALYZE для каждой базы дан-
ных, чтобы оптимизатор запросов получил полезную статистику; за подробностями обратитесь к
Подразделу 24.1.3 и Подразделу 24.1.6. Другие советы по эффективной загрузке больших объёмов
данных в PostgreSQL вы можете найти в Разделе 14.4.
25.1.2. Использование pg_dumpall
Программа pg_dump выгружает только одну базу данных в один момент времени и не включает
в дамп информацию о ролях и табличных пространствах (так как это информация уровня класте-
630Резервное копирова-
ние и восстановление
ра, а не самой базы данных). Для удобства создания дампа всего содержимого кластера баз дан-
ных предоставляется программа pg_dumpall, которая делает резервную копию всех баз данных
кластера, а также сохраняет данные уровня кластера, такие как роли и определения табличных
пространств. Простое использование этой команды:
pg_dumpall &gt; файл_дампа
Полученную копию можно восстановить с помощью psql:
psql -f файл_дампа postgres
(В принципе, здесь в качестве начальной базы данных можно указать имя любой существующей
базы, но если вы загружаете дамп в пустой кластер, обычно нужно использовать postgres). Вос-
станавливать дамп, который выдала pg_dumpall, всегда необходимо с правами суперпользователя,
так как они требуются для восстановления информации о ролях и табличных пространствах. Если
вы используете табличные пространства, убедитесь, что пути к табличным пространствам в дампе
соответствуют новой среде.
pg_dumpall выдаёт команды, которые заново создают роли, табличные пространства и пустые базы
данных, а затем вызывает для каждой базы pg_dump. Таким образом, хотя каждая база данных
будет внутренне согласованной, состояние разных баз не будет синхронным.
Только глобальные данные кластера можно выгрузить, передав pg_dumpall ключ –globals-only.
Это необходимо, чтобы полностью скопировать кластер, когда pg_dump выполняется для отдель-
ных баз данных.
25.1.3. Управление большими базами данных
Некоторые операционные системы накладывают ограничение на максимальный размер файла, что
приводит к проблемам при создании больших файлов с помощью pg_dump. К счастью, pg_dump
может писать в стандартный вывод, так что вы можете использовать стандартные инструменты
Unix для того, чтобы избежать потенциальных проблем. Вот несколько возможных методов:
Используйте сжатые дампы.
например gzip:
Вы можете использовать предпочитаемую программу сжатия,
pg_dump имя_базы | gzip &gt; имя_файла.gz
Затем загрузить сжатый дамп можно командой:
gunzip -c имя_файла.gz | psql имя_базы
или:
cat имя_файла.gz | gunzip | psql имя_базы
Используйте split.  Команда split может разбивать выводимые данные на небольшие файлы,
размер которых удовлетворяет ограничению нижележащей файловой системы. Например, чтобы
получить части по 1 мегабайту:
pg_dump имя_базы | split -b 1m - имя_файла
Восстановить их можно так:
cat имя_файла* | psql имя_базы
Используйте специальный формат дампа pg_dump.  Если при сборке PostgreSQL была под-
ключена библиотека zlib, дамп в специальном формате будет записываться в файл в сжатом виде.
В таком формате размер файла дампа будет близок к размеру, полученному с применением gzip,
но он лучше тем, что позволяет восстанавливать таблицы выборочно. Следующая команда выгру-
жает базу данных в специальном формате:
pg_dump -Fc имя_базы &gt; имя_файла
Дамп в специальном формате не является скриптом для psql и должен восстанавливаться с помо-
щью команды pg_restore, например:
631Резервное копирова-
ние и восстановление
pg_restore -d имя_базы имя_файла
За подробностями обратитесь к справке по командам pg_dump и pg_restore.
Для очень больших баз данных может понадобиться сочетать split с одним из двух других методов.
Используйте возможность параллельной выгрузки в pg_dump.  Чтобы ускорить выгрузку
большой БД, вы можете использовать режим параллельной выгрузки в pg_dump. При этом одно-
временно будут выгружаться несколько таблиц. Управлять числом параллельных заданий позво-
ляет параметр -j. Параллельная выгрузка поддерживается только для формата архива в каталоге.
pg_dump -j число -F d -f выходной_каталог имя_базы
Вы также можете восстановить копию в параллельном режиме с помощью pg_restore -j. Это под-
держивается для любого архива в формате каталога или специальном формате, даже если архив
создавался не командой pg_dump -j.
25.2. Резервное копирование на уровне файлов
Альтернативной стратегией резервного копирования является непосредственное копирование
файлов, в которых PostgreSQL хранит содержимое базы данных; в Разделе 18.2 рассказывается,
где находятся эти файлы. Вы можете использовать любой способ копирования файлов по желанию,
например:
tar -cf backup.tar /usr/local/pgsql/data
Однако, существуют два ограничения, которые делают этот метод непрактичным или как минимум
менее предпочтительным по сравнению с pg_dump:</p>
<ol>
  <li>Чтобы полученная резервная копия была годной, сервер баз данных должен быть остановлен.
Такие полумеры, как запрещение всех подключений к серверу, работать не будут (отчасти пото-
му что tar и подобные средства не получают мгновенный снимок состояния файловой системы,
но ещё и потому, что в сервере есть внутренние буферы). Узнать о том, как остановить сервер,
можно в Разделе 18.5. Необходимо отметить, что сервер нужно будет остановить и перед вос-
становлением данных.</li>
  <li>Если вы ознакомились с внутренней организацией базы данных в файловой системе, у вас может
возникнуть соблазн скопировать или восстановить только отдельные таблицы или базы данных
в соответствующих файлах или каталогах. Это не будет работать, потому что информацию, со-
держащуюся в этих файлах, нельзя использовать без файлов журналов транзакций, pg_xact/*,
которые содержат состояние всех транзакций. Без этих данных файлы таблиц непригодны к ис-
пользованию. Разумеется также невозможно восстановить только одну таблицу и соответствую-
щие данные pg_xact, потому что в результате нерабочими станут все другие таблицы в кластере
баз данных. Таким образом, копирование на уровне файловой системы будет работать, только
если выполняется полное копирование и восстановление всего кластера баз данных.
Ещё один подход к резервному копированию файловой системы заключается в создании «целост-
ного снимка» каталога с данными, если это поддерживает файловая система (и вы склонны счи-
тать, что эта функциональность реализована корректно). Типичная процедура включает создание
«замороженного снимка» тома, содержащего базу данных, затем копирование всего каталога с
данными (а не его избранных частей, см. выше) из этого снимка на устройство резервного копи-
рования, и наконец освобождение замороженного снимка. При этом сервер базы данных может
не прекращать свою работу. Однако резервная копия, созданная таким способом, содержит фай-
лы базы данных в таком состоянии, как если бы сервер баз данных не был остановлен штатным
образом; таким образом, когда вы запустите сервер баз данных с сохранёнными данными, он будет
считать, что до этого процесс сервера был прерван аварийно, и будет накатывать журнал WAL. Это
не проблема, просто имейте это в виду (и обязательно включите файлы WAL в резервную копию).
Чтобы сократить время восстановления, можно выполнить команду CHECKPOINT перед созданием
снимка.
Если ваша база данных размещена в нескольких файловых системах, получить в точности одновре-
менно замороженные снимки всех томов может быть невозможно. Например, если файлы данных
632Резервное копирова-
ние и восстановление
и журналы WAL находятся на разных дисках или табличные пространства расположены в разных
файловых системах, резервное копирование со снимками может быть неприменимо, потому что
снимки должны быть одновременными. В таких ситуациях очень внимательно изучите докумен-
тацию по вашей файловой системе, прежде чем довериться технологии согласованных снимков.
Если одновременные снимки невозможны, остаётся вариант с остановкой сервера баз данных на
время, достаточное для получения всех замороженных снимков. Другое возможное решение — по-
лучить базовую копию путём непрерывного архивирования (см. Подраздел 25.3.2), такие резерв-
ные копии не могут пострадать от изменений файловой системы в процессе резервного копирова-
ния. Для этого требуется включить непрерывное архивирование только на время резервного ко-
пирования; для восстановления применяется процедура восстановления из непрерывного архива
(Подраздел 25.3.4).
Ещё один вариант — копировать содержимое файловой системы с помощью rsync. Для этого rsync
запускается сначала во время работы сервера баз данных, а затем сервер останавливается на вре-
мя, достаточное для запуска rsync –checksum. (Ключ –checksum необходим, потому что rsync
различает время только с точностью до секунд.) Во второй раз rsync отработает быстрее, чем в
первый, потому что скопировать надо будет относительно немного данных; и в итоге будет полу-
чен согласованный результат, так как сервер был остановлен. Данный метод позволяет получить
копию на уровне файловой системы с минимальным временем простоя.
Обратите внимание, что размер копии на уровне файлов обычно больше, чем дампа SQL. (Про-
грамме pg_dump не нужно, например, записывать содержимое индексов, достаточно команд для
их пересоздания). Однако копирование на уровне файлов может выполняться быстрее.
25.3. Непрерывное архивирование и восстановление
на момент времени (Point-in-Time Recovery, PITR)
Всё время в процессе работы PostgreSQL ведёт журнал предзаписи (WAL), который расположен в
подкаталоге pg_wal/ каталога с данными кластера баз данных. В этот журнал записываются все
изменения, вносимые в файлы данных. Прежде всего, журнал существует для безопасного восста-
новления после краха сервера: если происходит крах, целостность СУБД может быть восстановле-
на в результате «воспроизведения» записей, зафиксированных после последней контрольной точ-
ки. Однако наличие журнала делает возможным использование третьей стратегии копирования
баз данных: можно сочетать резервное копирование на уровне файловой системы с копированием
файлов WAL. Если потребуется восстановить данные, мы можем восстановить копию файлов, а за-
тем воспроизвести журнал из скопированных файлов WAL, и таким образом привести систему в
нужное состояние. Такой подход более сложен для администрирования, чем любой из описанных
выше, но он имеет значительные преимущества:
• В качестве начальной точки для восстановления необязательно иметь полностью согласован-
ную копию на уровне файлов. Внутренняя несогласованность копии будет исправлена при
воспроизведении журнала (практически то же самое происходит при восстановлении после
краха). Таким образом, согласованный снимок файловой системы не требуется, вполне можно
использовать tar или похожие средства архивации.
• Поскольку при воспроизведении можно обрабатывать неограниченную последовательность
файлов WAL, непрерывную резервную копию можно получить, просто продолжая архивиро-
вать файлы WAL. Это особенно ценно для больших баз данных, полные резервные копии кото-
рых делать как минимум неудобно.
• Воспроизводить все записи WAL до самого конца нет необходимости. Воспроизведение мож-
но остановить в любой точке и получить целостный снимок базы данных на этот момент вре-
мени. Таким образом, данная технология поддерживает восстановление на момент времени:
можно восстановить состояние базы данных на любое время с момента создания резервной
копии.
• Если непрерывно передавать последовательность файлов WAL другому серверу, получивше-
му данные из базовой копии того же кластера, получается система тёплого резерва: в любой
633Резервное копирова-
ние и восстановление
момент мы можем запустить второй сервер и он будет иметь практически текущую копию баз
данных.
Примечание
Программы pg_dump и pg_dumpall не создают копии на уровне файловой системы и не
могут применяться как часть решения по непрерывной архивации. Создаваемые ими
копии являются логическими и не содержат информации, необходимой для воспроиз-
ведения WAL.
Как и обычное резервное копирование файловой системы, этот метод позволяет восстанавливать
только весь кластер баз данных целиком, но не его части. Кроме того, для архивов требуется боль-
шое хранилище: базовая резервная копия может быть объёмной, а нагруженные системы будут
генерировать многие мегабайты трафика WAL, который необходимо архивировать. Тем не менее,
этот метод резервного копирования предпочитается во многих ситуациях, где необходима высокая
надёжность.
Для успешного восстановления с применением непрерывного архивирования (также называемого
«оперативным резервным копированием» многими разработчиками СУБД), вам необходима непре-
рывная последовательность заархивированных файлов WAL, начинающаяся не позже, чем с мо-
мента начала копирования. Так что для начала вы должны настроить и протестировать процедуру
архивирования файлов WAL до того, как получите первую базовую копию. Соответственно, сна-
чала мы обсудим механику архивирования файлов WAL.
25.3.1. Настройка архивирования WAL
В абстрактном смысле, запущенная СУБД PostgreSQL производит неограниченно длинную после-
довательность записей WAL. СУБД физически делит эту последовательность на файлы сегментов
WAL, которые обычно имеют размер 16 МиБ (хотя размер сегмента может быть изменён при initdb).
Файлы сегментов получают цифровые имена, которые отражают их позицию в абстрактной после-
довательности WAL. Когда архивирование WAL не применяется, система обычно создаёт только
несколько файлов сегментов и затем «перерабатывает» их, меняя номер в имени ставшего ненуж-
ным файла на больший. Предполагается, что файлы сегментов, содержимое которых предшеству-
ет последней контрольной точке, уже не представляют интереса и могут быть переработаны.
При архивировании данных WAL необходимо считывать содержимое каждого файла-сегмента, как
только он заполняется, и сохранять эти данные куда-то, прежде чем файл-сегмент будет перера-
ботан и использован повторно. В зависимости от применения и доступного аппаратного обеспече-
ния, возможны разные способы «сохранить данные куда-то»: можно скопировать файлы-сегменты
в смонтированный по NFS каталог на другую машину, записать их на ленту (убедившись, что у вас
есть способ идентифицировать исходное имя каждого файла) или собрать их в пакет и записать
на CD, либо какие-то совсем другие варианты. Чтобы у администратора баз данных была гибкость
в этом плане, PostgreSQL пытается не делать каких-либо предположений о том, как будет выпол-
няться архивация. Вместо этого, PostgreSQL позволяет администратору указать команду оболоч-
ки, которая будет запускаться для копирования завершённого файла-сегмента в нужное место.
Эта команда может быть простой как cp, а может вызывать сложный скрипт оболочки — это ре-
шать вам.
Чтобы включить архивирование WAL, установите в параметре конфигурации wal_level уровень
replica (или выше), в archive_mode — значение on, и задайте желаемую команду оболочки в па-
раметре archive_command. На практике эти параметры всегда задаются в файле postgresql.conf.
В archive_command символы %p заменяются полным путём к файлу, подлежащему архивации, а %f
заменяются только именем файла. (Путь задаётся относительно текущего рабочего каталога, т. е.
каталога данных кластера). Если в команду нужно включить сам символ %, запишите %%. Простей-
шая команда, которая может быть полезна:
archive_command = ‘test ! -f /mnt/server/archivedir/%f &amp;&amp; cp %p /mnt/server/archivedir/
%f’ # Unix
archive_command = ‘copy “%p” “C:\server\archivedir\%f”’ # Windows
634Резервное копирова-
ние и восстановление
Она будет копировать архивируемые сегменты WAL в каталог /mnt/server/archivedir. (Команда дана
как пример, а не как рекомендация, и может не работать на всех платформах.) После замены
параметров %p и %f фактически запускаемая команда может выглядеть так:
test ! -f /mnt/server/archivedir/00000001000000A900000065 &amp;&amp; cp
pg_wal/00000001000000A900000065 /mnt/server/archivedir/00000001000000A900000065
Подобная команда будет генерироваться для каждого следующего архивируемого файла.
Команда архивирования будет запущена от имени того же пользователя, от имени которого рабо-
тает сервер PostgreSQL. Поскольку архивируемые последовательности файлов WAL фактически
содержат всё, что есть в вашей базе данных, вам нужно будет защитить архивируемые данные от
посторонних глаз; например, сохраните архив в каталог, чтение которого запрещено для группы
и остальных пользователей.
Важно, чтобы команда архивирования возвращала нулевой код завершения, если и только ес-
ли она завершилась успешно. Получив нулевой результат, PostgreSQL будет полагать, что файл
успешно заархивирован и удалит его или переработает. Однако, ненулевой код состояния скажет
PostgreSQL, что файл не заархивирован; попытки заархивировать его будут периодически повто-
ряться, пока это не удастся.
Команда архивирования обычно разрабатывается так, чтобы не допускать перезаписи любых су-
ществующих архивных файлов. Это важная мера безопасности, позволяющая сохранить целост-
ность архива в случае ошибки администратора (например, если архивируемые данные двух разных
серверов будут сохраняться в одном каталоге).
Рекомендуется протестировать команду архивирования, чтобы убедиться, что она действительно
не перезаписывает существующие файлы, и что она возвращает ненулевое состояние в этом
случае. В показанной выше команде для Unix для этого добавлен отдельный шаг test. На некото-
рых платформах Unix у cp есть ключ -i, который позволяет сделать то же, но менее явно; но не
проверив, какой код состояния при этом возвращается, полагаться на этот ключ не следует. (В
частности, GNU cp возвратит нулевой код состояния, если используется ключ -i и целевой файл
существует, а это не то, что нужно.)
Разрабатывая схему архивирования, подумайте, что произойдёт, если команда архивирования нач-
нёт постоянно выдавать ошибку, потому что требуется вмешательство оператора или для архиви-
рования не хватает места. Например, это может произойти, если вы записываете архивы на лен-
точное устройство без механизма автозамены; когда лента заполняется полностью, больше ниче-
го архивироваться не будет, пока вы не замените кассету. Вы должны убедиться, что любые воз-
никающие ошибки или обращения к человеку (оператору), обрабатываются так, чтобы проблема
решалась достаточно быстро. Пока она не разрешится, каталог pg_wal/ продолжит наполняться
файлами-сегментами WAL. (Если файловая система, в которой находится каталог pg_wal/ запол-
нится до конца, PostgreSQL завершит свою работу аварийно. Зафиксированные транзакции не по-
теряются, но база данных не будет работать, пока вы не освободите место.)
Не важно, с какой скоростью работает команда архивирования, если только она не ниже средней
скорости, с которой сервер генерирует записи WAL. Обычно работа продолжается, даже если про-
цесс архивирования немного отстаёт. Если же архивирование отстаёт значительно, это приводит
к увеличению объёма данных, которые могут быть потеряны в случае аварии. При этом каталог
pg_wal/ будет содержать большое количество ещё не заархивированных файлов-сегментов, кото-
рые в конце концов могут занять всё доступное дисковое пространство. Поэтому рекомендуется
контролировать процесс архивации и следить за тем, чтобы он выполнялся как задумано.
При написании команды архивирования вы должны иметь в виду, что имена файлов для архиви-
рования могут иметь длину до 64 символов и содержать любые комбинации из цифр, точек и букв
ASCII. Сохранять исходный относительный путь (%p) необязательно, но необходимо сохранять имя
файла (%f).
Обратите внимание, что хотя архивирование WAL позволяет сохранить любые изменения данных,
произведённые в базе данных PostgreSQL, оно не затрагивает изменения, внесённые в конфигура-
635Резервное копирова-
ние и восстановление
ционные файлы (такие как postgresql.conf, pg_hba.conf и pg_ident.conf), поскольку эти изме-
нения выполняются вручную, а не через SQL. Поэтому имеет смысл разместить конфигурацион-
ные файлы там, где они будут заархивированы обычными процедурами копирования файлов. Как
перемещать конфигурационные файлы, рассказывается в Разделе 19.2.
Команда архивирования вызывается, только когда сегмент WAL заполнен до конца. Таким обра-
зом, если сервер постоянно генерирует небольшой трафик WAL (или есть продолжительные пери-
оды, когда это происходит), между завершением транзакций и их безопасным сохранением в ар-
хиве может образоваться большая задержка. Чтобы ограничить время жизни неархивированных
данных, можно установить archive_timeout, чтобы сервер переключался на новый файл сегмента
WAL как минимум с заданной частотой. Заметьте, что неполные файлы, архивируемые досрочно
из-за принудительного переключения по тайм-ауту, будут иметь тот же размер, что и заполнен-
ные файлы. Таким образом, устанавливать очень маленький archive_timeout — неразумно; это
приведёт к неэффективному заполнению архива. Обычно подходящее значение archive_timeout
— минута или около того.
Также вы можете принудительно переключить сегмент WAL вручную с помощью pg_switch_wal,
если хотите, чтобы только что завершённая транзакция заархивировалась как можно скорее. Дру-
гие полезные функции, относящиеся к управлению WAL, перечисляются в Таблице 9.79.
Когда wal_level имеет значение minimal, некоторые команды SQL выполняются в обход журна-
ла WAL, как описывается в Подразделе  14.4.7. Если архивирование или потоковая репликация
были включены во время выполнения таких операторов, WAL не будет содержать информацию,
необходимую для восстановления. (На восстановление после краха это не распространяется). По-
этому wal_level можно изменить только при запуске сервера. Однако, чтобы изменить команду
archive_command, достаточно перезагрузить файл конфигурации. Если вы хотите на время остано-
вить архивирование, это можно сделать, например, задав в качестве значения archive_command
пустую строку (‘’). В результате файлы WAL будут накапливаться в каталоге pg_wal/, пока не будет
восстановлена действующая команда archive_command.
25.3.2. Создание базовой резервной копии
Проще всего получить базовую резервную копию, используя программу pg_basebackup. Эта про-
грамма сохраняет базовую копию в виде обычных файлов или в архиве tar. Если гибкости
pg_basebackup не хватает, вы также можете получить базовую резервную копию, используя низ-
коуровневый API (см. Подраздел 25.3.3).
Продолжительность создания резервной копии обычно не имеет большого значения. Однако, ес-
ли вы эксплуатируете сервер с отключённым режимом full_page_writes, вы можете заметить па-
дение производительности в процессе резервного копирования, так как режим full_page_writes
включается принудительно на время резервного копирования.
Чтобы резервной копией можно было пользоваться, нужно сохранить все файлы сегментов WAL,
сгенерированные во время и после копирования файлов. Для облегчения этой задачи, процесс
создания базовой резервной копии записывает файл истории резервного копирования, который
немедленно сохраняется в области архивации WAL. Данный файл получает имя по имени файла
первого сегмента WAL, который потребуется для восстановления скопированных файлов. Напри-
мер, если начальный файл WAL назывался 0000000100001234000055CD, файл истории резервного
копирования получит имя 0000000100001234000055CD.007C9330.backup. (Вторая часть имени фай-
ла обозначает точную позицию внутри файла WAL и обычно может быть проигнорирована.) Как
только вы заархивировали копии файлов данных и файлов сегментов WAL, полученных в процес-
се копирования (по сведениям в файле истории резервного копирования), все заархивированные
сегменты WAL с именами, меньшими по номеру, становятся ненужными для восстановления фай-
ловой копии и могут быть удалены. Но всё же рассмотрите возможность хранения нескольких на-
боров резервных копий, чтобы быть абсолютно уверенными, что вы сможете восстановить ваши
данные.
Файл истории резервного копирования — это просто небольшой текстовый файл. В него записы-
вается метка, которая была передана pg_basebackup, а также время и текущие сегменты WAL в
636Резервное копирова-
ние и восстановление
момент начала и завершения резервной копии. Если вы связали с данной меткой соответствующий
файл дампа, то заархивированного файла истории достаточно, чтобы найти файл дампа, нужный
для восстановления.
Поскольку необходимо хранить все заархивированные файлы WAL с момента последней базовой
резервной копии, интервал базового резервного копирования обычно выбирается в зависимости
от того, сколько места может быть выделено для архива файлов WAL. Также стоит отталкиваться
от того, сколько вы готовы ожидать восстановления, если оно понадобится — системе придётся
воспроизвести все эти сегменты WAL, а этот процесс может быть долгим, если с момента последней
базовой копии прошло много времени.
25.3.3. Создание базовой резервной копии через низкоуровневый
API
Процедура создания базовой резервной копии с использованием низкоуровневого API содержит
чуть больше шагов, чем метод pg_basebackup, но всё же относительно проста. Очень важно, чтобы
эти шаги выполнялись по порядку, и следующий шаг выполнялся, только если предыдущий успе-
шен.
Резервное копирование на низком уровне можно произвести в монопольном или немонопольном
режиме. Рекомендуется применять немонопольный метод, а монопольный считается устаревшим
и в конце концов будет ликвидирован.
25.3.3.1. Немонопольное резервное копирование на низком уровне
Немонопольное резервное копирование позволяет параллельно запускать другие процессы копи-
рования (используя тот же API или pg_basebackup).</li>
  <li>Убедитесь, что архивирование WAL включено и работает.</li>
  <li>Подключитесь к серверу (к любой базе данных) как пользователь с правами на выполнение
pg_start_backup (суперпользователь или пользователь, которому дано право EXECUTE для этой
функции) и выполните команду:
SELECT pg_start_backup(‘label’, false, false);
где label — любая метка, по которой можно однозначно идентифицировать данную операцию
резервного копирования. Соединение, через которое вызывается pg_start_backup, должно под-
держиваться до окончания резервного копирования, иначе этот процесс будет автоматически
прерван.
По умолчанию pg_start_backup может выполняться длительное время. Это объясняется тем,
что функция выполняет контрольную точку, а операции ввода/вывода, требуемые для этого, рас-
пределяются в интервале времени, по умолчанию равном половине интервала между контроль-
ными точками (см. параметр checkpoint_completion_target). Обычно это вполне приемлемо, так
как при этом минимизируется влияние на выполнение других запросов. Если же вы хотите на-
чать резервное копирование максимально быстро, передайте во втором параметре true. В этом
случае контрольная точка будет выполнена немедленно без ограничения объёма ввода/вывода.
Третий параметр, имеющий значение false, указывает pg_start_backup начать немонопольное
базовое копирование.</li>
  <li>Скопируйте файлы, используя любое удобное средство резервного копирования, например, tar
или cpio (не pg_dump или pg_dumpall). В процессе копирования останавливать работу базы дан-
ных не требуется, это ничего не даёт. В Подразделе 25.3.3.3 описано, что следует учитывать в
процессе копирования.</li>
  <li>Через то же подключение, что и раньше, выполните команду:
SELECT * FROM pg_stop_backup(false, true);
При этом сервер выйдет из режима резервного копирования. Ведущий сервер вместе с этим ав-
томатически переключится на следующий сегмент WAL. На ведомом автоматическое переклю-
637Резервное копирова-
ние и восстановление
чение сегментов WAL невозможно, поэтому вы можете выполнить pg_switch_wal на ведущем,
чтобы произвести переключение вручную. Такое переключение позволяет получить готовый к
архивированию последний сегмент WAL, записанный в процессе резервного копирования.
Функция pg_stop_backup возвратит одну строку с тремя значениями. Второе из них нужно за-
писать в файл backup_label в корневой каталог резервной копии. Третье значение, если оно не
пустое, должно быть записано в файл tablespace_map. Эти значения крайне важны для восста-
новления копии и должны записываться без изменений.</li>
  <li>После этого останется заархивировать файлы сегментов WAL, активных во время создания ре-
зервной копии, и процедура резервного копирования будет завершена. Функция pg_stop_backup
в первом значении результата указывает, какой последний сегмент требуется для форми-
рования полного набора файлов резервной копии. На ведущем сервере, если включён ре-
жим архивации (параметр archive_mode) и аргумент wait_for_archive равен true, функция
pg_stop_backup не завершится, пока не будет заархивирован последний сегмент. На ведомом
значением archive_mode должно быть always, чтобы pg_stop_backup ожидала архивации. Эти
файлы будут заархивированы автоматически, поскольку также должна быть настроена команда
archive_command. Чаще всего это происходит быстро, но мы советуем наблюдать за системой
архивации и проверять, не возникают ли задержки. Если архивирование остановится из-за оши-
бок команды архивации, попытки архивации будут продолжаться до успешного завершения, и
только тогда резервная копия будет завершена. Если вы хотите ограничить время выполнения
pg_stop_backup, установите соответствующее значение в statement_timeout, но заметьте, что в
случае прерывания pg_stop_backup по времени резервная копия может оказаться негодной.
Если в процедуре резервного копирования предусмотрено отслеживание и архивация всех фай-
лов сегментов WAL, необходимых для резервной копии, то в аргументе wait_for_archive (по
умолчанию равном true) можно передать false, чтобы функция pg_stop_backup завершилась
сразу, как только в WAL будет помещена запись о завершении копирования. По умолчанию
pg_stop_backup будет ждать окончания архивации всех файлов WAL, что может занять некото-
рое время. Использовать этот параметр следует с осторожностью: если архивация WAL не кон-
тролируется, в резервной копии могут оказаться не все необходимые файлы WAL и её нельзя
будет восстановить.
25.3.3.2. Монопольное резервное копирование на низком уровне
Монопольное резервное копирование во многом похоже на немонопольное, но имеет несколько
важных отличий. Такое копирование можно произвести только на ведущем сервере, и оно исклю-
чает одновременное выполнение других процессов копирования. До PostgreSQL 9.6 это был един-
ственный возможный метод низкоуровневого копирования, но сейчас пользователям рекоменду-
ется по возможности подкорректировать свои скрипты и перейти к использованию немонополь-
ного варианта.</li>
  <li>Убедитесь, что архивирование WAL включено и работает.</li>
  <li>Подключитесь к серверу (к любой базе данных) как пользователь с правами на выполнение
pg_start_backup (суперпользователь или пользователь, которому дано право EXECUTE для этой
функции) и выполните команду:
SELECT pg_start_backup(‘label’);
где label — любая метка, по которой можно однозначно идентифицировать данную операцию
резервного копирования. Функция pg_start_backup создаёт в каталоге кластера файл метки
резервного копирования, называемый backup_label, в который помещается информация о ре-
зервной копии, включающая время начала и строку метки. Эта функция также создаёт в ката-
логе кластера файл карты табличных пространств, называемый tablespace_map, с информа-
цией о символических ссылках табличных пространств в pg_tblspc/, если такие ссылки есть.
Оба файла важны для целостности резервных копии и понадобятся при восстановлении.
По умолчанию pg_start_backup может выполняться длительное время. Это объясняется тем,
что функция выполняет контрольную точку, а операции ввода/вывода, требуемые для этого, рас-
пределяются в интервале времени, по умолчанию равном половине интервала между контроль-
638Резервное копирова-
ние и восстановление
ными точками (см. параметр checkpoint_completion_target). Обычно это вполне приемлемо, так
как при этом минимизируется влияние на выполнение других запросов. Если же вы хотите на-
чать резервное копирование максимально быстро, выполните:
SELECT pg_start_backup(‘label’, true);
При этом контрольная точка будет выполнена как можно скорее.</li>
  <li>Скопируйте файлы, используя любое удобное средство резервного копирования, например, tar
или cpio (не pg_dump или pg_dumpall). В процессе копирования останавливать работу базы дан-
ных не требуется, это ничего не даёт. В Подразделе 25.3.3.3 описано, что следует учитывать в
процессе копирования.
Заметьте, что в случае сбоя сервера во время резервного копирования для успешного его пере-
запуска может потребоваться вручную удалить файл backup_label из каталога PGDATA.</li>
  <li>Снова подключитесь к базе данных как пользователь с правами на выполнение pg_stop_backup
(суперпользователь или пользователь, которому дано право EXECUTE для этой функции) и вы-
полните команду:
SELECT pg_stop_backup();
Эта функция завершит режим резервного копирования и автоматически переключится на сле-
дующий сегмент WAL. Это переключение выполняется для того, чтобы файл последнего сегмен-
та WAL, записанного во время копирования, был готов к архивации.</li>
  <li>После этого останется заархивировать файлы сегментов WAL, активных во время создания ре-
зервной копии, и процедура резервного копирования будет завершена. Функция pg_stop_backup
возвращает указание на файл последнего сегмента, который требуется для формирования пол-
ного набора файлов резервной копии. Если включён режим архивации (параметр archive_mode),
функция pg_stop_backup не завершится, пока не будет заархивирован последний сегмент. В
этом случае файлы будут заархивированы автоматически, поскольку также должна быть настро-
ена команда archive_command. Чаще всего это происходит быстро, но мы советуем наблюдать за
системой архивации и проверять, не возникают ли задержки. Если архивирование остановится
из-за ошибок команды архивации, попытки архивации будут продолжаться до успешного завер-
шения, и только тогда резервная копия будет завершена. Если вы хотите ограничить время вы-
полнения pg_stop_backup, установите соответствующее значение в statement_timeout, но за-
метьте, что в случае прерывания pg_stop_backup по времени резервная копия может оказаться
негодной.
25.3.3.3. Копирование каталога данных
Некоторые средства резервного копирования файлов выдают предупреждения или ошибки, если
файлы, которые они пытаются скопировать, изменяются в процессе копирования. При получении
базовой резервной копии активной базы данных это вполне нормально и не является ошибкой.
Однако, вам нужно знать, как отличить ошибки такого рода от реальных ошибок. Например, неко-
торые версии rsync возвращают отдельный код выхода для ситуации «исчезнувшие исходные фай-
лы», и вы можете написать управляющий скрипт, который примет этот код как не ошибочный.
Также некоторые версии GNU tar возвращают код выхода, неотличимый от кода фатальной ошиб-
ки, если файл был усечён, когда tar копировал его. К счастью, GNU tar версий 1.16 и более поздних
завершается с кодом 1, если файл был изменён во время копирования, и 2 в случае других ошибок.
С GNU tar версии 1.23 и более поздними, вы можете использовать следующие ключи –warning=no-
file-changed –warning=no-file-removed, чтобы скрыть соответствующие предупреждения.
Убедитесь, что ваша резервная копия включает все файлы из каталога кластера баз данных (на-
пример, /usr/local/pgsql/data). Если вы используете табличные пространства, которые находят-
ся не внутри этого каталога, не забудьте включить и их в резервную копию (также важно, что-
бы при создании резервной копии символьные ссылки сохранялись как ссылки, иначе табличные
пространства будут повреждены при восстановлении).
Однако следует исключить из резервной копии файлы в подкаталоге данных кластера pg_wal/. Эту
небольшую корректировку стоит внести для снижения риска ошибок при восстановлении. Это лег-
639Резервное копирова-
ние и восстановление
ко организовать, если pg_wal/ — символическая ссылка на каталог за пределами каталога данных
(так часто делают из соображений производительности). Также имеет смысл исключить файлы
postmaster.pid и postmaster.opts, содержащие информацию о работающем процессе postmaster
(а не о том процессе postmaster, который будет восстанавливать эту копию). (Эти файлы могут вве-
сти pg_ctl в заблуждение.)
Часто также стоит исключать из резервной копии каталог pg_replslot/ кластера, чтобы слоты
репликации, существующие на главном сервере, не попадали в копию. В противном случае, при
последующем восстановлении копии на резервном сервере может получиться так, что он будет
неограниченно долго сохранять файлы WAL, а главный не будет очищаться, если он следит за го-
рячим резервом, так как клиенты этих слотов репликации будут продолжать подключаться и из-
менять состояние слотов на главном, а не резервном сервере. Даже если резервная копия пред-
назначена только для создания нового главного сервера, копирование слотов репликации вряд ли
принесёт пользу, так как к моменту включения в работу этого нового сервера содержимое этих
слотов станет абсолютно неактуальным.
Содержимое каталогов pg_dynshmem/, pg_notify/, pg_serial/, pg_snapshots/, pg_stat_tmp/ и
pg_subtrans/ (но не сами эти каталоги) можно исключить из резервной копии, так как оно будет
инициализировано при запуске главного процесса. Если переменная stats_temp_directory установ-
лена и указывает на подкаталог внутри каталога данных, содержимое этого подкаталога также
можно не копировать.
Из резервной копии можно исключить и файлы и подкаталоги с именами, начинающимся с
pgsql_tmp. Эти файлы удаляются при запуске главного процесса, а каталоги создаются по мере
необходимости.
Из резервной копии могут быть исключены файлы pg_internal.init. Такие файлы содержат ке-
шируемые данные отношения и всегда перестраиваются при восстановлении.
В файл метки резервной копии записывается строка метки, заданная при вызове pg_start_backup,
время запуска функции pg_start_backup и имя начального файла WAL. Таким образом, в случае
сомнений можно заглянуть внутрь архива резервной копии и точно определить, в каком сеансе
резервного копирования он был создан. Файл карты табличных пространств содержит имена сим-
волических ссылок, как они существуют в каталоге pg_tblspc/, и полный путь каждой символи-
ческой ссылки. Эти файлы не только к вашему сведению; их существование и содержание важны
для правильного проведения процесса восстановления системы.
Вы также можете создать резервную копию, когда сервер остановлен. В этом случае, вы, очевид-
но, не сможете вызвать pg_start_backup или pg_stop_backup, и следовательно, вам надо будет
самостоятельно как-то идентифицировать резервные копии и понимать, какие файлы WAL долж-
ны быть заархивированы. Поэтому обычно всё-таки лучше следовать вышеописанной процедуре
непрерывного архивирования.
25.3.4. Восстановление непрерывной архивной копии
Допустим, худшее случилось и вам необходимо восстановить базу данных из резервной копии.
Порядок действий таков:</li>
  <li>Остановите сервер баз данных, если он запущен.</li>
  <li>Если у вас есть место для этого, скопируйте весь текущий каталог кластера баз данных и все таб-
личные пространства во временный каталог на случай, если они вам понадобятся. Учтите, что
эта мера предосторожности требует, чтобы свободного места на диске было достаточно для раз-
мещения двух копий существующих данных. Если места недостаточно, необходимо сохранить
как минимум содержимое подкаталога pg_wal каталога кластера, так как он может содержать
журналы, не попавшие в архив перед остановкой системы.</li>
  <li>Удалите все существующие файлы и подкаталоги из каталога кластера и из корневых каталогов
используемых табличных пространств.</li>
  <li>Восстановите файлы базы данных из архивной копии файлов. Важно, чтобы у восстановленных
файлов были правильные разрешения и правильный владелец (пользователь, запускающий сер-
640Резервное копирова-
ние и восстановление
вер, а не root!). Если вы используете табличные пространства, убедитесь также, что символьные
ссылки в pg_tblspc/ восстановились корректно.</li>
  <li>Удалите все файлы из pg_wal/; они восстановились из резервной копии файлов и поэтому, скорее
всего, будут старее текущих. Если вы вовсе не архивировали pg_wal/, создайте этот каталог с
правильными правами доступа, но если это была символьная ссылка, восстановите её.</li>
  <li>Если на шаге 2 вы сохранили незаархивированные файлы с сегментами WAL, скопируйте их в
pg_wal/. (Лучше всего именно копировать, а не перемещать их, чтобы у вас остались неизме-
нённые файлы на случай, если возникнет проблема и всё придётся начинать сначала.)</li>
  <li>Создайте командный файл восстановления recovery.conf в каталоге кластера баз данных (см.
Главу 27). Вы можете также временно изменить pg_hba.conf, чтобы обычные пользователи не
могли подключаться, пока вы не будете уверены, что восстановление завершилось успешно.</li>
  <li>Запустите сервер. Сервер запустится в режиме восстановления и начнёт считывать необходи-
мые ему архивные файлы WAL. Если восстановление будет прервано из-за внешней ошибки,
сервер можно просто перезапустить и он продолжит восстановление. По завершении процесса
восстановления сервер переименует файл recovery.conf в recovery.done (чтобы предотвратить
повторный запуск режима восстановления), а затем перейдёт к обычной работе с базой данных.</li>
  <li>Просмотрите содержимое базы данных, чтобы убедиться, что вы вернули её к желаемому состо-
янию. Если это не так, вернитесь к шагу 1. Если всё хорошо, разрешите пользователям подклю-
чаться к серверу, восстановив обычный файл pg_hba.conf.
Ключевой момент этой процедуры заключается в создании файла конфигурации восстановления,
описывающего, как будет выполняться восстановление и до какой точки. В качестве прототипа вы
можете использовать файл recovery.conf.sample (он обычно помещается в каталог share/ после
установки). Единственное, что совершенно необходимо указать в recovery.conf — это команду
restore_command, которая говорит PostgreSQL, как получать из архива файл-сегменты WAL. Как и
archive_command, это командная строка для оболочки. Она может содержать символы %f, которые
заменятся именем требующегося файла журнала, и %p, которые заменятся целевым путём для
копирования этого файла. (Путь задаётся относительно текущего рабочего каталога, т. е. каталога
кластера данных.) Если вам нужно включить в команду сам символ %, напишите %%. Простейшая
команда, которая может быть полезна, такая:
restore_command = ‘cp /mnt/server/archivedir/%f %p’
Эта команда копирует заархивированные ранее сегменты WAL из каталога /mnt/server/
archivedir. Разумеется, вы можете использовать что-то более сложное, возможно, даже скрипт
оболочки, который укажет оператору установить соответствующую ленту.
Важно, чтобы данная команда возвращала ненулевой код возврата в случае ошибки. Эта команда
будет вызываться и с запросом файлов, отсутствующих в архиве; в этом случае она должна вер-
нуть ненулевое значение и это считается штатной ситуацией. В исключительной ситуации, когда
команда была прервана сигналом (кроме SIGTERM, который применяется в процессе остановки
сервера базы данных) или произошла ошибка оболочки (например, команда не найдена), восста-
новление будет прервано и сервер не запустится.
Не все запрашиваемые файлы будут сегментами WAL; следует также ожидать запросов файлов с
суффиксом .history. Также учтите, что базовое имя пути %p будет отличаться от %f; не думайте,
что они взаимозаменяемы.
Сегменты WAL, которые не найдутся в архиве, система будет искать в pg_wal/; благодаря этому
можно использовать последние незаархивированные сегменты. Однако файлы в pg_wal/ будут ме-
нее предпочтительными, если такие сегменты окажутся в архиве.
Обычно при восстановлении обрабатываются все доступные сегменты WAL и, таким образом, база
данных восстанавливается до последнего момента времени (или максимально близкого к нему, в
зависимости от наличия сегментов WAL). Таким образом, восстановление обычно завершается с
сообщением «файл не найден»; точный текст сообщения об ошибке зависит от того, что делает
restore_command. Вы также можете увидеть сообщение об ошибке в начале восстановления для
641Резервное копирова-
ние и восстановление
файла с именем типа 00000001.history. Это также нормально и обычно не говорит о какой-либо
проблеме при восстановлении в простых ситуациях; подробнее об этом рассказывается в Подраз-
деле 25.3.5.
Если вы хотите восстановить базу на какой-то момент времени (скажем, до момента, когда неопыт-
ный администратор базы данных удалил основную таблицу транзакций), просто укажите требуе-
мую точку остановки в recovery.conf. Вы можете задать точку останова, так называемую «цель
восстановления», по дате/времени, именованной точке восстановления или определённому иден-
тификатору транзакции. На момент написания этой документации полезными могут быть только
указания даты/времени или имени точки восстановления, пока нет никаких средств, позволяющих
точно определить, какой идентификатор транзакции нужно выбрать.
Примечание
Точка останова должна указывать на момент после окончания базового копирования,
т. е. после времени завершения pg_stop_backup. Использовать базовую резервную ко-
пию для восстановления на момент времени, когда она ещё только создавалась, нель-
зя. (Чтобы восстановить данные на этот момент времени, придётся вернуться к преды-
дущей базовой резервной копии и накатывать изменения с этой позиции.)
Если при восстановлении обнаруживаются повреждённые данные WAL, восстановление прерыва-
ется в этом месте и сервер не запускается. В этом случае процесс восстановления можно пере-
запустить с начала, указав «цель восстановления» до точки повреждения, чтобы восстановление
могло завершиться нормально. Если восстановление завершается ошибкой из-за внешней причи-
ны, например, из-за краха системы или недоступности архива WAL, его можно просто перезапу-
стить, и оно продолжится с того места, где было прервано. Перезапуск восстановления реализован
по тому же принципу, что и контрольные точки при обычной работе: сервер периодически сохра-
няет всё текущее состояние на диске и отражает это в файле pg_control, чтобы уже обработанные
данные WAL не приходилось сканировать снова.
25.3.5. Линии времени
Возможность восстановить базу данных на некий предыдущий момент времени создаёт некоторые
сложности, сродни научно-фантастическим историям о путешествиях во времени и параллельных
мирах. Например, предположим, что в начальной истории базы данных вы удалили важную табли-
цу в 17:15 во вторник, но осознали эту ошибку только в среду в полдень. Вы можете спокойно взять
резервную копию, восстановить данные на 17:14 во вторник и запустить сервер. В этой истории
мира базы данных вы никогда не удаляли вышеупомянутую таблицу. Но предположим, что позже
вы заметили, что это была не такая уж хорошая идея и захотели вернуться к утру среды в перво-
начальной истории базы данных. Вы не сможете сделать это, если в процессе работы базы данных
она успеет перезаписать какие-либо файлы-сегменты WAL, приводящие к моменту времени, к ко-
торому вы хотите вернуться теперь. Таким образом, для получения желаемого результата необ-
ходимо как-то отличать последовательности записей WAL, добавленные после восстановления на
какой-то момент времени от тех, что существовали в начальной истории базы данных.
Для решения этой проблемы в PostgreSQL есть такое понятие, как линия времени. Всякий раз,
когда завершается восстановление из архива, создаётся новая линия времени, позволяющая иден-
тифицировать последовательность записей WAL, добавленных после этого восстановления. Номер
линии времени включается в имя файлов-сегментов WAL, так что файлы новой линии времени не
перезаписывают файлы WAL, сгенерированные предыдущими линиями времени. Фактически это
позволяет архивировать много различных линий времени. Хотя это может показаться бесполезной
возможностью, на самом деле она часто бывает спасительной. Представьте, что вы не определи-
лись, какую точку времени выбрать для восстановления, и таким образом должны проводить вос-
становление методом проб и ошибок, пока не найдёте лучший момент для ответвления от старой
истории. Без линий времени этот процесс быстро стал бы очень запутанным. А благодаря линиям
времени, вы можете вернуться к любому предыдущему состоянию, включая состояния в ветках
линий времени, покинутых ранее.
642Резервное копирова-
ние и восстановление
Каждый раз, когда образуется новая линия времени, PostgreSQL создаёт файл «истории линии
времени», показывающий, от какой линии времени ответвилась данная и когда. Эти файлы исто-
рии нужны, чтобы система могла выбрать правильные файлы-сегменты WAL при восстановлении
из архива, содержащего несколько линий времени. Таким образом, они помещаются в область ар-
хивов WAL так же, как и файлы сегментов WAL. Файлы истории представляют собой небольшие
текстовые файлы, так что они не занимают много места и их вполне можно сохранять неограни-
ченно долго (в отличие от файлов сегментов, имеющих большой размер). Если хотите, вы можете
добавлять в файл истории комментарии, свои собственные заметки о том, как и почему была со-
здана эта конкретная линия времени. Такие комментарии будут особенно ценны, если в результа-
те экспериментов у вас образуется хитросплетение разных линий времени.
По умолчанию при восстановлении восстанавливается та же линия времени, которая была теку-
щей при создании базовой резервной копии. Если вы хотите восстановить состояние на какой-либо
дочерней линии времени, (то есть, хотите вернуться к некоторому состоянию, которое тоже было
получено в результате попытки восстановления), вам необходимо указать идентификатор целевой
линии времени в recovery.conf. Восстановить состояние в линии времени, ответвившейся раньше,
чем была сделана базовая резервная копия, нельзя.
25.3.6. Советы и примеры
Ниже мы дадим несколько советов по настройке непрерывного архивирования.
25.3.6.1. Обособленные горячие резервные копии
Средства резервного копирования PostgreSQL можно применять для создания обособленных горя-
чих копий. Эти копии нельзя использовать для восстановления на момент времени, но создаются
и восстанавливаются они обычно гораздо быстрее, чем дампы pg_dump. (Они также намного боль-
ше, чем дампы pg_dump, так что в некоторых случаях выигрыш в скорости может быть потерян.)
Как и базовые резервные копии, обособленную горячую копию проще всего получить, используя
программу pg_basebackup. Если вы вызовете эту программу с параметром -X, в эту копию авто-
матически будет включён весь журнал предзаписи, необходимый для её использования, так что
никакие особые действия для восстановления не потребуются.
Если нужна дополнительная гибкость в процессе копирования файлов, создавать обособленные
горячие копии можно также на более низком уровне. Чтобы подготовиться к получению такой ко-
пии на низком уровне, установите в wal_level уровень replica (или выше), в archive_mode значе-
ние on и настройте команду archive_command, которая будет выполнять архивацию, только когда
существует файл-переключатель. Например:
archive_command = ‘test ! -f /var/lib/pgsql/backup_in_progress || (test ! -f /var/lib/
pgsql/archive/%f &amp;&amp; cp %p /var/lib/pgsql/archive/%f)’
Данная команда выполнит архивацию, если будет существовать файл /var/lib/pgsql/
backup_in_progress, а в противном случае просто вернёт нулевой код возврата (и тогда PostgreSQL
сможет переработать ненужный файл WAL).
После такой подготовки резервную копию можно создать, например таким скриптом:
touch /var/lib/pgsql/backup_in_progress
psql -c “select pg_start_backup(‘hot_backup’);”
tar -cf /var/lib/pgsql/backup.tar /var/lib/pgsql/data/
psql -c “select pg_stop_backup();”
rm /var/lib/pgsql/backup_in_progress
tar -rf /var/lib/pgsql/backup.tar /var/lib/pgsql/archive/
Сначала создаётся файл-переключатель /var/lib/pgsql/backup_in_progress, включающий архи-
вирование заполненных файлов WAL. По окончании резервного копирования файл-переключатель
удаляется. Затем заархивированные файлы WAL тоже добавляются в резервную копию, так что в
одном архиве tar оказывается и базовая резервная копия, и все требуемые файлы WAL. Пожалуй-
ста, не забудьте добавить в ваши скрипты резервного копирования обработку ошибок.
643Резервное копирова-
ние и восстановление
25.3.6.2. Сжатие журналов в архиве
Если размер архива имеет большое значение, можно воспользоваться gzip и сжимать архивные
файлы:
archive_command = ‘gzip &lt; %p &gt; /var/lib/pgsql/archive/%f’
При этом для восстановления придётся использовать gunzip:
restore_command = ‘gunzip &lt; /mnt/server/archivedir/%f &gt; %p’
25.3.6.3. Скрипты archive_command
Многие в качестве команды archive_command
postgresql.conf оказывается очень простой:
используют
скрипты,
так
что
запись
в
archive_command = ‘local_backup_script.sh “%p” “%f”’
Применять отдельный файла скрипта целесообразно всегда, когда вы хотите использовать в про-
цедуре архивирования несколько команд. Это позволяет управлять сложностью этой процедуры в
рамках одного скрипта, который можно написать на любом популярном языке скриптов, например
на bash или perl.
В частности, с помощью скриптов можно решить такие задачи:
• Копирование данных в безопасное внешнее хранилище
• Пакетная обработка файлов WAL, чтобы они передавались каждые три часа, а не по одному
• Взаимодействие с другими приложениями резервного копирования и восстановления
• Взаимодействие со средствами мониторинга, регистрация ошибок
Подсказка
Когда в archive_command используется скрипт, желательно включить logging_collector.
Тогда все сообщения, которые скрипт выведет в stderr, будут записываться в журнал
сервера баз данных, что позволит легко диагностировать ошибки в сложных конфигу-
рациях.
25.3.7. Ограничения
На момент написания документации методика непрерывного архивирования имеет несколько
ограничений. Они могут быть ликвидированы в будущих версиях:
• Если во время создания базовой резервной копии выполняется команда CREATE DATABASE, а
затем база-шаблон, задействованная в CREATE DATABASE, изменяется, пока продолжается ко-
пирование, возможно, что при восстановлении эти изменения распространятся также и на со-
зданную базу данных. Конечно, это нежелательно. Во избежание подобных рисков, лучше все-
го не изменять никакие базы-шаблоны во время получения базовой резервной копии.
• Команды CREATE TABLESPACE записываются в WAL с абсолютным путём и, таким образом,
при воспроизведении WAL будут выполнены с тем же абсолютным путём. Это может быть
нежелательно, если журнал воспроизводится на другой машине. Но опасность есть, даже ес-
ли журнал воспроизводится на той же машине, но в другом каталоге данных: при воспроизве-
дении будет так же перезаписано содержимое исходных табличных пространств. Чтобы избе-
жать потенциальных проблем такого рода, лучше всего делать новую базовую резервную ко-
пию после создания или удаления табличных пространств.
Также следует заметить, что стандартный формат WAL не очень компактный, так как включает
много снимков дисковых страниц. Эти снимки страниц предназначены для поддержки восстанов-
ления после сбоя, на случай, если понадобится исправить страницы, записанные на диск частич-
но. В зависимости от аппаратного и программного обеспечения вашей системы, риск частичной
644Резервное копирова-
ние и восстановление
записи может быть достаточно мал, так что его можно игнорировать, и в этом случае можно суще-
ственно уменьшить общий объём архивируемых журналов, выключив снимки страниц с помощью
параметра full_page_writes. (Прежде чем делать это, прочтите замечания и предупреждения в Гла-
ве 30.) Выключение снимков страниц не препятствует использованию журналов для восстановле-
ния PITR. Одним из направлений разработки в будущем является сжатие архивируемых данных
WAL, путём удаления ненужных копий страниц даже при включённом режиме full_page_writes.
Тем временем администраторы могут сократить количество снимков страниц, включаемых в WAL,
увеличив параметры интервала контрольных точек в разумных пределах.
645</li>
</ol>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/PostgreSQL-V11_Doc-024/" title="Глава 24. Регламентные задачи обслуживания базы данных"><img src="http://localhost:4000/images/abstract-11.jpg" alt="Глава 24. Регламентные задачи обслуживания базы данных"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-12-01T00:00:00+02:00"><a href="http://localhost:4000/PostgreSQL-V11_Doc-024/">December 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Sergey Khatsiola">Sergey Khatsiola</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~25 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/PostgreSQL-V11_Doc-024/" rel="bookmark" title="Глава 24. Регламентные задачи обслуживания базы данных" itemprop="url">Глава 24. Регламентные задачи обслуживания базы данных</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Глава 24. Регламентные задачи обслуживания базы данных</p>

<p>Как и в любой СУБД, в PostgreSQL для достижения оптимальной производительности нужно ре-
гулярно выполнять определённые процедуры. Задачи, которые рассматриваются в этой главе, яв-
ляются обязательными, но они по природе своей повторяющиеся и легко поддаются автоматиза-
ции с использованием стандартных средств, таких как задания cron или Планировщика задач в
Windows. Создание соответствующих заданий и контроль над их успешным выполнением входят в
обязанности администратора базы данных.
Одной из очевидных задач обслуживания СУБД является регулярное создание резервных копий
данных. При отсутствии свежей резервной копии у вас не будет шанса восстановить систему после
катастрофы (сбой диска, пожар, удаление важной таблицы по ошибке и т. д.). Механизмы резерв-
ного копирования и восстановления в PostgreSQL детально рассматриваются в Главе 25.
Другое важное направление обслуживания СУБД — периодическая «очистка» базы данных. Эта
операция рассматривается в Разделе 24.1. С ней тесно связано обновление статистики, которая
будет использоваться планировщиком запросов; оно рассматривается в Подразделе 24.1.3.
Ещё одной задачей, требующей периодического выполнения, является управление файлами жур-
нала. Она рассматривается в Разделе 24.3.
Для контроля состояния базы данных и для отслеживания нестандартных ситуаций можно исполь-
зовать check_postgres. Скрипт check_postgres можно интегрировать с Nagios и MRTG, однако он
может работать и самостоятельно.
По сравнению с некоторыми другими СУБД PostgreSQL неприхотлив в обслуживании. Тем не ме-
нее, должное внимание к вышеперечисленным задачам будет значительно способствовать ком-
фортной и производительной работе с СУБД.
24.1. Регламентная очистка
Базы данных PostgreSQL требуют периодического проведения процедуры обслуживания, которая
называется очисткой. Во многих случаях очистку достаточно выполнять с помощью демона ав-
тоочистки, который описан в Подразделе 24.1.6. Возможно, в вашей ситуации для получения оп-
тимальных результатов потребуется настроить описанные там же параметры автоочистки. Неко-
торые администраторы СУБД могут дополнить или заменить действие этого демона командами
VACUUM (обычно они выполняются по расписанию в заданиях cron или Планировщика задач). Чтобы
правильно организовать очистку вручную, необходимо понимать темы, которые будут рассмотре-
ны в следующих подразделах. Администраторы, которые полагаются на автоочистку, возможно,
всё же захотят просмотреть этот материал, чтобы лучше понимать и настраивать эту процедуру.
24.1.1. Основные принципы очистки
Команды VACUUM в PostgreSQL должны обрабатывать каждую таблицу по следующим причинам:</p>
<ol>
  <li>Для высвобождения или повторного использования дискового пространства, занятого изменён-
ными или удалёнными строками.</li>
  <li>Для обновления статистики по данным, используемой планировщиком запросов PostgreSQL.</li>
  <li>Для обновления карты видимости, которая ускоряет сканирование только индекса.</li>
  <li>Для предотвращения потери очень старых данных из-за зацикливания идентификаторов тран-
закций или мультитранзакций.
Разные причины диктуют выполнение действий VACUUM с разной частотой и в разном объёме, как
рассматривается в следующих подразделах.
Существует два варианта VACUUM: обычный VACUUM и VACUUM FULL. Команда VACUUM FULL может вы-
свободить больше дискового пространства, однако работает медленнее. Кроме того, обычная ко-
619Регламентные задачи об-
служивания базы данных
манда VACUUM может выполняться параллельно с использованием производственной базы данных.
(При этом такие команды как SELECT, INSERT, UPDATE и DELETE будут выполняться нормально, хотя
нельзя будет изменить определение таблицы командами типа ALTER TABLE.) Команда VACUUM FULL
требует исключительной блокировки обрабатываемой таблицы и поэтому не может выполняться
параллельно с другими операциями с этой таблицей. По этой причине администраторы, как пра-
вило, должны стараться использовать обычную команду VACUUM и избегать VACUUM FULL.
Команда VACUUM порождает существенный объём трафика ввода/вывода, который может стать при-
чиной низкой производительности в других активных сеансах. Это влияние фоновой очистки мож-
но регулировать, настраивая параметры конфигурации (см. Подраздел 19.4.4).
24.1.2. Высвобождение дискового пространства
В PostgreSQL команды UPDATE или DELETE не вызывают немедленного удаления старой версии из-
меняемых строк. Этот подход необходим для реализации эффективного многоверсионного управ-
ления конкурентным доступом (MVCC, см. Главу 13): версия строки не должна удаляться до тех
пор, пока она остаётся потенциально видимой для других транзакций. Однако в конце концов уста-
ревшая или удалённая версия строки оказывается не нужна ни одной из транзакций. После этого
занимаемое ей место должно быть освобождено и может быть отдано новым строкам, во избежа-
ние неограниченного роста потребности в дисковом пространстве. Это происходит при выполне-
нии команды VACUUM.
Обычная форма VACUUM удаляет неиспользуемые версии строк в таблицах и индексах и помечает
пространство свободным для дальнейшего использования. Однако это дисковое пространство не
возвращается операционной системе, кроме особого случая, когда полностью освобождаются од-
на или несколько страниц в конце таблицы и можно легко получить исключительную блокировку
таблицы. Команда VACUUM FULL, напротив, кардинально сжимает таблицы, записывая абсолютно
новую версию файла таблицы без неиспользуемого пространства. Это минимизирует размер таб-
лицы, однако может занять много времени. Кроме того, для этого требуется больше места на диске
для записи новой копии таблицы до завершения операции.
Обычно цель регулярной очистки — выполнять простую очистку (VACUUM) достаточно часто, чтобы
не возникала необходимость в VACUUM FULL. Демон автоочистки пытается работать в этом режиме,
и на самом деле он сам никогда не выполняет VACUUM FULL. Основная идея такого подхода не в
том, чтобы минимизировать размер таблиц, а в том, чтобы поддерживать использование дискового
пространства на стабильном уровне: каждая таблица занимает объём, равный её минимальному
размеру, плюс объём, который был занят между процедурами очистки. Хотя с помощью VACUUM
FULL можно сжать таблицу до минимума и возвратить дисковое пространство операционной систе-
ме, большого смысла в этом нет, если в будущем таблица так же вырастет снова. Следовательно,
для активно изменяемых таблиц лучше с умеренной частотой выполнять VACUUM, чем очень редко
выполнять VACUUM FULL.
Некоторые администраторы предпочитают планировать очистку БД самостоятельно, например,
проводя все работы ночью в период низкой загрузки. Однако очистка только по фиксированно-
му расписанию плоха тем, что при резком скачке интенсивности изменений таблица может раз-
растить настолько, что для высвобождения пространства действительно понадобится выполнить
VACUUM FULL. Использование демона автоочистки снимает эту проблему, поскольку он планирует
очистку динамически, отслеживая интенсивность изменений. Полностью отключать этот демон
может иметь смысл, только если вы имеете дело с предельно предсказуемой загрузкой. Возможен
и компромиссный вариант — настроить параметры демона автоочистки так, чтобы он реагировал
только на необычайно высокую интенсивность изменений и мог удержать ситуацию под контро-
лем, в то время как команды VACUUM, запускаемые по расписанию, будут выполнять основную ра-
боту в периоды нормальной загрузки.
Если же автоочистка не применяется, обычно планируется выполнение VACUUM для всей базы дан-
ных раз в сутки в период низкой активности, и в случае необходимости оно дополняется более
частой очисткой интенсивно изменяемых таблиц. (В некоторых ситуациях, когда изменения про-
изводятся крайне интенсивно, самые востребованные таблицы могут очищаться раз в несколько
620Регламентные задачи об-
служивания базы данных
минут.) Если в вашем кластере несколько баз данных, не забывайте выполнять VACUUM для каждой
из них; при этом может быть полезна программа vacuumdb.
Подсказка
Результат обычного VACUUM может быть неудовлетворительным, когда вследствие мас-
сового изменения или удаления строк в таблице оказывается много мёртвых версий
строк. Если у вас есть такая таблица и вам нужно освободить лишнее пространство,
которое она занимает, используйте команду VACUUM FULL или, в качестве альтернати-
вы, CLUSTER или один из вариантов ALTER TABLE, выполняющий перезапись таблицы.
Эти команды записывают абсолютно новую копию таблицы и строят для неё индексы.
Все эти варианты требуют исключительной блокировки. Заметьте, что они также на
время требуют дополнительного пространства на диске в объёме, приблизительно рав-
ном размеру таблицы, поскольку старые копии таблицы и индексов нельзя удалить до
завершения создания новых копий.
Подсказка
Если у вас есть таблица, всё содержимое которой периодически удаляется, рассмот-
рите возможность использования TRUNCATE вместо DELETE с последующей командой
VACUUM. TRUNCATE немедленно удаляет всё содержимое таблицы, не требуя последую-
щей очистки (VACUUM или VACUUM FULL) для высвобождения неиспользуемого дисково-
го пространства. Недостатком такого подхода является нарушение строгой семантики
MVCC.
24.1.3. Обновление статистики планировщика
Планировщик запросов в PostgreSQL, выбирая эффективные планы запросов, полагается на ста-
тистическую информацию о содержимом таблиц. Эта статистика собирается командой ANALYZE,
которая может вызываться сама по себе или как дополнительное действие команды VACUUM. Ста-
тистика должна быть достаточно точной, так как в противном случае неудачно выбранные планы
запросов могут снизить производительность базы данных.
Демон автоочистки, если он включён, будет автоматически выполнять ANALYZE после существен-
ных изменений содержимого таблицы. Однако администраторы могут предпочесть выполнение
ANALYZE вручную, в частности, если известно, что производимые в таблице изменения не повлия-
ют на статистику по «интересным» столбцам. Демон же планирует выполнение ANALYZE в зависи-
мости только от количества вставленных или изменённых строк; он не знает, приведут ли они к
значимым изменениям статистики.
Как и процедура очистки для высвобождения пространства, частое обновление статистики полез-
нее для интенсивно изменяемых таблиц, нежели для тех таблиц, которые изменяются редко. Од-
нако даже в случае часто изменяемой таблицы обновление статистики может не требоваться, ес-
ли статистическое распределение данных меняется слабо. Как правило, достаточно оценить, на-
сколько меняются максимальное и минимальное значения в столбцах таблицы. Например, макси-
мальное значение в столбце timestamp, хранящем время изменения строки, будет постоянно уве-
личиваться по мере добавления и изменения строк; для такого столбца может потребоваться более
частое обновление статистики, чем, к примеру, для столбца, содержащего адреса страниц (URL),
которые запрашивались с сайта. Столбец с URL-адресами может меняться столь же часто, однако
статистическое распределение его значений, вероятно, будет изменяться относительно медленно.
Команду ANALYZE можно выполнять для отдельных таблиц и даже просто для отдельных столбцов
таблицы, поэтому, если того требует приложение, одни статистические данные можно обновлять
чаще, чем другие. Однако на практике обычно лучше просто анализировать всю базу данных, по-
621Регламентные задачи об-
служивания базы данных
скольку это быстрая операция, так как ANALYZE читает не каждую отдельную строку, а статисти-
чески случайную выборку строк таблицы.
Подсказка
Хотя индивидуальная настройка частоты ANALYZE для отдельных столбцов может быть
не очень полезной, смысл может иметь настройка детализации статистики, собирае-
мой командой ANALYZE. Для столбцов, которые часто используются в предложениях
WHERE, и имеют очень неравномерное распределение данных, может потребоваться бо-
лее детальная, по сравнению с другими столбцами, гистограмма данных. В таких слу-
чаях можно воспользоваться командой ALTER TABLE SET STATISTICS или изменить зна-
чение по умолчанию параметра уровня БД default_statistics_target.
Кроме того, по умолчанию информация об избирательности функций ограничена. Од-
нако если вы создаёте индекс по выражению с вызовом функции, об этой функции бу-
дет собрана полезная статистическая информация, которая может значительно улуч-
шить планы запросов, в которых используется данный индекс.
Подсказка
Демон автоочистки не выполняет команды ANALYZE для сторонних таблиц, поскольку
он не знает, как часто это следует делать. Если для получения качественных планов
вашим запросам необходима статистика по сторонним таблицам, будет хорошей идеей
дополнительно запускать ANALYZE для них по подходящему расписанию.
24.1.4. Обновление карты видимости
Процедура очистки поддерживает карты видимости для каждой таблицы, позволяющие опреде-
лить, в каких страницах есть только записи, заведомо видимые для всех активных транзакций (и
всех будущих транзакций, пока страница не будет изменена). Это имеет два применения. Во-пер-
вых, сам процесс очистки может пропускать такие страницы при следующем запуске, поскольку
на этих страницах вычищать нечего.
Во-вторых, с такими картами PostgreSQL может выдавать результаты некоторых запросов, исполь-
зуя только индекс, не обращаясь к данным таблицы. Так как индексы PostgreSQL не содержат
информацию о видимости записей, при обычном сканировании по индексу необходимо извлечь
соответствующую запись из таблицы и проверить её видимость для текущей транзакции. Поэтому
при сканировании только индекса, наоборот, сначала проверяется карта видимости. Если извест-
но, что все записи на странице видимы, то выборку из таблицы можно пропустить. Это наиболее
полезно с большими наборах данных, когда благодаря карте видимости можно оптимизировать
чтение с диска. Карта видимости значительно меньше таблицы, поэтому она легко помещается в
кеш, даже когда объём самих страниц очень велик.
24.1.5. Предотвращение ошибок из-за зацикливания счётчика
транзакций
В PostgreSQL семантика транзакций MVCC зависит от возможности сравнения номеров идентифи-
каторов транзакций (XID): версия строки, у которой XID добавившей её транзакции больше, чем
XID текущей транзакции, относится «к будущему» и не должна быть видна в текущей транзакции.
Однако поскольку идентификаторы транзакций имеют ограниченный размер (32 бита), кластер,
работающий долгое время (более 4 миллиардов транзакций) столкнётся с зацикливанием иденти-
фикаторов транзакций: счётчик XID прокрутится до нуля, и внезапно транзакции, которые отно-
сились к прошлому, окажутся в будущем — это означает, что их результаты станут невидимыми.
Одним словом, это катастрофическая потеря данных. (На самом деле данные никуда не пропада-
622Регламентные задачи об-
служивания базы данных
ют, однако если вы не можете их получить, то это слабое утешение.) Для того чтобы этого избе-
жать, необходимо выполнять очистку для каждой таблицы в каждой базе данных как минимум
единожды на два миллиардов транзакций.
Периодическое выполнение очистки решает эту проблему, потому что процедура VACUUM помечает
строки как замороженные, указывая, что они были вставлены транзакцией, зафиксированной до-
статочно давно, так что эффект добавляющей транзакции с точки зрения MVCC определённо бу-
дет виден во всех текущих и будущих транзакциях. Обычные значения XID сравниваются по моду-
32
лю 2 . Это означает, что для каждого обычного XID существуют два миллиарда значений XID, ко-
торые «старше» него, и два миллиарда значений, которые «младше» него; другими словами, про-
странство значений XID циклично и не имеет конечной точки. Следовательно, как только создаёт-
ся версия строки с обычным XID, для следующих двух миллиардов транзакций эта версия строки
оказывается «в прошлом», неважно о каком значении обычного XID идет речь. Если после двух
миллиардов транзакций эта версия строки всё ещё существует, она внезапно окажется в будущем.
Для того чтобы это предотвратить, в какой-то момент значение XID для старых версий строк долж-
но быть заменено на FrozenTransactionId (заморожено) до того, как будет достигнута граница в
два миллиарда транзакций. После получения этого особенного XID для всех обычных транзакций
эти версии строк будут относиться «к прошлому», независимо от зацикливания, и, таким образом,
эти версии строк будут действительны до момента их удаления, когда бы это ни произошло.
Примечание
В версиях PostgreSQL до 9.4 замораживание было реализовано как замена XID добав-
ления строки специальным идентификатором FrozenTransactionId, который можно
было увидеть в системной колонке xmin данной строки. В новых версиях просто уста-
навливается битовый флаг, а исходный xmin строки сохраняется для возможного рас-
следования в будущем. Однако строки с xmin, равным FrozenTransactionId (2), мож-
но по-прежнему встретить в базах данных, обновлённых (с применением pg_upgrade)
с версий до 9.4.
Также системные каталоги могут содержать строки со значением xmin, равным
BootstrapTransactionId (1), показывающим, что они были вставлены на первом этапе
initdb. Как и FrozenTransactionId, этот специальный XID считается более старым, чем
любой обычный XID.
Параметр vacuum_freeze_min_age определяет, насколько старым должен стать XID, чтобы строки
с таким XID были заморожены. Увеличение его значения помогает избежать ненужной работы,
если строки, которые могли бы быть заморожены в ближайшее время, будут изменены ещё раз, а
уменьшение приводит к увеличению количества транзакций, которые могут выполниться, прежде
чем потребуется очередная очистка таблицы.
VACUUM определяет, какие страницы таблицы нужно сканировать, анализируя карту видимости.
Обычно при этой операции пропускаются страницы, в которых нет мёртвых версий строк, даже
если в них могут быть версии строк со старыми XID. Таким образом, обычная команда VACUUM не
будет всегда замораживать все версии строк, имеющиеся в таблице. Периодически VACUUM будет
также производить агрессивную очистку, пропуская только те страницы, которые не содержат ни
мёртвых строк, ни незамороженных значений XID или MXID. Когда VACUUM будет делать это, зави-
сит от параметра vacuum_freeze_table_age: полностью видимые, но не полностью замороженные
страницы будут сканироваться, если число транзакций, прошедших со времени последнего такого
сканирования, оказывается больше чем vacuum_freeze_table_age минус vacuum_freeze_min_age.
Если vacuum_freeze_table_age равно 0, VACUUM будет применять эту более агрессивную стратегию
при каждом сканировании.
Максимальное время, в течение которого таблица может обходиться без очистки, составляет два
миллиарда транзакций минус значение vacuum_freeze_min_age с момента последней агрессивной
очистки. Если бы таблица не подвергалась очистке дольше, была бы возможна потеря данных. Что-
бы гарантировать, что это не произойдёт, для любой таблицы, которая может содержать значения
623Регламентные задачи об-
служивания базы данных
XID старше, чем возраст, указанный в конфигурационном параметре autovacuum_freeze_max_age,
вызывается автоочистка. (Это случится, даже если автоочистка отключена.)
Это означает, что если очистка таблицы не вызывается другим способом, то автоочистка
для неё будет вызываться приблизительно через каждые autovacuum_freeze_max_age минус
vacuum_freeze_min_age транзакций. Для таблиц, очищаемых регулярно для высвобождения про-
странства, это неактуально. В то же время статичные таблицы (включая таблицы, в которых дан-
ные вставляются, но не изменяются и не удаляются) не нуждаются в очистке для высвобожде-
ния пространства, поэтому для очень больших статичных таблиц имеет смысл увеличить интер-
вал между вынужденными запусками автоочистки. Очевидно, это можно сделать, либо увеличив
autovacuum_freeze_max_age, либо уменьшив vacuum_freeze_min_age.
Фактический
максимум
для
vacuum_freeze_table_age
составляет
0.95
*
autovacuum_freeze_max_age; большее значение будет ограничено этим пределом. Значение, пре-
вышающее autovacuum_freeze_max_age, не имело бы смысла, поскольку по достижении этого зна-
чения в любом случае вызывалась бы автоочистка для предотвращения зацикливания, а коэффи-
циент 0.95 оставляет немного времени для того, чтобы запустить команду VACUUM вручную до того,
как это произойдёт. Как правило, установленное значение vacuum_freeze_table_age должно быть
несколько меньше autovacuum_freeze_max_age, чтобы оставленный промежуток был достаточен
для выполнения в этом окне VACUUM по расписанию или автоочистки, управляемой обычной актив-
ностью операций удаления и изменения. Если это значение будет слишком близким к максимуму,
автоочистка для предотвращения зацикливания будет выполняться, даже если таблица только что
была очищена для высвобождения пространства, в то же время при небольшом значении будет
чаще производиться агрессивная очистка.
Единственный минус увеличения autovacuum_freeze_max_age (и vacuum_freeze_table_age с
ним) заключается в том, что подкаталоги pg_xact и pg_commit_ts в кластере баз дан-
ных будут занимать больше места, поскольку в них нужно будет хранить статус и (при
включённом track_commit_timestamp) время фиксации всех транзакций вплоть до горизонта
autovacuum_freeze_max_age. Для статуса фиксации используется по два бита на транзакцию, по-
этому если в autovacuum_freeze_max_age установлено максимально допустимое значение в два
миллиарда, то размер pg_xact может составить примерно половину гигабайта, а pg_commit_ts
примерно 20 ГБ. Если по сравнению с объёмом вашей базы данных этот объём незначителен, тогда
рекомендуется установить для autovacuum_freeze_max_age максимально допустимое значение. В
противном случае установите значение этого параметра в зависимости от объёма, который вы го-
товы выделить для pg_xact и pg_commit_ts. (Значению по умолчанию, 200 миллионам транзакций,
соответствует приблизительно 50 МБ в pg_xact и около 2 ГБ в pg_commit_ts.)
Уменьшение значения vacuum_freeze_min_age, с другой стороны, чревато тем, что команда VACUUM
может выполнять бесполезную работу: замораживание версии строки — пустая трата времени,
если эта строка будет вскоре изменена (и в результате получит новый XID). Поэтому значение
этого параметра должно быть достаточно большим для того, чтобы строки не замораживались,
пока их последующее изменение не станет маловероятным.
Для отслеживания возраста самых старых значений XID в базе данных команда VACUUM сохра-
няет статистику по XID в системных таблицах pg_class и pg_database. В частности, столбец
relfrozenxid в записи для определённой таблицы в pg_class содержит граничное значение XID,
с которым в последний раз выполнялась агрессивная очистка (VACUUM) этой таблицы. Все строки,
добавленные транзакциями с более ранними XID, гарантированно будут заморожены. Аналогич-
но столбец datfrozenxid в записи для базы данных в pg_database представляет нижнюю грани-
цу обычных значений XID, встречающихся в этой базе — он просто хранит минимальное из всех
значений relfrozenxid для таблиц этой базы. Эту информацию удобно получать с помощью таких
запросов:
SELECT c.oid::regclass as table_name,
greatest(age(c.relfrozenxid),age(t.relfrozenxid)) as age
FROM pg_class c
LEFT JOIN pg_class t ON c.reltoastrelid = t.oid
624Регламентные задачи об-
служивания базы данных
WHERE c.relkind IN (‘r’, ‘m’);
SELECT datname, age(datfrozenxid) FROM pg_database;
Столбец age показывает количество транзакций от граничного значения XID до XID текущей тран-
закции.
Обычно VACUUM сканирует только те страницы, которые изменялись после последней очистки, од-
нако relfrozenxid может увеличиться только при сканировании всех страниц таблицы, включая
те, что могут содержать незамороженные XID. Это происходит когда возраст relfrozenxid дости-
гает vacuum_freeze_table_age транзакций, когда VACUUM вызывается с указанием FREEZE, или ко-
гда оказывается, что очистку для удаления мёртвых версий строк нужно провести во всех ещё не
замороженных страницах. Когда VACUUM сканирует в таблице каждую ещё не полностью заморо-
женную страницу, значение age(relfrozenxid) в результате должно стать немного больше, чем
установленное значение vacuum_freeze_min_age (больше на число транзакций, начатых с момента
запуска VACUUM). Если по достижении autovacuum_freeze_max_age для таблицы ни разу не будет
выполнена операция relfrozenxid, в скором времени для неё будет принудительно запущена ав-
тоочистка.
Если по какой-либо причине автоочистка не может вычистить старые значения XID из таблицы,
система начинает выдавать предупреждающие сообщения, подобные приведённому ниже, когда
самое старое значение XID в базе данных оказывается в десяти миллионах транзакций от точки
зацикливания:
ПРЕДУПРЕЖДЕНИЕ: база данных “mydb” должна быть очищена (предельное число транзакций:
177009986)
ПОДСКАЗКА: Во избежание отключения базы данных выполните очистку (VACUUM) всей базы
“mydb”.
(Проблему можно решить, как предлагает подсказка, запустив VACUUM вручную; однако учтите, что
выполнять VACUUM должен суперпользователь, в противном случае эта процедура не сможет об-
работать системные каталоги и, следовательно, не сможет увеличить значение datfrozenxid для
базы данных.) Если эти предупреждения игнорировать, система отключится и не будет начинать
никаких транзакций, как только до точки зацикливания останется менее 1 миллиона транзакций:
ОШИБКА: база данных не принимает команды во избежание потери данных из-за зацикливания
в БД “mydb”
ПОДСКАЗКА: Остановите управляющий процесс (postmaster) и выполните очистку (VACUUM)
базы данных в однопользовательском режиме.
Резерв в 1 миллион транзакций позволяет администратору провести восстановление без потери
данных, выполнив необходимые команды VACUUM вручную. Однако, поскольку после безопасной
остановки система не будет исполнять команды, администратору останется только перезапустить
сервер в однопользовательском режиме, чтобы запустить VACUUM. За подробной информацией об
использовании однопользовательского режима обратитесь к странице справки по postgres.
24.1.5.1. Мультитранзакции и зацикливание
Идентификаторы мультитранзакций используются для поддержки блокировки строк несколь-
кими транзакциями одновременно. Поскольку в заголовке строки есть только ограниченное про-
странство для хранения информации о блокировках, в нём указывается «идентификатор множе-
ственной транзакции», или идентификатор мультитранзакции для краткости, когда строку бло-
кируют одновременно несколько транзакций. Информация о том, какие именно идентификато-
ры транзакций относятся к определённой мультитранзакции, хранится отдельно в подкаталоге
pg_multixact, а в поле xmax в заголовке строки сохраняется только идентификатор мультитранзак-
ции. Как и идентификаторы транзакций, идентификаторы мультитранзакций исполнены в виде 32-
разрядного счётчика и хранятся аналогично, что требует аккуратного управления их возрастом,
очисткой хранилища и предотвращением зацикливаний. Существует отдельная область, в которой
содержится список членов каждой мультитранзакции, где счётчики также 32-битные и требуют
должного контроля.
625Регламентные задачи об-
служивания базы данных
Когда VACUUM сканирует какую-либо часть таблицы, каждый идентификатор мультитранзакции
старее чем vacuum_multixact_freeze_min_age заменяется другим значением, которое может быть
нулевым, идентификатором одиночной транзакции или новым идентификатором мультитранзак-
ции. Для каждой таблицы в pg_class.relminmxid хранится самый старый возможный идентифика-
тор мультитранзакции, всё ещё задействованный в какой-либо строке этой таблицы. Если это зна-
чение оказывается старее vacuum_multixact_freeze_table_age, выполняется агрессивная очистка.
Как рассказывалось в предыдущем разделе, при агрессивной очистке будут пропускаться только
те страницы, которые считаются полностью замороженными. Узнать возраст pg_class.relminmxid
можно с помощью функции mxid_age().
Благодаря агрессивным операциям VACUUM, вне зависимости от их причины, это значение для таб-
лицы будет увеличиваться. В конце концов, по мере сканирования всех таблиц во всех базах дан-
ных и увеличения их старейших значений мультитранзакций, информация о старых мультитран-
закциях может быть удалена с диска.
В качестве меры защиты, агрессивное сканирование с целью очистки будет происходить для любой
таблицы, возраст мультитранзакций которой больше, чем autovacuum_multixact_freeze_max_age.
Агрессивное сканирование также будет выполняться постепенно со всеми таблицами, начиная с
имеющих старейшие мультитранзакции, если объём занятой области членов мультитранзакций
превышает 50% от объёма адресуемого пространства. Эти два варианта агрессивного сканирова-
ния осуществляются, даже если процесс автоочистки отключён.
24.1.6. Демон автоочистки
В PostgreSQL имеется не обязательная, но настоятельно рекомендуемая к использованию функ-
ция, называемая автоочисткой, предназначение которой — автоматизировать выполнение ко-
манд VACUUM и ANALYZE . Когда автоочистка включена, она проверяет, в каких таблицах было встав-
лено, изменено или удалено много строк. При этих проверках используются средства сбора ста-
тистики; поэтому автоочистка будет работать, только если параметр track_counts имеет значение
true. В конфигурации по умолчанию автоочистка включена и соответствующие параметры имеют
подходящие значения.
«Демон автоочистки» на самом деле состоит из нескольких процессов. Существует постоянный
фоновый процесс, называемый процессом запуска автоочистки, который отвечает за запуск ра-
бочих процессов автоочистки для всех баз данных. Этот контролирующий процесс распреде-
ляет работу по времени, стараясь запускать рабочий процесс для каждой базы данных каждые
autovacuum_naptime секунд. (Следовательно, если всего имеется N баз данных, новый рабочий про-
цесс будет запускаться каждые autovacuum_naptime/N секунд.) Одновременно могут выполняться
до autovacuum_max_workers рабочих процессов. Если число баз данных, требующих обработки,
превышает autovacuum_max_workers, обработка следующей базы начинается сразу по завершении
первого рабочего процесса. Каждый рабочий процесс проверяет все таблицы в своей базе данных
и в случае необходимости выполняет VACUUM и/или ANALYZE. Для отслеживания действий рабочих
процессов можно установить параметр log_autovacuum_min_duration.
Если в течение короткого промежутка времени потребность в очистке возникает для нескольких
больших таблиц, все рабочие процессы автоочистки могут продолжительное время заниматься
очисткой только этих таблиц. В результате другие таблицы и базы данных будут ожидать очистки,
пока не появится свободный рабочий процесс. Число рабочих процессов для одной базы не огра-
ничивается, при этом каждый процесс старается не повторять работу, только что выполненную
другими. Заметьте, что в ограничениях max_connections или superuser_reserved_connections число
выполняющихся рабочих процессов не учитывается.
Для таблиц с relfrozenxid, устаревшим более чем на autovacuum_freeze_max_age транзакций,
очистка выполняется всегда (это также применимо к таблицам, для которых максимальный по-
рог заморозки был изменён через параметры хранения; см. ниже). В противном случае, очистка
таблицы производится, если количество кортежей, устаревших с момента последнего выполнения
VACUUM, превышает «пороговое значение очистки». Пороговое значение очистки определяется как:
порог очистки = базовый порог очистки + коэффициент доли для очистки * количество
кортежей
626Регламентные задачи об-
служивания базы данных
где базовый порог очистки — значение autovacuum_vacuum_threshold, коэффициент доли —
autovacuum_vacuum_scale_factor, а количество кортежей — pg_class.reltuples. Количество уста-
ревших кортежей получается от сборщика статистики; оно представляет собой приблизительное
число, обновляемое после каждой операции UPDATE и DELETE. (Точность не гарантируется, потому
что при большой нагрузке часть информации может быть утеряна.) Если значение relfrozenxid
для таблицы старее vacuum_freeze_table_age транзакций, производится агрессивная очистка с
целью заморозить старые версии строк и увеличить значение relfrozenxid; в противном случае
сканируются только страницы, изменённые после последней очистки.
Для выполнения сбора статистики используется аналогичное условие: пороговое значение, опре-
деляемое как:
порог анализа = базовый порог анализа + коэффициент доли для анализа * количество
кортежей
сравнивается с общим количеством кортежей добавленных, изменённых или удалённых после по-
следнего выполнения ANALYZE.
Автоочистка не обрабатывает временные таблицы. Поэтому очистку и сбор статистики в них нужно
производить с помощью SQL-команд в обычном сеансе.
Используемые по умолчанию пороговые значения и коэффициенты берутся из postgresql.conf,
однако их (и многие другие параметры, управляющие автоочисткой) можно переопределить для
каждой таблицы; за подробностями обратитесь к Подразделу «Параметры хранения». Если ка-
кие-либо значения определены через параметры хранения таблицы, при обработке этой таблицы
действуют они, а в противном случае — глобальные параметры. За более подробной информацией
о глобальных параметрах обратитесь к Разделу 19.10.
Когда выполняются несколько рабочих процессов, параметры задержки автоочистки по стоимости
(см. Подраздел 19.4.4) «распределяются» между всеми этими процессами, так что общее воздей-
ствие на систему остаётся неизменным, независимо от их числа. Однако этот алгоритм распреде-
ления нагрузки не учитывает процессы, обрабатывающие таблицы с индивидуальными значения-
ми параметров хранения autovacuum_vacuum_cost_delay и autovacuum_vacuum_cost_limit.
24.2. Регулярная переиндексация
В некоторых ситуациях стоит периодически перестраивать индексы, выполняя команду REINDEX
или последовательность отдельных шагов по восстановлению индексов.
Страницы индексов на основе B-деревьев, которые стали абсолютно пустыми, могут быть исполь-
зованы повторно. Однако возможность неэффективного использования пространства всё же оста-
ётся: если со страницы были удалены почти все, но не все ключи индекса, страница всё равно оста-
ётся занятой. Следовательно, шаблон использования, при котором со временем удаляются многие,
но не все ключи в каждом диапазоне, приведёт к неэффективному расходованию пространства. В
таких случаях рекомендуется периодически проводить переиндексацию.
Возможность потери пространства в индексах на основе не B-деревьев глубоко не исследовалась.
Поэтому имеет смысл периодически отслеживать физический размер индекса, когда применяется
индекс такого типа.
Кроме того, с B-деревьями доступ по недавно построенному индексу осуществляется немного быст-
рее, нежели доступ по индексу, который неоднократно изменялся, поскольку в недавно построен-
ном индексе страницы, близкие логически, обычно расположены так же близко и физически. (Это
соображение неприменимо к индексам, которые основаны не на B-деревьях.) Поэтому периодиче-
ски проводить переиндексацию стоит хотя бы для того, чтобы увеличить скорость доступа.
Команду REINDEX можно безопасно и просто применять во всех случаях. Но так как она требует
исключительной блокировки таблицы, часто предпочтительнее перестраивать индекс в несколько
этапов, включающих создание и замену индекса. Типы индексов, которые поддерживает CREATE
INDEX с указанием CONCURRENTLY, можно построить именно так. Если это удаётся и получен рабо-
627Регламентные задачи об-
служивания базы данных
чий индекс, изначальный индекс можно заменить им, выполнив ALTER INDEX и DROP INDEX. Ко-
гда индекс используется для обеспечения уникальности или других ограничений, может потребо-
ваться команда ALTER TABLE, чтобы поменять существующее ограничение на то, что обеспечива-
ет новый индекс. Обстоятельно продумайте эту многоходовую процедуру, прежде чем выполнять
её, так как не все индексы можно перестроить таким образом, и предусмотрите обработку ошибок.
24.3. Обслуживание журнала
Журнал сервера базы данных желательно сохранять где-либо, а не просто сбрасывать его в /dev/
null. Этот журнал бесценен при диагностике проблем. Однако он может быть очень объёмным
(особенно при высоких уровнях отладки), так что хранить его неограниченно долго вы вряд ли за-
хотите. Поэтому необходимо организовать ротацию журнальных файлов так, чтобы новые файлы
создавались, а старые удалялись через разумный промежуток времени.
Если просто направить stderr команды postgres в файл, вы получите в нём журнал сообщений,
но очистить этот файл можно будет, только если остановить и перезапустить сервер. Это может
быть допустимо при использовании PostgreSQL в среде разработки, но вряд ли такой вариант будет
приемлемым в производственной среде.
Лучшим подходом будет перенаправление вывода сервера stderr в какую-либо программу ро-
тации журнальных файлов. Существует и встроенное средство ротации журнальных файлов,
которое можно использовать, установив для параметра logging_collector значение true в
postgresql.conf. Параметры, управляющие этой программой, описаны в Подразделе 19.8.1. Этот
подход также можно использовать для получения содержимого журнала в формате CSV (значе-
ния, разделённые запятыми).
Вы также можете использовать внешнюю программу для ротации журнальных файлов, если уже
применяете такое приложение для других серверных приложений. Например, утилиту rotatelogs,
включённую в дистрибутив Apache, можно использовать и с PostgreSQL. Для этого просто направь-
те вывод stderr сервера в желаемую программу. Если вы запускаете сервер, используя pg_ctl, то
stderr уже будет перенаправлен в stdout, так что будет достаточно просто применить конвейер,
например:
pg_ctl start | rotatelogs /var/log/pgsql_log 86400
Ещё одно решение промышленного уровня заключается в передаче журнала в syslog, чтобы ро-
тацией файлов занималась уже служба syslog. Для этого присвойте параметру конфигурации
log_destination значение syslog (для вывода журнала только в syslog) в postgresql.conf. Затем
вы сможете посылать сигнал SIGHUP службе syslog, когда захотите принудительно начать запись
нового журнального файла. Если вы хотите автоматизировать ротацию журнальных файлов, про-
грамму logrotate можно настроить и для работы с журнальными файлами, которые формирует
syslog.
Однако во многих системах, а особенно c большими сообщениями, syslog работает не очень надёж-
но; он может обрезать или терять сообщения как раз тогда, когда они вам нужны. Кроме того, в
Linux, syslog&gt; сбрасывает каждое сообщение на диск, от чего страдает производительность. (Для
отключения этой синхронной записи можно добавить «-» перед именем файла в файле конфигу-
рации syslog.)
Обратите внимание, что все описанные выше решения обеспечивают создание новых журнальных
файлов через задаваемые промежутки времени, но не удаление старых, ставших бесполезными
файлов журналов. Возможно, вы захотите создать задание для периодического удаления старых
файлов. Кроме того, вы можете настроить программу ротации файлов так, чтобы старые файлы
журналов циклически перезаписывались.
Также вам может быть полезен pgBadger — инструмент для сложного анализа файлов журнала.
Кроме того, check_postgres может посылать уведомления в Nagios, когда в журнале появляются
важные сообщения, а также при обнаружении других нестандартных ситуаций.
628</li>
</ol>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->



<div class="pagination">
  <ul class="inline-list">
    
    
      
        <li><a href="http://localhost:4000/page22/" class="btn">Previous</a></li>
      
    

    
    
      <li><a href="http://localhost:4000">1</a></li>
    

    
    
      
      
      <li>…</li>
    

    
    
    

    
      
        
        
        
        <li><a href="http://localhost:4000/page21/">21</a></li>
      
    
      
        
        
        
        <li><a href="http://localhost:4000/page22/">22</a></li>
      
    
      
        <li><strong class="current-page">23</strong></li>
      
    
      
        
        
        
        <li><a href="http://localhost:4000/page24/">24</a></li>
      
    
      
        
        
        
        <li><a href="http://localhost:4000/page25/">25</a></li>
      
    

    
    
      <li>…</li>
    

    
      <li><a href="http://localhost:4000/page40/">40</a></li>
    

    
    
      <li><a href="http://localhost:4000/page24/" class="btn">Next</a></li>
    
  </ul>
</div>

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Sergey Khatsiola. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-130427752-1', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>


          

</body>
</html>